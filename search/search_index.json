{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Portfolio Hello, and welcome to my portfolio! I\u2019m Miguel Amaral , a passionate Python Developer and Data Engineer with over 15 years of experience in Python programming and data science. With a deep understanding of Linux/Ubuntu , I specialize in building data pipelines, managing ETL processes, and developing predictive models. My expertise extends across various domains, including container orchestration , API management , cloud services , and system integration . I am proficient in search and information retrieval techniques, web scraping , and machine learning \u2014skills honed through years of hands-on experience in diverse projects. For a deeper dive into my professional journey and technologies I work with, visit \ud83d\ude80 AmaralApps.com \ud83d\ude80 What I Offer Custom Python Development : Tailored solutions to meet your specific needs. Advanced Data Engineering : Expertise in ETL processes, data pipelines, and cloud services such as AWS. Machine Learning Solutions : Implementing AI and predictive modeling to drive innovation. System Integration and API Development : Seamless integration across platforms and services, leveraging microservices architecture. Search and Information Retrieval : Skilled in semantic search, vector search, and RAG (Retriever-Augmented Generation) pipelines for advanced information retrieval. Core Skills Programming Languages : Python (15 years), Bash, JavaScript Cloud & Infrastructure : AWS (EC2, S3, RDS, Lambda, etc.), MongoDB, Supabase Container Orchestration : Kubernetes, Docker, Rancher Data Engineering Tools : Apache Airflow, Kedro, Dagster Databases : PostgreSQL, MySQL, MongoDB, ChromaDB Machine Learning : TensorFlow, scikit-learn, Transformers, OpenAI API Web Development : Flask, Django, HTML/CSS, JavaScript Data Visualization : Matplotlib, Seaborn, Plotly, Kibana Feel free to explore my work, dive into my projects, and see how I can help you achieve your data and development goals. Check out my GitHub for examples of my work and contact me to discuss your next project. Thank you for visiting!","title":"Home"},{"location":"#welcome-to-my-portfolio","text":"Hello, and welcome to my portfolio! I\u2019m Miguel Amaral , a passionate Python Developer and Data Engineer with over 15 years of experience in Python programming and data science. With a deep understanding of Linux/Ubuntu , I specialize in building data pipelines, managing ETL processes, and developing predictive models. My expertise extends across various domains, including container orchestration , API management , cloud services , and system integration . I am proficient in search and information retrieval techniques, web scraping , and machine learning \u2014skills honed through years of hands-on experience in diverse projects. For a deeper dive into my professional journey and technologies I work with, visit \ud83d\ude80 AmaralApps.com \ud83d\ude80","title":"Welcome to My Portfolio"},{"location":"#what-i-offer","text":"Custom Python Development : Tailored solutions to meet your specific needs. Advanced Data Engineering : Expertise in ETL processes, data pipelines, and cloud services such as AWS. Machine Learning Solutions : Implementing AI and predictive modeling to drive innovation. System Integration and API Development : Seamless integration across platforms and services, leveraging microservices architecture. Search and Information Retrieval : Skilled in semantic search, vector search, and RAG (Retriever-Augmented Generation) pipelines for advanced information retrieval.","title":"What I Offer"},{"location":"#core-skills","text":"Programming Languages : Python (15 years), Bash, JavaScript Cloud & Infrastructure : AWS (EC2, S3, RDS, Lambda, etc.), MongoDB, Supabase Container Orchestration : Kubernetes, Docker, Rancher Data Engineering Tools : Apache Airflow, Kedro, Dagster Databases : PostgreSQL, MySQL, MongoDB, ChromaDB Machine Learning : TensorFlow, scikit-learn, Transformers, OpenAI API Web Development : Flask, Django, HTML/CSS, JavaScript Data Visualization : Matplotlib, Seaborn, Plotly, Kibana Feel free to explore my work, dive into my projects, and see how I can help you achieve your data and development goals. Check out my GitHub for examples of my work and contact me to discuss your next project. Thank you for visiting!","title":"Core Skills"},{"location":"contact/","text":"Miguel Amaral Data Engineer & Python Dev Website: \ud83d\ude80 www.amaralApps.com \ud83d\ude80 Email: miguel.junior.mat@hotmail.com LinkedIn: Miguel Amaral GitHub: MiguelZeph Feel free to reach out to me through any of the contact methods listed above.","title":"\ud83d\udcde Contact"},{"location":"about/about/","text":"About Me Hello! I\u2019m a seasoned Python Developer and Data Engineer with over 15 years of experience in data science and engineering. My background spans various roles, allowing me to adapt and excel as a Data Scientist , Machine Learning Engineer , Software Engineer , or Data Analyst . I have worked on a diverse range of projects throughout my career, which has equipped me with the skills and insights to contribute effectively across different areas. I\u2019m passionate about continuous learning, teaching, and making impactful contributions to my team. \ud83d\ude80 For more detailed information about my professional journey and technologies I work with, visit AmaralApps.com \ud83d\ude80 Key Skills and Expertise ETL/ELT Processes : Development and management of robust ETL/ELT pipelines for efficient data processing. Data Pipelines : Optimization of data workflows to ensure efficiency and scalability. Containerized Environments : Proficient in orchestrating and managing Docker containers for consistent deployments. API Management : Expertise in designing and implementing APIs for streamlined data access and manipulation. Dashboard Creation : Skilled in building interactive dashboards to visualize data insights effectively. AI and Machine Learning : Application of machine learning techniques for predictive analysis and business intelligence. Web Scraping & Text Parsing : Extraction and processing of data from various sources to support data-driven projects. Web Development : Building web applications that complement data solutions and enhance accessibility. Database Management : Administration and optimization of databases to ensure high performance and reliability. Cloud Services : Leveraging AWS and other cloud platforms for scalable and resilient data solutions. Deployment : Experience with deployment of applications, services, and machine learning models in production environments. Environment Development : Experienced with Linux/Ubuntu and Windows (WSL2) for flexible development environments. Education PhD in Materials Engineering Instituto Nacional de Pesquisas Espaciais (INPE), 2014 - 2018 Master's Degree in Materials Engineering Instituto Nacional de Pesquisas Espaciais (INPE), 2012 \u2013 2014 Bachelor's Degree in Physics Universidade de Taubat\u00e9, 2009 \u2013 2012 Location Let\u2019s Connect! Feel free to reach out to discuss how I can help with your next project or explore potential opportunities. You can contact me via email or check out my GitHub for examples of my work.","title":"\ud83d\udc64 About Me"},{"location":"about/about/#about-me","text":"Hello! I\u2019m a seasoned Python Developer and Data Engineer with over 15 years of experience in data science and engineering. My background spans various roles, allowing me to adapt and excel as a Data Scientist , Machine Learning Engineer , Software Engineer , or Data Analyst . I have worked on a diverse range of projects throughout my career, which has equipped me with the skills and insights to contribute effectively across different areas. I\u2019m passionate about continuous learning, teaching, and making impactful contributions to my team. \ud83d\ude80 For more detailed information about my professional journey and technologies I work with, visit AmaralApps.com \ud83d\ude80","title":"About Me"},{"location":"about/about/#key-skills-and-expertise","text":"ETL/ELT Processes : Development and management of robust ETL/ELT pipelines for efficient data processing. Data Pipelines : Optimization of data workflows to ensure efficiency and scalability. Containerized Environments : Proficient in orchestrating and managing Docker containers for consistent deployments. API Management : Expertise in designing and implementing APIs for streamlined data access and manipulation. Dashboard Creation : Skilled in building interactive dashboards to visualize data insights effectively. AI and Machine Learning : Application of machine learning techniques for predictive analysis and business intelligence. Web Scraping & Text Parsing : Extraction and processing of data from various sources to support data-driven projects. Web Development : Building web applications that complement data solutions and enhance accessibility. Database Management : Administration and optimization of databases to ensure high performance and reliability. Cloud Services : Leveraging AWS and other cloud platforms for scalable and resilient data solutions. Deployment : Experience with deployment of applications, services, and machine learning models in production environments. Environment Development : Experienced with Linux/Ubuntu and Windows (WSL2) for flexible development environments.","title":"Key Skills and Expertise"},{"location":"about/about/#education","text":"PhD in Materials Engineering Instituto Nacional de Pesquisas Espaciais (INPE), 2014 - 2018 Master's Degree in Materials Engineering Instituto Nacional de Pesquisas Espaciais (INPE), 2012 \u2013 2014 Bachelor's Degree in Physics Universidade de Taubat\u00e9, 2009 \u2013 2012","title":"Education"},{"location":"about/about/#location","text":"","title":"Location"},{"location":"about/about/#lets-connect","text":"Feel free to reach out to discuss how I can help with your next project or explore potential opportunities. You can contact me via email or check out my GitHub for examples of my work.","title":"Let\u2019s Connect!"},{"location":"concepts/api/01_api_basic/","text":"1. API Documentation Table of Contents What is an API? Types of APIs 1. REST APIs 2. SOAP APIs 3. GraphQL APIs 4. WebSocket APIs 5. Database APIs 6. Open APIs 7. Private APIs 8. Cloud APIs 9. E-commerce APIs 10. Social Media APIs Pros and Cons of Each API Type What is an API? An API (Application Programming Interface) is a set of rules and protocols for building and interacting with software applications. It allows different software systems to communicate with each other and share data and functionalities. Types of APIs 1. REST APIs Description : REST (Representational State Transfer) APIs use HTTP methods to interact with resources. Usage : Common in web applications and microservices. 2. SOAP APIs Description : SOAP (Simple Object Access Protocol) APIs use XML for messaging and follow a strict protocol. Usage : Often used in enterprise environments requiring high security. 3. GraphQL APIs Description : GraphQL APIs allow clients to request only the data they need, reducing over-fetching. Usage : Gaining popularity in applications that require flexible data queries. 4. WebSocket APIs Description : WebSocket APIs enable real-time, bidirectional communication between client and server. Usage : Used in chat applications, online games, and other real-time applications. 5. Database APIs Description : Provide interfaces for accessing and manipulating data in databases. Usage : Common in web and mobile applications. 6. Open APIs Description : Publicly available APIs that allow third-party developers to access certain features or data. Usage : Promotes third-party application development. 7. Private APIs Description : Used internally within organizations to improve efficiency and communication. Usage : Helps connect different internal systems. 8. Cloud APIs Description : Provide access to cloud services such as storage, computing, and databases. Usage : Widely used by businesses leveraging cloud infrastructure. 9. E-commerce APIs Description : Facilitate integration with payment processors, shipping, and inventory management. Usage : Essential for online retail operations. 10. Social Media APIs Description : Enable interaction with social media platforms, allowing access to user data and functionalities. Usage : Used for sharing content, authentication, and data analysis. Pros and Cons of Each API Type API Type Pros Cons REST APIs - Simplicity and ease of use. - Uses standard HTTP methods (GET, POST, PUT, DELETE). - Supports multiple formats (JSON, XML). - May lead to over-fetching or under-fetching. - Less efficient for complex queries. SOAP APIs - Strict structure and standards. - Supports transactions and security (WS-Security). - Works well in corporate environments. - More complex and verbose. - Requires more bandwidth due to XML usage. - Difficult for beginners. GraphQL APIs - Flexibility in querying data. - Reduces over-fetching and under-fetching. - Single endpoint for multiple operations. - Steeper learning curve. - Requires more planning in structuring queries. - Increased complexity on the server side. WebSocket APIs - Real-time, bidirectional communication. - Lower latency compared to HTTP calls. - More challenging to implement and maintain. - Potential scalability issues. - Requires server configuration. Database APIs - Direct access to data and efficient operations. - Flexibility in data manipulation. - Can become a single point of failure. - Managing connections and security can be challenging. Open APIs - Facilitates integration by external developers. - Increases service adoption. - Creates an ecosystem of applications. - Security can be a concern if not properly managed. - Potential for misuse or abuse. Private APIs - Better control over access to data. - Optimized for specific organizational needs. - Limited access for external developers. - Less visibility and collaboration with the outside community. Cloud APIs - Scalability and flexibility. - Reduces need for physical infrastructure. - Integration with other cloud services. - Dependency on cloud provider. - Variable costs based on usage. - Latency issues. E-commerce APIs - Facilitates integration with payment and logistics platforms. - Increases operational efficiency. - Dependency on external payment providers. - Vulnerable to security attacks. Social Media APIs - Increases user interaction with social platforms. - Provides access to powerful data and functionalities. - Restrictive usage policies and frequent changes. - Privacy and data security concerns. Conclusion Understanding the different types of APIs and their pros and cons is crucial for making informed decisions in software development. Each API type has its unique strengths and weaknesses, making it essential to choose the right one based on the specific requirements of a project. Author Miguel Angelo Do Amaral Junior","title":"01 - API base"},{"location":"concepts/api/01_api_basic/#1-api-documentation","text":"","title":"1. API Documentation"},{"location":"concepts/api/01_api_basic/#table-of-contents","text":"What is an API? Types of APIs 1. REST APIs 2. SOAP APIs 3. GraphQL APIs 4. WebSocket APIs 5. Database APIs 6. Open APIs 7. Private APIs 8. Cloud APIs 9. E-commerce APIs 10. Social Media APIs Pros and Cons of Each API Type","title":"Table of Contents"},{"location":"concepts/api/01_api_basic/#what-is-an-api","text":"An API (Application Programming Interface) is a set of rules and protocols for building and interacting with software applications. It allows different software systems to communicate with each other and share data and functionalities.","title":"What is an API?"},{"location":"concepts/api/01_api_basic/#types-of-apis","text":"","title":"Types of APIs"},{"location":"concepts/api/01_api_basic/#1-rest-apis","text":"Description : REST (Representational State Transfer) APIs use HTTP methods to interact with resources. Usage : Common in web applications and microservices.","title":"1. REST APIs"},{"location":"concepts/api/01_api_basic/#2-soap-apis","text":"Description : SOAP (Simple Object Access Protocol) APIs use XML for messaging and follow a strict protocol. Usage : Often used in enterprise environments requiring high security.","title":"2. SOAP APIs"},{"location":"concepts/api/01_api_basic/#3-graphql-apis","text":"Description : GraphQL APIs allow clients to request only the data they need, reducing over-fetching. Usage : Gaining popularity in applications that require flexible data queries.","title":"3. GraphQL APIs"},{"location":"concepts/api/01_api_basic/#4-websocket-apis","text":"Description : WebSocket APIs enable real-time, bidirectional communication between client and server. Usage : Used in chat applications, online games, and other real-time applications.","title":"4. WebSocket APIs"},{"location":"concepts/api/01_api_basic/#5-database-apis","text":"Description : Provide interfaces for accessing and manipulating data in databases. Usage : Common in web and mobile applications.","title":"5. Database APIs"},{"location":"concepts/api/01_api_basic/#6-open-apis","text":"Description : Publicly available APIs that allow third-party developers to access certain features or data. Usage : Promotes third-party application development.","title":"6. Open APIs"},{"location":"concepts/api/01_api_basic/#7-private-apis","text":"Description : Used internally within organizations to improve efficiency and communication. Usage : Helps connect different internal systems.","title":"7. Private APIs"},{"location":"concepts/api/01_api_basic/#8-cloud-apis","text":"Description : Provide access to cloud services such as storage, computing, and databases. Usage : Widely used by businesses leveraging cloud infrastructure.","title":"8. Cloud APIs"},{"location":"concepts/api/01_api_basic/#9-e-commerce-apis","text":"Description : Facilitate integration with payment processors, shipping, and inventory management. Usage : Essential for online retail operations.","title":"9. E-commerce APIs"},{"location":"concepts/api/01_api_basic/#10-social-media-apis","text":"Description : Enable interaction with social media platforms, allowing access to user data and functionalities. Usage : Used for sharing content, authentication, and data analysis.","title":"10. Social Media APIs"},{"location":"concepts/api/01_api_basic/#pros-and-cons-of-each-api-type","text":"API Type Pros Cons REST APIs - Simplicity and ease of use. - Uses standard HTTP methods (GET, POST, PUT, DELETE). - Supports multiple formats (JSON, XML). - May lead to over-fetching or under-fetching. - Less efficient for complex queries. SOAP APIs - Strict structure and standards. - Supports transactions and security (WS-Security). - Works well in corporate environments. - More complex and verbose. - Requires more bandwidth due to XML usage. - Difficult for beginners. GraphQL APIs - Flexibility in querying data. - Reduces over-fetching and under-fetching. - Single endpoint for multiple operations. - Steeper learning curve. - Requires more planning in structuring queries. - Increased complexity on the server side. WebSocket APIs - Real-time, bidirectional communication. - Lower latency compared to HTTP calls. - More challenging to implement and maintain. - Potential scalability issues. - Requires server configuration. Database APIs - Direct access to data and efficient operations. - Flexibility in data manipulation. - Can become a single point of failure. - Managing connections and security can be challenging. Open APIs - Facilitates integration by external developers. - Increases service adoption. - Creates an ecosystem of applications. - Security can be a concern if not properly managed. - Potential for misuse or abuse. Private APIs - Better control over access to data. - Optimized for specific organizational needs. - Limited access for external developers. - Less visibility and collaboration with the outside community. Cloud APIs - Scalability and flexibility. - Reduces need for physical infrastructure. - Integration with other cloud services. - Dependency on cloud provider. - Variable costs based on usage. - Latency issues. E-commerce APIs - Facilitates integration with payment and logistics platforms. - Increases operational efficiency. - Dependency on external payment providers. - Vulnerable to security attacks. Social Media APIs - Increases user interaction with social platforms. - Provides access to powerful data and functionalities. - Restrictive usage policies and frequent changes. - Privacy and data security concerns.","title":"Pros and Cons of Each API Type"},{"location":"concepts/api/01_api_basic/#conclusion","text":"Understanding the different types of APIs and their pros and cons is crucial for making informed decisions in software development. Each API type has its unique strengths and weaknesses, making it essential to choose the right one based on the specific requirements of a project.","title":"Conclusion"},{"location":"concepts/api/01_api_basic/#author","text":"Miguel Angelo Do Amaral Junior","title":"Author"},{"location":"concepts/api/02_api_rest_graphql/","text":"2. REST API vs GraphQL API Table of Contents REST API vs GraphQL API Overview Key Differences Protocol Requests Response Format Data Fetching Multiple Queries URL Specification Client Libraries Caching CRUD Operations Conclusion Overview REST (Representational State Transfer) and GraphQL are two popular approaches for building APIs. While both serve the purpose of enabling communication between clients and servers, they have different methodologies, features, and use cases. Key Differences Feature REST API GraphQL API Protocol Uses HTTP Uses HTTP Requests Makes requests to specific URLs Makes requests using a single endpoint Response Format Returns JSON (or XML) as a response Returns JSON as a response Data Fetching API implementer decides the source data included Client specifies the required resources and fields Multiple Queries May require multiple queries to retrieve related data Allows fetching multiple resources in a single query URL Specification Requires URL to specify the resource Uses GraphQL schema to specify queries (e.g., type Book { ... } ) Client Libraries Does not require special libraries; can use simple tools (e.g., curl , requests ) Often requires more complex support on both client and server sides Caching Easy to implement caching strategies (e.g., cache database, web) More difficult to cache due to flexible queries CRUD Operations Straightforward and simple for basic CRUD operations More complex, but allows for flexibility in queries Protocol REST APIs and GraphQL APIs both use HTTP as their primary protocol for communication between the client and server, making them compatible with standard web infrastructure. Requests REST APIs typically make requests to specific URLs that represent each resource, while GraphQL APIs make all requests to a single endpoint, relying on the query structure to define the requested resources. Response Format REST APIs commonly return data in JSON or XML format, while GraphQL APIs return data exclusively in JSON. Data Fetching With REST, the API developer determines what data is included in each response. In contrast, GraphQL allows the client to specify which fields and related resources it wants in a single query. Multiple Queries REST often requires multiple calls to retrieve related resources (e.g., multiple endpoints for different parts of data), while GraphQL enables querying multiple resources within a single request. URL Specification In REST APIs, the URL directly specifies the resource being accessed. GraphQL does not use URLs to define resources but instead relies on a schema with types and fields to guide requests. Client Libraries REST can be accessed with basic tools such as curl or simple HTTP libraries, while GraphQL often requires specialized support on both the client and server sides, increasing the complexity. Caching REST APIs generally make caching easier, as each resource has a unique URL that can be cached. In GraphQL, caching is more challenging due to the flexible query structure, making it difficult to predict resource usage. CRUD Operations CRUD operations are straightforward in REST with distinct HTTP methods for each operation. In GraphQL, CRUD operations can be more complex but provide greater flexibility, allowing for custom queries and mutations. Conclusion Both REST APIs and GraphQL APIs have their unique advantages and disadvantages. Choosing between them depends on the specific needs of your application, including data retrieval requirements, complexity, and caching strategies. REST APIs are ideal for applications that require a straightforward approach to CRUD operations and can benefit from easy caching. GraphQL APIs are more suitable for applications that need to query multiple resources efficiently and where the client needs to specify exactly what data is required. By understanding these differences, developers can make informed decisions on which approach to take when designing their APIs. Author : Miguel Amaral","title":"02 - REST API vs GraphQL API"},{"location":"concepts/api/02_api_rest_graphql/#2-rest-api-vs-graphql-api","text":"","title":"2. REST API vs GraphQL API"},{"location":"concepts/api/02_api_rest_graphql/#table-of-contents","text":"REST API vs GraphQL API Overview Key Differences Protocol Requests Response Format Data Fetching Multiple Queries URL Specification Client Libraries Caching CRUD Operations Conclusion","title":"Table of Contents"},{"location":"concepts/api/02_api_rest_graphql/#overview","text":"REST (Representational State Transfer) and GraphQL are two popular approaches for building APIs. While both serve the purpose of enabling communication between clients and servers, they have different methodologies, features, and use cases.","title":"Overview"},{"location":"concepts/api/02_api_rest_graphql/#key-differences","text":"Feature REST API GraphQL API Protocol Uses HTTP Uses HTTP Requests Makes requests to specific URLs Makes requests using a single endpoint Response Format Returns JSON (or XML) as a response Returns JSON as a response Data Fetching API implementer decides the source data included Client specifies the required resources and fields Multiple Queries May require multiple queries to retrieve related data Allows fetching multiple resources in a single query URL Specification Requires URL to specify the resource Uses GraphQL schema to specify queries (e.g., type Book { ... } ) Client Libraries Does not require special libraries; can use simple tools (e.g., curl , requests ) Often requires more complex support on both client and server sides Caching Easy to implement caching strategies (e.g., cache database, web) More difficult to cache due to flexible queries CRUD Operations Straightforward and simple for basic CRUD operations More complex, but allows for flexibility in queries","title":"Key Differences"},{"location":"concepts/api/02_api_rest_graphql/#protocol","text":"REST APIs and GraphQL APIs both use HTTP as their primary protocol for communication between the client and server, making them compatible with standard web infrastructure.","title":"Protocol"},{"location":"concepts/api/02_api_rest_graphql/#requests","text":"REST APIs typically make requests to specific URLs that represent each resource, while GraphQL APIs make all requests to a single endpoint, relying on the query structure to define the requested resources.","title":"Requests"},{"location":"concepts/api/02_api_rest_graphql/#response-format","text":"REST APIs commonly return data in JSON or XML format, while GraphQL APIs return data exclusively in JSON.","title":"Response Format"},{"location":"concepts/api/02_api_rest_graphql/#data-fetching","text":"With REST, the API developer determines what data is included in each response. In contrast, GraphQL allows the client to specify which fields and related resources it wants in a single query.","title":"Data Fetching"},{"location":"concepts/api/02_api_rest_graphql/#multiple-queries","text":"REST often requires multiple calls to retrieve related resources (e.g., multiple endpoints for different parts of data), while GraphQL enables querying multiple resources within a single request.","title":"Multiple Queries"},{"location":"concepts/api/02_api_rest_graphql/#url-specification","text":"In REST APIs, the URL directly specifies the resource being accessed. GraphQL does not use URLs to define resources but instead relies on a schema with types and fields to guide requests.","title":"URL Specification"},{"location":"concepts/api/02_api_rest_graphql/#client-libraries","text":"REST can be accessed with basic tools such as curl or simple HTTP libraries, while GraphQL often requires specialized support on both the client and server sides, increasing the complexity.","title":"Client Libraries"},{"location":"concepts/api/02_api_rest_graphql/#caching","text":"REST APIs generally make caching easier, as each resource has a unique URL that can be cached. In GraphQL, caching is more challenging due to the flexible query structure, making it difficult to predict resource usage.","title":"Caching"},{"location":"concepts/api/02_api_rest_graphql/#crud-operations","text":"CRUD operations are straightforward in REST with distinct HTTP methods for each operation. In GraphQL, CRUD operations can be more complex but provide greater flexibility, allowing for custom queries and mutations.","title":"CRUD Operations"},{"location":"concepts/api/02_api_rest_graphql/#conclusion","text":"Both REST APIs and GraphQL APIs have their unique advantages and disadvantages. Choosing between them depends on the specific needs of your application, including data retrieval requirements, complexity, and caching strategies. REST APIs are ideal for applications that require a straightforward approach to CRUD operations and can benefit from easy caching. GraphQL APIs are more suitable for applications that need to query multiple resources efficiently and where the client needs to specify exactly what data is required. By understanding these differences, developers can make informed decisions on which approach to take when designing their APIs. Author : Miguel Amaral","title":"Conclusion"},{"location":"concepts/api/03_api_tips_performance/","text":"3 API Performance Optimization Documentation Table of Contents 1. Pagination 2. Asynchronous Logging 3. Caching 4. Payload Compression 5. Connection Pooling Summary Conclusion 1. Pagination Purpose Pagination helps manage and reduce API load by sending only the necessary data per request, avoiding the transfer of large data volumes all at once. Implementation Pagination Types : Offset-based : Sends a limited number of records per page with limit and offset parameters. Example: GET /items?limit=10&offset=20 . Cursor-based : Uses a reference ID for the next page, ensuring better consistency with large datasets. Example: GET /items?cursor=abc123 . Common Parameters : limit : Defines the maximum number of records per page. offset or cursor : Specifies the starting point for the next page of data. Best Practices Limit the maximum number of records per page to prevent large payloads. Allow clients to specify the number of records within a predefined range. 2. Asynchronous Logging Purpose Synchronous logging can be time-consuming and, if done within the request-response cycle, impacts performance. Asynchronous logging helps avoid blocking, freeing the server to respond quickly to the client. Implementation Asynchronous Logging Libraries : Use libraries like asyncio or configure asynchronous logging in frameworks and languages that support it. Logging Queues : Set up a queue system (such as Kafka or RabbitMQ) to record logs outside the request cycle. Best Practices Set log levels properly (debug, info, warning, error) and avoid verbose logging in production. Log only essential information to prevent overload and excessive storage usage. 3. Caching Purpose Caching stores responses for frequently accessed data, reducing server load and decreasing response times. Implementation Cache Layer : Use tools like Redis or Memcached for caching data. HTTP Caching Headers : Cache-Control : Defines cache rules for clients and proxies. ETag : Allows clients to validate if cached data is still valid. Expires : Specifies an expiration date for the cache. Best Practices Consider data volatility when setting cache lifetimes. More dynamic data should have shorter cache durations, while static data can be cached for longer. Set up an invalidation system for frequently changing data to prevent stale cache. 4. Payload Compression Purpose Payload compression reduces the size of data transferred between client and server, speeding up response times and lowering bandwidth usage. Implementation Compression Methods : Enable GZIP or Brotli on the server to compress JSON and other payload formats. Compression Configuration : Most frameworks allow automatic compression for specific content types. Best Practices Compress payloads that exceed a minimum size (e.g., 1 KB) to avoid unnecessary overhead on small data. Ensure clients are compatible with the chosen compression through the Accept-Encoding header in the request. 5. Connection Pooling Purpose Connection pooling improves efficiency by reusing network connections, reducing latency, and enhancing scalability in high-concurrency scenarios. Implementation Libraries and Tools : Use tools that manage pooling automatically, such as requests.Session in Python or database frameworks with built-in connection pooling. Pool Configuration : Set pool size and connection lifetime, adjusting according to system load. Best Practices Monitor pool size and adjust based on load and server resources. Configure timeouts for idle connections to prevent resource waste. Summary Technique Purpose Best Practices Pagination Reduce data per request Limit records per page and use cursors when necessary Asynchronous Logging Avoid blocking in request-response cycle Set appropriate log levels and use logging queues Caching Store frequently accessed responses Set cache lifetimes based on data volatility and implement invalidation Payload Compression Reduce transferred data size Compress larger payloads and verify client compatibility Connection Pooling Reuse connections for efficiency Adjust pool size and monitor idle connections Conclusion Applying these practices helps reduce resource consumption, improve user experience, and increase API scalability. Proper selection and configuration of each technique depend on the specific environment and application requirements.","title":"03 - API Performance"},{"location":"concepts/api/03_api_tips_performance/#3-api-performance-optimization-documentation","text":"","title":"3 API Performance Optimization Documentation"},{"location":"concepts/api/03_api_tips_performance/#table-of-contents","text":"1. Pagination 2. Asynchronous Logging 3. Caching 4. Payload Compression 5. Connection Pooling Summary Conclusion","title":"Table of Contents"},{"location":"concepts/api/03_api_tips_performance/#1-pagination","text":"","title":"1. Pagination"},{"location":"concepts/api/03_api_tips_performance/#purpose","text":"Pagination helps manage and reduce API load by sending only the necessary data per request, avoiding the transfer of large data volumes all at once.","title":"Purpose"},{"location":"concepts/api/03_api_tips_performance/#implementation","text":"Pagination Types : Offset-based : Sends a limited number of records per page with limit and offset parameters. Example: GET /items?limit=10&offset=20 . Cursor-based : Uses a reference ID for the next page, ensuring better consistency with large datasets. Example: GET /items?cursor=abc123 . Common Parameters : limit : Defines the maximum number of records per page. offset or cursor : Specifies the starting point for the next page of data.","title":"Implementation"},{"location":"concepts/api/03_api_tips_performance/#best-practices","text":"Limit the maximum number of records per page to prevent large payloads. Allow clients to specify the number of records within a predefined range.","title":"Best Practices"},{"location":"concepts/api/03_api_tips_performance/#2-asynchronous-logging","text":"","title":"2. Asynchronous Logging"},{"location":"concepts/api/03_api_tips_performance/#purpose_1","text":"Synchronous logging can be time-consuming and, if done within the request-response cycle, impacts performance. Asynchronous logging helps avoid blocking, freeing the server to respond quickly to the client.","title":"Purpose"},{"location":"concepts/api/03_api_tips_performance/#implementation_1","text":"Asynchronous Logging Libraries : Use libraries like asyncio or configure asynchronous logging in frameworks and languages that support it. Logging Queues : Set up a queue system (such as Kafka or RabbitMQ) to record logs outside the request cycle.","title":"Implementation"},{"location":"concepts/api/03_api_tips_performance/#best-practices_1","text":"Set log levels properly (debug, info, warning, error) and avoid verbose logging in production. Log only essential information to prevent overload and excessive storage usage.","title":"Best Practices"},{"location":"concepts/api/03_api_tips_performance/#3-caching","text":"","title":"3. Caching"},{"location":"concepts/api/03_api_tips_performance/#purpose_2","text":"Caching stores responses for frequently accessed data, reducing server load and decreasing response times.","title":"Purpose"},{"location":"concepts/api/03_api_tips_performance/#implementation_2","text":"Cache Layer : Use tools like Redis or Memcached for caching data. HTTP Caching Headers : Cache-Control : Defines cache rules for clients and proxies. ETag : Allows clients to validate if cached data is still valid. Expires : Specifies an expiration date for the cache.","title":"Implementation"},{"location":"concepts/api/03_api_tips_performance/#best-practices_2","text":"Consider data volatility when setting cache lifetimes. More dynamic data should have shorter cache durations, while static data can be cached for longer. Set up an invalidation system for frequently changing data to prevent stale cache.","title":"Best Practices"},{"location":"concepts/api/03_api_tips_performance/#4-payload-compression","text":"","title":"4. Payload Compression"},{"location":"concepts/api/03_api_tips_performance/#purpose_3","text":"Payload compression reduces the size of data transferred between client and server, speeding up response times and lowering bandwidth usage.","title":"Purpose"},{"location":"concepts/api/03_api_tips_performance/#implementation_3","text":"Compression Methods : Enable GZIP or Brotli on the server to compress JSON and other payload formats. Compression Configuration : Most frameworks allow automatic compression for specific content types.","title":"Implementation"},{"location":"concepts/api/03_api_tips_performance/#best-practices_3","text":"Compress payloads that exceed a minimum size (e.g., 1 KB) to avoid unnecessary overhead on small data. Ensure clients are compatible with the chosen compression through the Accept-Encoding header in the request.","title":"Best Practices"},{"location":"concepts/api/03_api_tips_performance/#5-connection-pooling","text":"","title":"5. Connection Pooling"},{"location":"concepts/api/03_api_tips_performance/#purpose_4","text":"Connection pooling improves efficiency by reusing network connections, reducing latency, and enhancing scalability in high-concurrency scenarios.","title":"Purpose"},{"location":"concepts/api/03_api_tips_performance/#implementation_4","text":"Libraries and Tools : Use tools that manage pooling automatically, such as requests.Session in Python or database frameworks with built-in connection pooling. Pool Configuration : Set pool size and connection lifetime, adjusting according to system load.","title":"Implementation"},{"location":"concepts/api/03_api_tips_performance/#best-practices_4","text":"Monitor pool size and adjust based on load and server resources. Configure timeouts for idle connections to prevent resource waste.","title":"Best Practices"},{"location":"concepts/api/03_api_tips_performance/#summary","text":"Technique Purpose Best Practices Pagination Reduce data per request Limit records per page and use cursors when necessary Asynchronous Logging Avoid blocking in request-response cycle Set appropriate log levels and use logging queues Caching Store frequently accessed responses Set cache lifetimes based on data volatility and implement invalidation Payload Compression Reduce transferred data size Compress larger payloads and verify client compatibility Connection Pooling Reuse connections for efficiency Adjust pool size and monitor idle connections","title":"Summary"},{"location":"concepts/api/03_api_tips_performance/#conclusion","text":"Applying these practices helps reduce resource consumption, improve user experience, and increase API scalability. Proper selection and configuration of each technique depend on the specific environment and application requirements.","title":"Conclusion"},{"location":"concepts/data/01_data_modeling/","text":"1. Data Modeling Introduction Data modeling is a fundamental aspect of data engineering, involving the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. Understanding how to organize and structure data is crucial for creating robust and scalable data systems. Key Concepts in Data Modeling 1. Entities Definition: An entity represents a real-world object or concept that has data stored about it. Entities are typically nouns like Customer , Product , Order , etc. Example: In a retail database, entities might include Customer , Order , and Product . Attributes: Entities have attributes, which are characteristics or properties of that entity. For example, a Customer entity might have attributes like CustomerID , Name , and Email . 2. Relationships Definition: Relationships define how entities interact with each other. They describe the connections between different entities. Types of Relationships: One-to-One (1:1): A single entity instance of one type is related to a single entity instance of another type. Example: A Person entity is associated with one Passport . One-to-Many (1:N): A single entity instance of one type is related to multiple instances of another type. Example: A Customer can place multiple Orders . Many-to-Many (M:N): Multiple instances of one entity type are related to multiple instances of another type. Example: Students enroll in Courses , and each Course can have multiple Students . 3. Normalization Definition: Normalization is the process of structuring a relational database in a way that reduces redundancy and dependency by organizing fields and table relationships. Normalization Levels: 1NF (First Normal Form): Ensure that the table is a faithful representation of a relation and eliminate repeating groups (e.g., no arrays or lists as column values). 2NF (Second Normal Form): Achieve 1NF and ensure that all non-key attributes are fully functionally dependent on the primary key. 3NF (Third Normal Form): Achieve 2NF and remove transitive dependencies, meaning no non-key attribute should depend on another non-key attribute. Benefits of Normalization: Reduces data redundancy. Ensures data integrity. Improves query performance by reducing the complexity of queries. 4. Entity-Relationship (ER) Models Definition: The Entity-Relationship model is a graphical representation of entities and their relationships to each other, typically used in database design. Components: Entities: Represented as rectangles. Relationships: Represented as diamonds. Attributes: Represented as ovals. ER Diagram: ER diagrams are visual tools used to model the structure of a database. They show how entities are related to each other and the nature of these relationships (e.g., 1:1, 1:N, M:N). 5. Data Types and Constraints Data Types: Define the nature of the data that can be stored in a field, such as integers, decimals, strings, dates, etc. Constraints: Rules enforced on data columns on the table. Types of constraints include: Primary Key: Ensures that each record in a table is unique. Foreign Key: Ensures referential integrity between two tables. Unique Constraint: Ensures that all values in a column are different. Check Constraint: Ensures that all values in a column satisfy a specific condition. Importance of Data Modeling Improves Communication: Data models serve as a blueprint for developers, data architects, and stakeholders to discuss the database structure. Ensures Data Quality: By creating a well-structured data model, data consistency and integrity are maintained, reducing the likelihood of errors. Facilitates Database Design: Helps in creating an efficient database design that meets the requirements of the business. Scalability: A well-designed data model can easily accommodate growth in data volume or changes in business requirements. Conclusion Data modeling is a critical step in the design and implementation of databases. It provides a structured approach to define and organize data, ensuring that data is stored efficiently and retrieved effectively. As a data engineer, mastering data modeling techniques is essential for building scalable and reliable data systems.","title":"01 - Data Modeling"},{"location":"concepts/data/01_data_modeling/#1-data-modeling","text":"","title":"1. Data Modeling"},{"location":"concepts/data/01_data_modeling/#introduction","text":"Data modeling is a fundamental aspect of data engineering, involving the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. Understanding how to organize and structure data is crucial for creating robust and scalable data systems.","title":"Introduction"},{"location":"concepts/data/01_data_modeling/#key-concepts-in-data-modeling","text":"","title":"Key Concepts in Data Modeling"},{"location":"concepts/data/01_data_modeling/#1-entities","text":"Definition: An entity represents a real-world object or concept that has data stored about it. Entities are typically nouns like Customer , Product , Order , etc. Example: In a retail database, entities might include Customer , Order , and Product . Attributes: Entities have attributes, which are characteristics or properties of that entity. For example, a Customer entity might have attributes like CustomerID , Name , and Email .","title":"1. Entities"},{"location":"concepts/data/01_data_modeling/#2-relationships","text":"Definition: Relationships define how entities interact with each other. They describe the connections between different entities. Types of Relationships: One-to-One (1:1): A single entity instance of one type is related to a single entity instance of another type. Example: A Person entity is associated with one Passport . One-to-Many (1:N): A single entity instance of one type is related to multiple instances of another type. Example: A Customer can place multiple Orders . Many-to-Many (M:N): Multiple instances of one entity type are related to multiple instances of another type. Example: Students enroll in Courses , and each Course can have multiple Students .","title":"2. Relationships"},{"location":"concepts/data/01_data_modeling/#3-normalization","text":"Definition: Normalization is the process of structuring a relational database in a way that reduces redundancy and dependency by organizing fields and table relationships. Normalization Levels: 1NF (First Normal Form): Ensure that the table is a faithful representation of a relation and eliminate repeating groups (e.g., no arrays or lists as column values). 2NF (Second Normal Form): Achieve 1NF and ensure that all non-key attributes are fully functionally dependent on the primary key. 3NF (Third Normal Form): Achieve 2NF and remove transitive dependencies, meaning no non-key attribute should depend on another non-key attribute. Benefits of Normalization: Reduces data redundancy. Ensures data integrity. Improves query performance by reducing the complexity of queries.","title":"3. Normalization"},{"location":"concepts/data/01_data_modeling/#4-entity-relationship-er-models","text":"Definition: The Entity-Relationship model is a graphical representation of entities and their relationships to each other, typically used in database design. Components: Entities: Represented as rectangles. Relationships: Represented as diamonds. Attributes: Represented as ovals. ER Diagram: ER diagrams are visual tools used to model the structure of a database. They show how entities are related to each other and the nature of these relationships (e.g., 1:1, 1:N, M:N).","title":"4. Entity-Relationship (ER) Models"},{"location":"concepts/data/01_data_modeling/#5-data-types-and-constraints","text":"Data Types: Define the nature of the data that can be stored in a field, such as integers, decimals, strings, dates, etc. Constraints: Rules enforced on data columns on the table. Types of constraints include: Primary Key: Ensures that each record in a table is unique. Foreign Key: Ensures referential integrity between two tables. Unique Constraint: Ensures that all values in a column are different. Check Constraint: Ensures that all values in a column satisfy a specific condition.","title":"5. Data Types and Constraints"},{"location":"concepts/data/01_data_modeling/#importance-of-data-modeling","text":"Improves Communication: Data models serve as a blueprint for developers, data architects, and stakeholders to discuss the database structure. Ensures Data Quality: By creating a well-structured data model, data consistency and integrity are maintained, reducing the likelihood of errors. Facilitates Database Design: Helps in creating an efficient database design that meets the requirements of the business. Scalability: A well-designed data model can easily accommodate growth in data volume or changes in business requirements.","title":"Importance of Data Modeling"},{"location":"concepts/data/01_data_modeling/#conclusion","text":"Data modeling is a critical step in the design and implementation of databases. It provides a structured approach to define and organize data, ensuring that data is stored efficiently and retrieved effectively. As a data engineer, mastering data modeling techniques is essential for building scalable and reliable data systems.","title":"Conclusion"},{"location":"concepts/data/02_data_structures/","text":"2. Data Structures Overview Data structures are specialized formats for organizing, processing, retrieving, and storing data. They play a crucial role in optimizing data manipulation and access efficiency, impacting performance in terms of search, insertion, and update operations. Why? Understanding how data can be organized efficiently in code will enhance your ability to manipulate and access data effectively. Objectives Facilitate Efficient Data Manipulation: Improve the performance of operations such as searching, inserting, and updating data. Optimize Operations: Choose the appropriate data structure to match the requirements of different operations and use cases. Key Concepts 1. Arrays Description: An array is a collection of elements identified by index or key. All elements in an array are of the same type. Usage: Best for fixed-size collections and quick access to elements via index. Operations: Access, update, insert, and delete operations are generally O(1) for arrays. 2. Linked Lists Description: A linked list is a linear collection of elements, where each element (node) contains a reference (link) to the next node in the sequence. Usage: Suitable for dynamic size collections where elements are frequently inserted or deleted. Operations: Insertions and deletions are O(1) if the node reference is known; access and search operations are O(n). 3. Trees Description: A tree is a hierarchical structure consisting of nodes, with each node containing a value and references to child nodes. The top node is called the root. Usage: Ideal for hierarchical data and applications like file systems and databases. Types: Binary Trees: Each node has at most two children. Binary Search Trees (BST): Nodes are arranged in a way that for each node, all elements in the left subtree are less and all in the right subtree are greater. Balanced Trees: Like AVL or Red-Black Trees, where balance is maintained to ensure O(log n) operations. Operations: Search, insertion, and deletion operations are O(log n) for balanced trees. 4. Graphs Description: A graph is a collection of nodes (vertices) connected by edges. Graphs can be directed or undirected. Usage: Useful for representing relationships between entities, such as social networks, web page links, and more. Types: Directed Graphs: Edges have a direction. Undirected Graphs: Edges do not have a direction. Operations: Graph traversal algorithms like Depth-First Search (DFS) and Breadth-First Search (BFS) are commonly used. 5. Hash Tables Description: A hash table stores data in an associative manner using a hash function to map keys to values. It provides a very efficient way of indexing and retrieving data. Usage: Excellent for scenarios requiring fast lookup and retrieval operations. Operations: Average time complexity for insertion, deletion, and lookup is O(1), although worst-case complexity can be O(n) due to collisions. Conclusion Understanding data structures is fundamental for efficient programming. Each structure offers unique advantages and is suited to different types of operations. By mastering these concepts, you can write code that is more efficient and scalable.","title":"02 - Data Structures"},{"location":"concepts/data/02_data_structures/#2-data-structures","text":"","title":"2. Data Structures"},{"location":"concepts/data/02_data_structures/#overview","text":"Data structures are specialized formats for organizing, processing, retrieving, and storing data. They play a crucial role in optimizing data manipulation and access efficiency, impacting performance in terms of search, insertion, and update operations.","title":"Overview"},{"location":"concepts/data/02_data_structures/#why","text":"Understanding how data can be organized efficiently in code will enhance your ability to manipulate and access data effectively.","title":"Why?"},{"location":"concepts/data/02_data_structures/#objectives","text":"Facilitate Efficient Data Manipulation: Improve the performance of operations such as searching, inserting, and updating data. Optimize Operations: Choose the appropriate data structure to match the requirements of different operations and use cases.","title":"Objectives"},{"location":"concepts/data/02_data_structures/#key-concepts","text":"","title":"Key Concepts"},{"location":"concepts/data/02_data_structures/#1-arrays","text":"Description: An array is a collection of elements identified by index or key. All elements in an array are of the same type. Usage: Best for fixed-size collections and quick access to elements via index. Operations: Access, update, insert, and delete operations are generally O(1) for arrays.","title":"1. Arrays"},{"location":"concepts/data/02_data_structures/#2-linked-lists","text":"Description: A linked list is a linear collection of elements, where each element (node) contains a reference (link) to the next node in the sequence. Usage: Suitable for dynamic size collections where elements are frequently inserted or deleted. Operations: Insertions and deletions are O(1) if the node reference is known; access and search operations are O(n).","title":"2. Linked Lists"},{"location":"concepts/data/02_data_structures/#3-trees","text":"Description: A tree is a hierarchical structure consisting of nodes, with each node containing a value and references to child nodes. The top node is called the root. Usage: Ideal for hierarchical data and applications like file systems and databases. Types: Binary Trees: Each node has at most two children. Binary Search Trees (BST): Nodes are arranged in a way that for each node, all elements in the left subtree are less and all in the right subtree are greater. Balanced Trees: Like AVL or Red-Black Trees, where balance is maintained to ensure O(log n) operations. Operations: Search, insertion, and deletion operations are O(log n) for balanced trees.","title":"3. Trees"},{"location":"concepts/data/02_data_structures/#4-graphs","text":"Description: A graph is a collection of nodes (vertices) connected by edges. Graphs can be directed or undirected. Usage: Useful for representing relationships between entities, such as social networks, web page links, and more. Types: Directed Graphs: Edges have a direction. Undirected Graphs: Edges do not have a direction. Operations: Graph traversal algorithms like Depth-First Search (DFS) and Breadth-First Search (BFS) are commonly used.","title":"4. Graphs"},{"location":"concepts/data/02_data_structures/#5-hash-tables","text":"Description: A hash table stores data in an associative manner using a hash function to map keys to values. It provides a very efficient way of indexing and retrieving data. Usage: Excellent for scenarios requiring fast lookup and retrieval operations. Operations: Average time complexity for insertion, deletion, and lookup is O(1), although worst-case complexity can be O(n) due to collisions.","title":"5. Hash Tables"},{"location":"concepts/data/02_data_structures/#conclusion","text":"Understanding data structures is fundamental for efficient programming. Each structure offers unique advantages and is suited to different types of operations. By mastering these concepts, you can write code that is more efficient and scalable.","title":"Conclusion"},{"location":"concepts/data/03_data_types/","text":"3. Data Types Overview Data types refer to the categories of data that determine what kind of value can be stored and manipulated within a program. Understanding data types is crucial for ensuring that data is stored correctly and for performing accurate operations. When to Study Data types can be studied either in parallel with or after Data Structures. A solid understanding of data types enhances your ability to work with different kinds of data and to implement operations effectively. Objectives Ensure Correct Data Storage: Properly store and manage various types of data. Perform Accurate Operations: Carry out operations on data with appropriate precision and correctness. Key Concepts 1. Primitive Data Types Description: Basic types of data built into a programming language. They represent single values and have a fixed size. Examples: Integer (int): Represents whole numbers. E.g., 5 , -23 . Floating Point (float, double): Represents real numbers with decimal points. E.g., 3.14 , -0.001 . Character (char): Represents a single character. E.g., 'A' , 'z' . Boolean (bool): Represents true or false values. E.g., true , false . 2. Composite Data Types Description: Data types that are composed of multiple primitive types. Examples: String: Represents a sequence of characters. E.g., \"Hello, World!\" . Array: Represents a collection of elements of the same type. E.g., [1, 2, 3, 4] . Struct (C/C++), Class (Object-Oriented Languages): User-defined types that group together different data types. E.g., a Person struct with name and age fields. 3. Abstract Data Types Description: Data types that are defined by their behavior (operations) rather than their implementation. Examples: List: An ordered collection of elements that can be dynamically sized. Queue: A collection that follows the FIFO (First-In-First-Out) principle. Stack: A collection that follows the LIFO (Last-In-First-Out) principle. Map (or Dictionary): A collection of key-value pairs. 4. Special Data Types Description: Data types used for specific purposes or with special constraints. Examples: Date and Time: Represents dates and times. E.g., 2024-08-29 , 12:00:00 . Enumeration (enum): Represents a set of named values. E.g., enum Days { Sunday, Monday, Tuesday, ... } . Null: Represents the absence of a value. E.g., null in Java or None in Python. Conclusion Understanding data types is essential for correct data handling and manipulation. By mastering data types, you can ensure that your programs store and process data accurately and efficiently.","title":"03 - Data Types"},{"location":"concepts/data/03_data_types/#3-data-types","text":"","title":"3. Data Types"},{"location":"concepts/data/03_data_types/#overview","text":"Data types refer to the categories of data that determine what kind of value can be stored and manipulated within a program. Understanding data types is crucial for ensuring that data is stored correctly and for performing accurate operations.","title":"Overview"},{"location":"concepts/data/03_data_types/#when-to-study","text":"Data types can be studied either in parallel with or after Data Structures. A solid understanding of data types enhances your ability to work with different kinds of data and to implement operations effectively.","title":"When to Study"},{"location":"concepts/data/03_data_types/#objectives","text":"Ensure Correct Data Storage: Properly store and manage various types of data. Perform Accurate Operations: Carry out operations on data with appropriate precision and correctness.","title":"Objectives"},{"location":"concepts/data/03_data_types/#key-concepts","text":"","title":"Key Concepts"},{"location":"concepts/data/03_data_types/#1-primitive-data-types","text":"Description: Basic types of data built into a programming language. They represent single values and have a fixed size. Examples: Integer (int): Represents whole numbers. E.g., 5 , -23 . Floating Point (float, double): Represents real numbers with decimal points. E.g., 3.14 , -0.001 . Character (char): Represents a single character. E.g., 'A' , 'z' . Boolean (bool): Represents true or false values. E.g., true , false .","title":"1. Primitive Data Types"},{"location":"concepts/data/03_data_types/#2-composite-data-types","text":"Description: Data types that are composed of multiple primitive types. Examples: String: Represents a sequence of characters. E.g., \"Hello, World!\" . Array: Represents a collection of elements of the same type. E.g., [1, 2, 3, 4] . Struct (C/C++), Class (Object-Oriented Languages): User-defined types that group together different data types. E.g., a Person struct with name and age fields.","title":"2. Composite Data Types"},{"location":"concepts/data/03_data_types/#3-abstract-data-types","text":"Description: Data types that are defined by their behavior (operations) rather than their implementation. Examples: List: An ordered collection of elements that can be dynamically sized. Queue: A collection that follows the FIFO (First-In-First-Out) principle. Stack: A collection that follows the LIFO (Last-In-First-Out) principle. Map (or Dictionary): A collection of key-value pairs.","title":"3. Abstract Data Types"},{"location":"concepts/data/03_data_types/#4-special-data-types","text":"Description: Data types used for specific purposes or with special constraints. Examples: Date and Time: Represents dates and times. E.g., 2024-08-29 , 12:00:00 . Enumeration (enum): Represents a set of named values. E.g., enum Days { Sunday, Monday, Tuesday, ... } . Null: Represents the absence of a value. E.g., null in Java or None in Python.","title":"4. Special Data Types"},{"location":"concepts/data/03_data_types/#conclusion","text":"Understanding data types is essential for correct data handling and manipulation. By mastering data types, you can ensure that your programs store and process data accurately and efficiently.","title":"Conclusion"},{"location":"concepts/data/04_data_structured/","text":"4. Structured Data Introduction Structured data refers to data that is highly organized and formatted in a way that is easily searchable and accessible by both humans and computers. This type of data is typically stored in tabular formats with rows and columns, such as in databases or spreadsheets. Structured data is the foundation of traditional data management systems, and it plays a crucial role in applications where data integrity and efficiency are paramount. Learning Objectives By the end of this lesson on structured data, you should be able to: Understand what structured data is and how it differs from other data types. Identify the key characteristics of structured data. Recognize the advantages and limitations of using structured data. Apply knowledge of structured data to practical examples, such as writing SQL queries. What is Structured Data? Structured data is data that is organized into a well-defined model or schema, making it easy to store, search, and analyze. Each piece of structured data is stored in a specific field, and the structure of these fields is consistent across all records in the dataset. This consistency allows for efficient querying and manipulation of the data. Key Characteristics Rigid Schema: Structured data follows a predefined schema or model, meaning each data point conforms to a specific format and type (e.g., integers, dates, strings). Tabular Format: Typically organized in tables with rows and columns, where each column represents a different attribute, and each row represents a different record. Data Types: Common data types in structured data include integers, floats, dates, and strings, each defining the kind of data that can be stored in a particular field. Accessibility: The organization of structured data allows for efficient querying using languages like SQL (Structured Query Language). Storage: Structured data is often stored in relational databases such as MySQL, PostgreSQL, or Oracle, which are designed to maintain data consistency and integrity. Advantages Efficient Querying: The predefined structure allows for fast and efficient data retrieval using indexing and optimized query paths. Data Integrity: The strict schema ensures data consistency, accuracy, and validation, which are essential for maintaining data quality. Ease of Analysis: Structured data can be easily analyzed using standard analytical tools, making it suitable for reporting, data visualization, and statistical analysis. Limitations Inflexibility: Changes to the data structure, such as adding new fields or altering existing ones, require modifications to the schema, which can be time-consuming and complex. Scalability Issues: As the volume of structured data grows, scaling relational databases to handle larger datasets can become challenging and may require significant resources. Limited Complexity: Structured data is ideal for simple, predictable data models but may struggle with representing more complex, hierarchical, or interconnected data. Examples Relational Databases: Structured data is most commonly found in relational databases, where information is stored in tables. For example, a SQL database that stores customer information might have tables for Customers , Orders , and Products . Spreadsheets: Another common example of structured data is a spreadsheet, where data is organized in rows and columns. Financial data, such as budgets or expense reports, is often managed in Excel or Google Sheets. Sensor Data: Data collected from IoT (Internet of Things) devices, such as temperature readings or humidity levels, is typically structured and stored in databases for easy querying and analysis. Practical Exercise Imagine you are working with a relational database that stores employee information. The database includes tables such as Employees , Departments , and Salaries . Try to write a simple SQL query to retrieve all employees in the \"Marketing\" department who earn more than $50,000. SELECT EmployeeName, Department, Salary FROM Employees WHERE Department = 'Marketing' AND Salary > 50000; In this exercise, the SQL query selects the EmployeeName, Department, and Salary from the Employees table, filtering the results to show only those employees in the \"Marketing\" department with a salary greater than $50,000. Conclusion Structured data is the backbone of traditional data management systems, providing an organized, efficient, and reliable way to store and retrieve data. Its rigid schema and ease of querying make it ideal for applications requiring precise and consistent data management. However, it also comes with limitations in flexibility and scalability, which are important considerations when designing systems to handle large or complex datasets","title":"04 - Data Structured"},{"location":"concepts/data/04_data_structured/#4-structured-data","text":"","title":"4. Structured Data"},{"location":"concepts/data/04_data_structured/#introduction","text":"Structured data refers to data that is highly organized and formatted in a way that is easily searchable and accessible by both humans and computers. This type of data is typically stored in tabular formats with rows and columns, such as in databases or spreadsheets. Structured data is the foundation of traditional data management systems, and it plays a crucial role in applications where data integrity and efficiency are paramount.","title":"Introduction"},{"location":"concepts/data/04_data_structured/#learning-objectives","text":"By the end of this lesson on structured data, you should be able to: Understand what structured data is and how it differs from other data types. Identify the key characteristics of structured data. Recognize the advantages and limitations of using structured data. Apply knowledge of structured data to practical examples, such as writing SQL queries.","title":"Learning Objectives"},{"location":"concepts/data/04_data_structured/#what-is-structured-data","text":"Structured data is data that is organized into a well-defined model or schema, making it easy to store, search, and analyze. Each piece of structured data is stored in a specific field, and the structure of these fields is consistent across all records in the dataset. This consistency allows for efficient querying and manipulation of the data.","title":"What is Structured Data?"},{"location":"concepts/data/04_data_structured/#key-characteristics","text":"Rigid Schema: Structured data follows a predefined schema or model, meaning each data point conforms to a specific format and type (e.g., integers, dates, strings). Tabular Format: Typically organized in tables with rows and columns, where each column represents a different attribute, and each row represents a different record. Data Types: Common data types in structured data include integers, floats, dates, and strings, each defining the kind of data that can be stored in a particular field. Accessibility: The organization of structured data allows for efficient querying using languages like SQL (Structured Query Language). Storage: Structured data is often stored in relational databases such as MySQL, PostgreSQL, or Oracle, which are designed to maintain data consistency and integrity.","title":"Key Characteristics"},{"location":"concepts/data/04_data_structured/#advantages","text":"Efficient Querying: The predefined structure allows for fast and efficient data retrieval using indexing and optimized query paths. Data Integrity: The strict schema ensures data consistency, accuracy, and validation, which are essential for maintaining data quality. Ease of Analysis: Structured data can be easily analyzed using standard analytical tools, making it suitable for reporting, data visualization, and statistical analysis.","title":"Advantages"},{"location":"concepts/data/04_data_structured/#limitations","text":"Inflexibility: Changes to the data structure, such as adding new fields or altering existing ones, require modifications to the schema, which can be time-consuming and complex. Scalability Issues: As the volume of structured data grows, scaling relational databases to handle larger datasets can become challenging and may require significant resources. Limited Complexity: Structured data is ideal for simple, predictable data models but may struggle with representing more complex, hierarchical, or interconnected data.","title":"Limitations"},{"location":"concepts/data/04_data_structured/#examples","text":"Relational Databases: Structured data is most commonly found in relational databases, where information is stored in tables. For example, a SQL database that stores customer information might have tables for Customers , Orders , and Products . Spreadsheets: Another common example of structured data is a spreadsheet, where data is organized in rows and columns. Financial data, such as budgets or expense reports, is often managed in Excel or Google Sheets. Sensor Data: Data collected from IoT (Internet of Things) devices, such as temperature readings or humidity levels, is typically structured and stored in databases for easy querying and analysis.","title":"Examples"},{"location":"concepts/data/04_data_structured/#practical-exercise","text":"Imagine you are working with a relational database that stores employee information. The database includes tables such as Employees , Departments , and Salaries . Try to write a simple SQL query to retrieve all employees in the \"Marketing\" department who earn more than $50,000. SELECT EmployeeName, Department, Salary FROM Employees WHERE Department = 'Marketing' AND Salary > 50000; In this exercise, the SQL query selects the EmployeeName, Department, and Salary from the Employees table, filtering the results to show only those employees in the \"Marketing\" department with a salary greater than $50,000.","title":"Practical Exercise"},{"location":"concepts/data/04_data_structured/#conclusion","text":"Structured data is the backbone of traditional data management systems, providing an organized, efficient, and reliable way to store and retrieve data. Its rigid schema and ease of querying make it ideal for applications requiring precise and consistent data management. However, it also comes with limitations in flexibility and scalability, which are important considerations when designing systems to handle large or complex datasets","title":"Conclusion"},{"location":"concepts/data/05_data_semi_structured/","text":"5. Semi-Structured Data Introduction Semi-structured data lies between structured and unstructured data. While it does not conform to a rigid, predefined schema like structured data, it still contains organizational elements such as tags or markers that separate different data fields and records. This flexibility allows for the storage and exchange of complex data in a more adaptable format, which can be easily parsed and processed by computers. Learning Objectives By the end of this lesson on semi-structured data, you should be able to: Define what semi-structured data is and how it differs from structured and unstructured data. Identify the key characteristics of semi-structured data. Recognize the advantages and limitations of using semi-structured data. Apply knowledge of semi-structured data to real-world formats like XML and JSON. What is Semi-Structured Data? Semi-structured data refers to data that does not adhere to a strict, predefined schema but still possesses some organizational structure that allows for the efficient storage, search, and retrieval of data. Unlike structured data, semi-structured data allows for varying formats and types within the same dataset, making it more flexible. However, it retains tags or markers that provide a framework for organizing and categorizing data elements. Key Characteristics Flexible Schema: Semi-structured data does not require a fixed schema. Instead, it uses tags or markers (like in XML or JSON) to define and separate data elements. The structure can vary between records. Hierarchical Structure: Often organized in a hierarchical or tree-like format, which allows for nested data. For example, a JSON object might contain arrays or other objects as values. Data Elements: The data may contain structured elements (e.g., key-value pairs) mixed with unstructured elements (e.g., free text). Interchange Formats: Commonly used formats for semi-structured data include XML (Extensible Markup Language) and JSON (JavaScript Object Notation), both of which are widely used for data interchange on the web. Storage: Semi-structured data is typically stored in NoSQL databases (e.g., MongoDB, CouchDB) or as files (e.g., XML, JSON documents). Advantages Flexibility: The absence of a rigid schema allows semi-structured data to accommodate a wide variety of data types and structures without requiring major changes. Interoperability: Formats like XML and JSON are widely supported across different programming languages and systems, making data exchange between systems easier. Ease of Parsing: Semi-structured data can be parsed and processed using specialized tools and languages (e.g., XPath for XML, JSONPath for JSON), making it easier to extract and manipulate information. Limitations Complexity in Querying: Although more flexible, querying semi-structured data can be more complex than querying structured data, often requiring specialized query languages or tools. Performance: The lack of a strict schema can lead to slower performance in data retrieval and manipulation, especially with large datasets. Data Consistency: Without a rigid schema, ensuring data consistency and integrity can be more challenging, as different records may not adhere to the same structure. Examples XML (Extensible Markup Language): XML is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. It is often used for configuration files, data exchange between web services, and document storage. <book> <title>Effective Java</title> <author>Joshua Bloch</author> <publicationYear>2008</publicationYear> </book> JSON (JavaScript Object Notation): JSON is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate. JSON is commonly used in web applications to transmit data between a server and a client. { \"title\": \"Effective Java\", \"author\": \"Joshua Bloch\", \"publicationYear\": 2008 } Email: Emails often contain semi-structured data. The headers (e.g., To, From, Subject, Date) are structured, while the body of the email may be unstructured text. Conclusion Semi-structured data provides a middle ground between the rigidity of structured data and the flexibility of unstructured data. It allows for the storage and exchange of complex data formats while still retaining some level of organization, making it easier to manage and analyze. However, working with semi-structured data requires specialized tools and techniques, especially for querying and ensuring data consistency. By understanding and utilizing semi-structured data, you can take advantage of its flexibility to handle a wider range of data formats and use cases, particularly in scenarios where data structures are not fixed or predefined.","title":"05 - Data Semi-Structured"},{"location":"concepts/data/05_data_semi_structured/#5-semi-structured-data","text":"","title":"5. Semi-Structured Data"},{"location":"concepts/data/05_data_semi_structured/#introduction","text":"Semi-structured data lies between structured and unstructured data. While it does not conform to a rigid, predefined schema like structured data, it still contains organizational elements such as tags or markers that separate different data fields and records. This flexibility allows for the storage and exchange of complex data in a more adaptable format, which can be easily parsed and processed by computers.","title":"Introduction"},{"location":"concepts/data/05_data_semi_structured/#learning-objectives","text":"By the end of this lesson on semi-structured data, you should be able to: Define what semi-structured data is and how it differs from structured and unstructured data. Identify the key characteristics of semi-structured data. Recognize the advantages and limitations of using semi-structured data. Apply knowledge of semi-structured data to real-world formats like XML and JSON.","title":"Learning Objectives"},{"location":"concepts/data/05_data_semi_structured/#what-is-semi-structured-data","text":"Semi-structured data refers to data that does not adhere to a strict, predefined schema but still possesses some organizational structure that allows for the efficient storage, search, and retrieval of data. Unlike structured data, semi-structured data allows for varying formats and types within the same dataset, making it more flexible. However, it retains tags or markers that provide a framework for organizing and categorizing data elements.","title":"What is Semi-Structured Data?"},{"location":"concepts/data/05_data_semi_structured/#key-characteristics","text":"Flexible Schema: Semi-structured data does not require a fixed schema. Instead, it uses tags or markers (like in XML or JSON) to define and separate data elements. The structure can vary between records. Hierarchical Structure: Often organized in a hierarchical or tree-like format, which allows for nested data. For example, a JSON object might contain arrays or other objects as values. Data Elements: The data may contain structured elements (e.g., key-value pairs) mixed with unstructured elements (e.g., free text). Interchange Formats: Commonly used formats for semi-structured data include XML (Extensible Markup Language) and JSON (JavaScript Object Notation), both of which are widely used for data interchange on the web. Storage: Semi-structured data is typically stored in NoSQL databases (e.g., MongoDB, CouchDB) or as files (e.g., XML, JSON documents).","title":"Key Characteristics"},{"location":"concepts/data/05_data_semi_structured/#advantages","text":"Flexibility: The absence of a rigid schema allows semi-structured data to accommodate a wide variety of data types and structures without requiring major changes. Interoperability: Formats like XML and JSON are widely supported across different programming languages and systems, making data exchange between systems easier. Ease of Parsing: Semi-structured data can be parsed and processed using specialized tools and languages (e.g., XPath for XML, JSONPath for JSON), making it easier to extract and manipulate information.","title":"Advantages"},{"location":"concepts/data/05_data_semi_structured/#limitations","text":"Complexity in Querying: Although more flexible, querying semi-structured data can be more complex than querying structured data, often requiring specialized query languages or tools. Performance: The lack of a strict schema can lead to slower performance in data retrieval and manipulation, especially with large datasets. Data Consistency: Without a rigid schema, ensuring data consistency and integrity can be more challenging, as different records may not adhere to the same structure.","title":"Limitations"},{"location":"concepts/data/05_data_semi_structured/#examples","text":"XML (Extensible Markup Language): XML is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. It is often used for configuration files, data exchange between web services, and document storage. <book> <title>Effective Java</title> <author>Joshua Bloch</author> <publicationYear>2008</publicationYear> </book> JSON (JavaScript Object Notation): JSON is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate. JSON is commonly used in web applications to transmit data between a server and a client. { \"title\": \"Effective Java\", \"author\": \"Joshua Bloch\", \"publicationYear\": 2008 } Email: Emails often contain semi-structured data. The headers (e.g., To, From, Subject, Date) are structured, while the body of the email may be unstructured text.","title":"Examples"},{"location":"concepts/data/05_data_semi_structured/#conclusion","text":"Semi-structured data provides a middle ground between the rigidity of structured data and the flexibility of unstructured data. It allows for the storage and exchange of complex data formats while still retaining some level of organization, making it easier to manage and analyze. However, working with semi-structured data requires specialized tools and techniques, especially for querying and ensuring data consistency. By understanding and utilizing semi-structured data, you can take advantage of its flexibility to handle a wider range of data formats and use cases, particularly in scenarios where data structures are not fixed or predefined.","title":"Conclusion"},{"location":"concepts/data/06_data_unstructured/","text":"6. Unstructured Data Introduction Unstructured data refers to information that does not follow a predefined data model or schema. This type of data is typically text-heavy, but it can also include images, videos, and other multimedia. Unlike structured or semi-structured data, unstructured data lacks a specific format, making it more difficult to store, manage, and analyze using traditional data processing methods. However, it represents the majority of data generated and stored today, particularly in the context of big data and advanced analytics. Learning Objectives By the end of this lesson on unstructured data, you should be able to: Define what unstructured data is and distinguish it from structured and semi-structured data. Identify the key characteristics of unstructured data. Recognize the challenges and limitations of working with unstructured data. Understand the tools and techniques used to analyze and process unstructured data. What is Unstructured Data? Unstructured data refers to any data that does not conform to a specific, organized format or model. This type of data is often generated by humans and can take many forms, including text documents, emails, social media posts, images, audio files, and videos. Unlike structured data, which is neatly organized into tables and columns, unstructured data does not have a predefined data model, making it more complex to manage and analyze. Key Characteristics Lack of Structure: Unstructured data does not have a predefined schema or format. It can be text-heavy, with irregularities and ambiguities that make it difficult to fit into a relational database. Variety: Unstructured data comes in many forms, including text (e.g., documents, emails), multimedia (e.g., images, audio, video), and other formats (e.g., PDFs, sensor data). Volume: Unstructured data accounts for a significant portion of the data generated and stored in today's digital world, particularly with the rise of the internet, social media, and IoT (Internet of Things) devices. Complexity: The analysis of unstructured data requires advanced techniques such as natural language processing (NLP), image recognition, and machine learning. Advantages Richness of Information: Unstructured data can capture a wide range of information, including nuances, context, and detailed descriptions, which structured data might miss. Flexibility: Since unstructured data is not bound by a predefined schema, it can easily accommodate various types of information, allowing for more comprehensive data collection. Potential for Insights: With the right tools, unstructured data can be mined for valuable insights that structured data might not reveal, such as customer sentiment, trends, and patterns. Limitations Difficulty in Storage and Management: Traditional databases are not designed to handle unstructured data, making storage and management more complex. Challenges in Analysis: Analyzing unstructured data requires specialized tools and techniques, which can be resource-intensive and require expertise in fields like machine learning, NLP, and big data analytics. Data Quality: Unstructured data often contains noise, such as irrelevant or redundant information, which can complicate analysis and lead to less accurate results. Examples Text Documents: Emails, reports, and other documents that contain text with no fixed structure. Social Media: Posts, comments, and reviews on platforms like Twitter, Facebook, and Instagram are classic examples of unstructured data. Multimedia Files: Photos, videos, and audio recordings that lack a standard format for data storage. Sensor Data: Data collected from IoT devices, such as temperature readings or GPS data, often comes in an unstructured format. Practical Exercise Consider a large set of customer reviews from an e-commerce site stored as unstructured text. Write a Python script using the nltk library (Natural Language Toolkit) to perform basic sentiment analysis on these reviews. import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sample unstructured data: customer reviews reviews = [ \"This product is fantastic! I've never been happier.\", \"Terrible experience. The product broke after one use.\", \"Good quality, but the price is too high.\", \"Excellent service, quick delivery, and a great product.\" ] # Initialize the sentiment analyzer nltk.download('vader_lexicon') sid = SentimentIntensityAnalyzer() # Perform sentiment analysis for review in reviews: sentiment_scores = sid.polarity_scores(review) print(f\"Review: {review}\") print(f\"Sentiment Scores: {sentiment_scores}\\n\") In this exercise, the Python script uses the nltk library to analyze the sentiment of each customer review, providing insights into whether the feedback is positive, negative, or neutral. This demonstrates one approach to processing and analyzing unstructured data. Conclusion Unstructured data represents a vast, complex, and valuable source of information that, while challenging to manage and analyze, holds the potential for deep insights and new opportunities. As the volume of unstructured data continues to grow, mastering the tools and techniques to harness this data is increasingly important. Whether through text analysis, image recognition, or machine learning, the ability to extract meaning from unstructured data is a key skill in the modern data landscape. By understanding the nature of unstructured data and the methods available for working with it, you can unlock its full potential and apply it to a wide range of real-world scenarios.","title":"06 - Data Unstructured"},{"location":"concepts/data/06_data_unstructured/#6-unstructured-data","text":"","title":"6. Unstructured Data"},{"location":"concepts/data/06_data_unstructured/#introduction","text":"Unstructured data refers to information that does not follow a predefined data model or schema. This type of data is typically text-heavy, but it can also include images, videos, and other multimedia. Unlike structured or semi-structured data, unstructured data lacks a specific format, making it more difficult to store, manage, and analyze using traditional data processing methods. However, it represents the majority of data generated and stored today, particularly in the context of big data and advanced analytics.","title":"Introduction"},{"location":"concepts/data/06_data_unstructured/#learning-objectives","text":"By the end of this lesson on unstructured data, you should be able to: Define what unstructured data is and distinguish it from structured and semi-structured data. Identify the key characteristics of unstructured data. Recognize the challenges and limitations of working with unstructured data. Understand the tools and techniques used to analyze and process unstructured data.","title":"Learning Objectives"},{"location":"concepts/data/06_data_unstructured/#what-is-unstructured-data","text":"Unstructured data refers to any data that does not conform to a specific, organized format or model. This type of data is often generated by humans and can take many forms, including text documents, emails, social media posts, images, audio files, and videos. Unlike structured data, which is neatly organized into tables and columns, unstructured data does not have a predefined data model, making it more complex to manage and analyze.","title":"What is Unstructured Data?"},{"location":"concepts/data/06_data_unstructured/#key-characteristics","text":"Lack of Structure: Unstructured data does not have a predefined schema or format. It can be text-heavy, with irregularities and ambiguities that make it difficult to fit into a relational database. Variety: Unstructured data comes in many forms, including text (e.g., documents, emails), multimedia (e.g., images, audio, video), and other formats (e.g., PDFs, sensor data). Volume: Unstructured data accounts for a significant portion of the data generated and stored in today's digital world, particularly with the rise of the internet, social media, and IoT (Internet of Things) devices. Complexity: The analysis of unstructured data requires advanced techniques such as natural language processing (NLP), image recognition, and machine learning.","title":"Key Characteristics"},{"location":"concepts/data/06_data_unstructured/#advantages","text":"Richness of Information: Unstructured data can capture a wide range of information, including nuances, context, and detailed descriptions, which structured data might miss. Flexibility: Since unstructured data is not bound by a predefined schema, it can easily accommodate various types of information, allowing for more comprehensive data collection. Potential for Insights: With the right tools, unstructured data can be mined for valuable insights that structured data might not reveal, such as customer sentiment, trends, and patterns.","title":"Advantages"},{"location":"concepts/data/06_data_unstructured/#limitations","text":"Difficulty in Storage and Management: Traditional databases are not designed to handle unstructured data, making storage and management more complex. Challenges in Analysis: Analyzing unstructured data requires specialized tools and techniques, which can be resource-intensive and require expertise in fields like machine learning, NLP, and big data analytics. Data Quality: Unstructured data often contains noise, such as irrelevant or redundant information, which can complicate analysis and lead to less accurate results.","title":"Limitations"},{"location":"concepts/data/06_data_unstructured/#examples","text":"Text Documents: Emails, reports, and other documents that contain text with no fixed structure. Social Media: Posts, comments, and reviews on platforms like Twitter, Facebook, and Instagram are classic examples of unstructured data. Multimedia Files: Photos, videos, and audio recordings that lack a standard format for data storage. Sensor Data: Data collected from IoT devices, such as temperature readings or GPS data, often comes in an unstructured format.","title":"Examples"},{"location":"concepts/data/06_data_unstructured/#practical-exercise","text":"Consider a large set of customer reviews from an e-commerce site stored as unstructured text. Write a Python script using the nltk library (Natural Language Toolkit) to perform basic sentiment analysis on these reviews. import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sample unstructured data: customer reviews reviews = [ \"This product is fantastic! I've never been happier.\", \"Terrible experience. The product broke after one use.\", \"Good quality, but the price is too high.\", \"Excellent service, quick delivery, and a great product.\" ] # Initialize the sentiment analyzer nltk.download('vader_lexicon') sid = SentimentIntensityAnalyzer() # Perform sentiment analysis for review in reviews: sentiment_scores = sid.polarity_scores(review) print(f\"Review: {review}\") print(f\"Sentiment Scores: {sentiment_scores}\\n\") In this exercise, the Python script uses the nltk library to analyze the sentiment of each customer review, providing insights into whether the feedback is positive, negative, or neutral. This demonstrates one approach to processing and analyzing unstructured data.","title":"Practical Exercise"},{"location":"concepts/data/06_data_unstructured/#conclusion","text":"Unstructured data represents a vast, complex, and valuable source of information that, while challenging to manage and analyze, holds the potential for deep insights and new opportunities. As the volume of unstructured data continues to grow, mastering the tools and techniques to harness this data is increasingly important. Whether through text analysis, image recognition, or machine learning, the ability to extract meaning from unstructured data is a key skill in the modern data landscape. By understanding the nature of unstructured data and the methods available for working with it, you can unlock its full potential and apply it to a wide range of real-world scenarios.","title":"Conclusion"},{"location":"concepts/data/07_data_blending/","text":"7. Data Blending: An Overview What is Data Blending? Data blending is the process of pulling data from multiple sources to create a single, unified dataset for analysis and visualization. This allows for a comprehensive view of the data without needing to move or join all the data in one place. Key Features of Data Blending: Integrates data from different sources like spreadsheets, business intelligence systems, IoT devices, cloud platforms, and web applications. Enables fast and intuitive data combination for ad hoc reporting and rapid analysis . Typically used for surface-level analysis and quick insights . Traditional ETL vs. Modern ELT Data blending is now often performed using ELT (Extract, Load, Transform) instead of the traditional ETL (Extract, Transform, Load) method. ETL: Extract data from various sources. Transform the data into a standardized format. Load it into a data warehouse for analysis. ELT: Extract the data. Load it directly into a data warehouse. Transform the data as needed after it has been loaded. ELT is typically faster and easier, allowing for more flexibility in analyzing the data post-loading. Primary vs. Secondary Data Sources Data blending relies on the concept of primary and secondary data sources. Primary Data Source: The main, raw data set you are pulling from (e.g., sales data). Secondary Data Source: Additional data sets you combine with the primary source for further analysis (e.g., quota data). These operate independently from the primary data. Benefits: Keeping data separate prevents information loss during blending. Offers flexibility and control during data analysis. Data Blending vs. Data Joining Data Joining combines data from a single source, such as merging two tables from the same SQL database or joining Excel sheets. However, joins: - Work best for small datasets . - May not work across different tools or databases. Data Blending goes further by merging data from multiple tools or sources and is typically used for high-level analysis across disparate systems. Data Blending vs. Data Integration Data Integration : Comprehensive process of merging and cleansing data from different sources. Typically used for in-depth analysis in a data warehouse setting, involving multiple joins . Data Blending : Quicker and more focused on surface-level analysis rather than extensive data cleansing and merging. Benefits of Data Blending Rapid Analysis Data blending enables analysts to create insights faster, especially for ad hoc reporting. It can uncover relationships between datasets, like combining sales data and quota data for performance insights. Reduction of Data Silos Blending data helps break down data silos by allowing analysts to merge data as needed, without having to move it out of separate storage systems. Greater Efficiency Data blending is often more efficient than traditional data joins, particularly when dealing with multiple data tables and datasets with varying levels of detail. Non-Technical Accessibility Non-technical professionals can easily leverage data blending tools, making it accessible to departments like sales , marketing , and finance without requiring extensive knowledge in data science. Improved Decision-Making Businesses with data blending capabilities can more easily derive insights from data, leading to more informed decisions, improved ROI , and the ability to capitalize on sales opportunities . How to Blend Multiple Data Sources Step 1: Acquire the Data Identify the data sources you need, such as social media data, spreadsheets, and tables. Step 2: Join and Store the Data Load the combined data into a storage destination, such as a data warehouse . Step 3: Clean the Data Correct errors : Fix data entry mistakes. Delete unnecessary data : Remove irrelevant or redundant data. Redesign the dataset : Format the data into a structured, analyzable format. Step 4: Analyze the Data Use data blending tools like Tableau or Alteryx to quickly visualize and analyze the blended data. Tools for Data Blending Tableau : A powerful visualization tool that allows you to blend data for fast analysis. Alteryx : A data analytics platform that simplifies data blending and analysis. Panoply : Streamlines the ETL and data warehousing process, making it easy to blend and store data. Supercharging Your Data Blending with Panoply Panoply makes the ETL and data warehousing process seamless, allowing you to focus on data analysis instead of backend management. This enables you to quickly access and blend data across different systems and visualize it in tools like Tableau and Alteryx. For more information, Book a Demo today! Key Takeaways: Data blending is crucial for making sense of data spread across multiple sources. It facilitates rapid insights, reduces silos, and makes data more accessible to non-technical users. ELT is more efficient than traditional ETL for data blending, allowing faster analysis.","title":"07 - Data Blending"},{"location":"concepts/data/07_data_blending/#7-data-blending-an-overview","text":"","title":"7. Data Blending: An Overview"},{"location":"concepts/data/07_data_blending/#what-is-data-blending","text":"Data blending is the process of pulling data from multiple sources to create a single, unified dataset for analysis and visualization. This allows for a comprehensive view of the data without needing to move or join all the data in one place.","title":"What is Data Blending?"},{"location":"concepts/data/07_data_blending/#key-features-of-data-blending","text":"Integrates data from different sources like spreadsheets, business intelligence systems, IoT devices, cloud platforms, and web applications. Enables fast and intuitive data combination for ad hoc reporting and rapid analysis . Typically used for surface-level analysis and quick insights .","title":"Key Features of Data Blending:"},{"location":"concepts/data/07_data_blending/#traditional-etl-vs-modern-elt","text":"Data blending is now often performed using ELT (Extract, Load, Transform) instead of the traditional ETL (Extract, Transform, Load) method.","title":"Traditional ETL vs. Modern ELT"},{"location":"concepts/data/07_data_blending/#etl","text":"Extract data from various sources. Transform the data into a standardized format. Load it into a data warehouse for analysis.","title":"ETL:"},{"location":"concepts/data/07_data_blending/#elt","text":"Extract the data. Load it directly into a data warehouse. Transform the data as needed after it has been loaded. ELT is typically faster and easier, allowing for more flexibility in analyzing the data post-loading.","title":"ELT:"},{"location":"concepts/data/07_data_blending/#primary-vs-secondary-data-sources","text":"Data blending relies on the concept of primary and secondary data sources. Primary Data Source: The main, raw data set you are pulling from (e.g., sales data). Secondary Data Source: Additional data sets you combine with the primary source for further analysis (e.g., quota data). These operate independently from the primary data.","title":"Primary vs. Secondary Data Sources"},{"location":"concepts/data/07_data_blending/#benefits","text":"Keeping data separate prevents information loss during blending. Offers flexibility and control during data analysis.","title":"Benefits:"},{"location":"concepts/data/07_data_blending/#data-blending-vs-data-joining","text":"Data Joining combines data from a single source, such as merging two tables from the same SQL database or joining Excel sheets. However, joins: - Work best for small datasets . - May not work across different tools or databases. Data Blending goes further by merging data from multiple tools or sources and is typically used for high-level analysis across disparate systems.","title":"Data Blending vs. Data Joining"},{"location":"concepts/data/07_data_blending/#data-blending-vs-data-integration","text":"Data Integration : Comprehensive process of merging and cleansing data from different sources. Typically used for in-depth analysis in a data warehouse setting, involving multiple joins . Data Blending : Quicker and more focused on surface-level analysis rather than extensive data cleansing and merging.","title":"Data Blending vs. Data Integration"},{"location":"concepts/data/07_data_blending/#benefits-of-data-blending","text":"Rapid Analysis Data blending enables analysts to create insights faster, especially for ad hoc reporting. It can uncover relationships between datasets, like combining sales data and quota data for performance insights. Reduction of Data Silos Blending data helps break down data silos by allowing analysts to merge data as needed, without having to move it out of separate storage systems. Greater Efficiency Data blending is often more efficient than traditional data joins, particularly when dealing with multiple data tables and datasets with varying levels of detail. Non-Technical Accessibility Non-technical professionals can easily leverage data blending tools, making it accessible to departments like sales , marketing , and finance without requiring extensive knowledge in data science. Improved Decision-Making Businesses with data blending capabilities can more easily derive insights from data, leading to more informed decisions, improved ROI , and the ability to capitalize on sales opportunities .","title":"Benefits of Data Blending"},{"location":"concepts/data/07_data_blending/#how-to-blend-multiple-data-sources","text":"","title":"How to Blend Multiple Data Sources"},{"location":"concepts/data/07_data_blending/#step-1-acquire-the-data","text":"Identify the data sources you need, such as social media data, spreadsheets, and tables.","title":"Step 1: Acquire the Data"},{"location":"concepts/data/07_data_blending/#step-2-join-and-store-the-data","text":"Load the combined data into a storage destination, such as a data warehouse .","title":"Step 2: Join and Store the Data"},{"location":"concepts/data/07_data_blending/#step-3-clean-the-data","text":"Correct errors : Fix data entry mistakes. Delete unnecessary data : Remove irrelevant or redundant data. Redesign the dataset : Format the data into a structured, analyzable format.","title":"Step 3: Clean the Data"},{"location":"concepts/data/07_data_blending/#step-4-analyze-the-data","text":"Use data blending tools like Tableau or Alteryx to quickly visualize and analyze the blended data.","title":"Step 4: Analyze the Data"},{"location":"concepts/data/07_data_blending/#tools-for-data-blending","text":"Tableau : A powerful visualization tool that allows you to blend data for fast analysis. Alteryx : A data analytics platform that simplifies data blending and analysis. Panoply : Streamlines the ETL and data warehousing process, making it easy to blend and store data.","title":"Tools for Data Blending"},{"location":"concepts/data/07_data_blending/#supercharging-your-data-blending-with-panoply","text":"Panoply makes the ETL and data warehousing process seamless, allowing you to focus on data analysis instead of backend management. This enables you to quickly access and blend data across different systems and visualize it in tools like Tableau and Alteryx. For more information, Book a Demo today!","title":"Supercharging Your Data Blending with Panoply"},{"location":"concepts/data/07_data_blending/#key-takeaways","text":"Data blending is crucial for making sense of data spread across multiple sources. It facilitates rapid insights, reduces silos, and makes data more accessible to non-technical users. ELT is more efficient than traditional ETL for data blending, allowing faster analysis.","title":"Key Takeaways:"},{"location":"concepts/data_engineering/01_introduction_data_engineering/","text":"Data Engineering Course Structure Course Sequence Overview This course structure for Data Engineering is organized to provide a logical sequence that builds from foundational topics toward advanced concepts. This progression allows students to develop a solid base before delving into more complex topics, covering all key aspects of data engineering, from basic understanding of data types and structures to implementing scalable data architectures and using advanced technologies. The course is divided into the following sections: 1. Data Fundamentals Basic Data Concepts Data Types Data Structures 2. Data Modeling Conceptual Modeling Logical Modeling Physical Modeling Entity-Relationship Diagrams (ERD) 3. Databases Relational Databases SQL NoSQL Databases Optimization and Indexing 4. Data Processing ETL (Extract, Transform, Load) Processes Data Pipelines Batch and Real-Time Processing 5. Data Storage Data Warehouses Data Lakes Data Lakehouses 6. Big Data Big Data Technologies (e.g., Hadoop, Spark) Distributed Processing 7. Data Quality and Governance Data Cleaning and Validation Metadata Management Data Security and Privacy 8. Data Analysis and Visualization Business Intelligence (BI) Tools Dashboards and Reporting 9. Data Architecture Design of Scalable Data Systems Integration of Various Data Sources 10. Advanced Technologies and Tools Cloud Computing for Data Engineering Machine Learning for Data Engineering Data Streaming Rationale for the Sequence This order enables students to progress logically, beginning with foundational knowledge and moving towards more advanced and specialized topics. For example, understanding data types and structures is essential before approaching data modeling, which is in turn crucial for comprehending database systems, and so on. Additionally, this structure mirrors the typical data flow within many organizations: from initial data collection and storage to processing, analysis, and ultimately data-driven decision making. By following this sequence, students will be equipped with a comprehensive understanding of data engineering, building proficiency in each area in a way that supports their growth into skilled data engineers capable of tackling both foundational and advanced challenges in the field.","title":"01 - DE Introduction"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#data-engineering-course-structure","text":"","title":"Data Engineering Course Structure"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#course-sequence-overview","text":"This course structure for Data Engineering is organized to provide a logical sequence that builds from foundational topics toward advanced concepts. This progression allows students to develop a solid base before delving into more complex topics, covering all key aspects of data engineering, from basic understanding of data types and structures to implementing scalable data architectures and using advanced technologies. The course is divided into the following sections:","title":"Course Sequence Overview"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#1-data-fundamentals","text":"Basic Data Concepts Data Types Data Structures","title":"1. Data Fundamentals"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#2-data-modeling","text":"Conceptual Modeling Logical Modeling Physical Modeling Entity-Relationship Diagrams (ERD)","title":"2. Data Modeling"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#3-databases","text":"Relational Databases SQL NoSQL Databases Optimization and Indexing","title":"3. Databases"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#4-data-processing","text":"ETL (Extract, Transform, Load) Processes Data Pipelines Batch and Real-Time Processing","title":"4. Data Processing"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#5-data-storage","text":"Data Warehouses Data Lakes Data Lakehouses","title":"5. Data Storage"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#6-big-data","text":"Big Data Technologies (e.g., Hadoop, Spark) Distributed Processing","title":"6. Big Data"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#7-data-quality-and-governance","text":"Data Cleaning and Validation Metadata Management Data Security and Privacy","title":"7. Data Quality and Governance"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#8-data-analysis-and-visualization","text":"Business Intelligence (BI) Tools Dashboards and Reporting","title":"8. Data Analysis and Visualization"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#9-data-architecture","text":"Design of Scalable Data Systems Integration of Various Data Sources","title":"9. Data Architecture"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#10-advanced-technologies-and-tools","text":"Cloud Computing for Data Engineering Machine Learning for Data Engineering Data Streaming","title":"10. Advanced Technologies and Tools"},{"location":"concepts/data_engineering/01_introduction_data_engineering/#rationale-for-the-sequence","text":"This order enables students to progress logically, beginning with foundational knowledge and moving towards more advanced and specialized topics. For example, understanding data types and structures is essential before approaching data modeling, which is in turn crucial for comprehending database systems, and so on. Additionally, this structure mirrors the typical data flow within many organizations: from initial data collection and storage to processing, analysis, and ultimately data-driven decision making. By following this sequence, students will be equipped with a comprehensive understanding of data engineering, building proficiency in each area in a way that supports their growth into skilled data engineers capable of tackling both foundational and advanced challenges in the field.","title":"Rationale for the Sequence"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/","text":"2. ETL vs. ELT: Understanding Data Integration Processes - parte 1 Data integration is crucial in modern data management, especially in big data environments. Two common processes used for data integration are ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) . While they share similar steps, they differ in the sequence and environment in which data transformations are applied. 1. What are ETL and ELT? ETL (Extract, Transform, Load) ETL is a process where data is extracted from source systems, transformed to fit business needs ( processed data ), and then loaded into a destination, usually a database . Extract : Data is extracted from various sources , such as databases, APIs, Webscraping or files . Transform : Extracted data is transformed to meet business rules and formatting requirements. This may include data cleaning, aggregation, or enrichment. Load : Transformed data is loaded into the target system, typically a data warehouse or database, ready for analysis. Advantages of ETL: Ensures data is clean and conforms to business requirements before loading, facilitating consistent and accurate analysis. Ideal for databases with lower processing capacity , as transformations occur outside the destination database, reducing processing load on the target system. Disadvantages of ETL: The transformation process can be slow with large data volumes since it occurs before loading. Less flexibility for ad-hoc analysis, as data is pre-processed and transformed. ELT (Extract, Load, Transform) ELT is a process that loads raw data directly into the target system, often a data lake or a cloud data warehouse , and then applies transformations within as needed. This approach leverages the processing power of modern data warehouses to perform transformations after loading. Extract : Data is extracted from source systems, similar to ETL. Load : Data is loaded in its raw form into the destination, such as a data lake or cloud data warehouse. Transform : After loading, transformations are applied within the data warehouse, using its processing capabilities to clean and format the data. Advantages of ELT: Takes advantage of the scalability and processing power of cloud-based systems, allowing for rapid loading of large datasets. More suitable for ad-hoc analysis and big data environments, as raw data is available for various types of processing. Disadvantages of ELT: Requires more processing capacity and a robust data warehouse, which may increase costs. Raw data can consume more storage and may lead to processing delays if transformations are complex. 2. Key Differences Between ETL and ELT Aspect ETL ELT Transformation Occurs before loading; suited for customized and complex transformations. Occurs after loading; takes advantage of cloud processing. Processing External processing server In-database processing Use Case Traditional data warehouses Cloud data lakes/warehouses Data Volume Smaller datasets Large, complex datasets Flexibility Less flexible High flexibility for ad-hoc analysis 3. Pros and Cons of ETL and ELT Process Pros Cons ETL - Ensures data is transformed before loading, guaranteeing quality and consistency. - Slower processing time with large datasets due to pre-loading transformations. - Ideal for systems with limited processing capabilities, as transformations occur externally. - Less flexible for ad-hoc analysis as data is pre-processed. - Reduces storage and processing demands on the database. - Higher maintenance complexity due to transformation pipelines. ELT - Enables rapid loading of raw data, allowing quick access to large datasets. - Requires significant storage, as raw data can be voluminous. - Utilizes cloud-based data warehouses for scalable transformations. - Transformation delays can occur with complex operations. - Suitable for big data environments and flexible for exploratory data processing. - Requires robust cloud resources, potentially increasing costs. 4. Criteria-Based Comparison Criterion ETL ELT Winner Staffing Needs Requires more specialized staff, particularly data engineers. Fewer staff needed due to automation within modern data warehouses. ELT Cost Lower initial costs, especially for small-scale or on-premises setups, but higher maintenance costs. Higher initial costs, but more cost-effective at scale. Depends (ETL for small scale; ELT for large scale) Processing Memory Lower demand, as transformations occur outside the final system. High demand, as processing occurs directly within the data warehouse. ELT Storage Requirements Less storage needed due to pre-processing. More storage required for raw data. ETL Best for Small Businesses More suitable due to lower initial costs and on-premises compatibility. Less suitable due to initial cloud infrastructure costs. ETL Best for Large Businesses May not scale well for large data volumes. Highly scalable for big data environments. ELT Ease of Maintenance More complex due to external transformation. Lower maintenance due to automation. ELT Overall Ease More initial setup and infrastructure management. Simpler with cloud automation and scalability. ELT Better for Future, Unexpected Analysis Less flexible for unforeseen analyses. More flexible, as raw data remains for various analyses. ELT Community Support Strong community support with established tools. Growing community with focus on cloud solutions. Tie (ETL has tradition; ELT has innovation) 5. Choosing Between ETL and ELT When deciding between ETL and ELT, consider the following: Data Volume : ELT is preferable for large datasets that require cloud scalability. Transformation Needs : ETL is ideal when complex data transformations are necessary before analysis. Infrastructure : ETL suits on-premises setups, whereas ELT leverages cloud computing. Budget : While ETL is often more cost-effective for smaller projects, ELT can provide greater flexibility and scalability over time. 6. Python Tooling for ETL and ELT Projects ETL Tools In an ETL project, transformations occur before data reaches the destination. Common tools include: Python : General-purpose scripting. Pandas : Data manipulation. SQLAlchemy : Database interaction. Airflow : Pipeline orchestration and scheduling. ELT Tools In an ELT project, data is loaded first, and transformations happen within the destination, leveraging cloud power: Snowflake/BigQuery : Cloud data warehouses. DBT (Data Build Tool) : Transformation in the warehouse. Airflow or Prefect : ELT pipeline orchestration. 7. Summary of ETL vs ELT ETL : Commonly used with traditional databases where transformations occur before the final load. ELT : Suited for modern data warehouses, allowing in-database transformations for scalability and flexibility. Both ETL and ELT are essential in different scenarios. While ETL offers control and is suitable for structured data, ELT thrives in cloud environments with scalability and flexibility for various data types.","title":"02 - ETL vs ELT - part 1"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#2-etl-vs-elt-understanding-data-integration-processes-parte-1","text":"Data integration is crucial in modern data management, especially in big data environments. Two common processes used for data integration are ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) . While they share similar steps, they differ in the sequence and environment in which data transformations are applied.","title":"2. ETL vs. ELT: Understanding Data Integration Processes - parte 1"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#1-what-are-etl-and-elt","text":"","title":"1. What are ETL and ELT?"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#etl-extract-transform-load","text":"ETL is a process where data is extracted from source systems, transformed to fit business needs ( processed data ), and then loaded into a destination, usually a database . Extract : Data is extracted from various sources , such as databases, APIs, Webscraping or files . Transform : Extracted data is transformed to meet business rules and formatting requirements. This may include data cleaning, aggregation, or enrichment. Load : Transformed data is loaded into the target system, typically a data warehouse or database, ready for analysis. Advantages of ETL: Ensures data is clean and conforms to business requirements before loading, facilitating consistent and accurate analysis. Ideal for databases with lower processing capacity , as transformations occur outside the destination database, reducing processing load on the target system. Disadvantages of ETL: The transformation process can be slow with large data volumes since it occurs before loading. Less flexibility for ad-hoc analysis, as data is pre-processed and transformed.","title":"ETL (Extract, Transform, Load)"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#elt-extract-load-transform","text":"ELT is a process that loads raw data directly into the target system, often a data lake or a cloud data warehouse , and then applies transformations within as needed. This approach leverages the processing power of modern data warehouses to perform transformations after loading. Extract : Data is extracted from source systems, similar to ETL. Load : Data is loaded in its raw form into the destination, such as a data lake or cloud data warehouse. Transform : After loading, transformations are applied within the data warehouse, using its processing capabilities to clean and format the data. Advantages of ELT: Takes advantage of the scalability and processing power of cloud-based systems, allowing for rapid loading of large datasets. More suitable for ad-hoc analysis and big data environments, as raw data is available for various types of processing. Disadvantages of ELT: Requires more processing capacity and a robust data warehouse, which may increase costs. Raw data can consume more storage and may lead to processing delays if transformations are complex.","title":"ELT (Extract, Load, Transform)"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#2-key-differences-between-etl-and-elt","text":"Aspect ETL ELT Transformation Occurs before loading; suited for customized and complex transformations. Occurs after loading; takes advantage of cloud processing. Processing External processing server In-database processing Use Case Traditional data warehouses Cloud data lakes/warehouses Data Volume Smaller datasets Large, complex datasets Flexibility Less flexible High flexibility for ad-hoc analysis","title":"2. Key Differences Between ETL and ELT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#3-pros-and-cons-of-etl-and-elt","text":"Process Pros Cons ETL - Ensures data is transformed before loading, guaranteeing quality and consistency. - Slower processing time with large datasets due to pre-loading transformations. - Ideal for systems with limited processing capabilities, as transformations occur externally. - Less flexible for ad-hoc analysis as data is pre-processed. - Reduces storage and processing demands on the database. - Higher maintenance complexity due to transformation pipelines. ELT - Enables rapid loading of raw data, allowing quick access to large datasets. - Requires significant storage, as raw data can be voluminous. - Utilizes cloud-based data warehouses for scalable transformations. - Transformation delays can occur with complex operations. - Suitable for big data environments and flexible for exploratory data processing. - Requires robust cloud resources, potentially increasing costs.","title":"3. Pros and Cons of ETL and ELT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#4-criteria-based-comparison","text":"Criterion ETL ELT Winner Staffing Needs Requires more specialized staff, particularly data engineers. Fewer staff needed due to automation within modern data warehouses. ELT Cost Lower initial costs, especially for small-scale or on-premises setups, but higher maintenance costs. Higher initial costs, but more cost-effective at scale. Depends (ETL for small scale; ELT for large scale) Processing Memory Lower demand, as transformations occur outside the final system. High demand, as processing occurs directly within the data warehouse. ELT Storage Requirements Less storage needed due to pre-processing. More storage required for raw data. ETL Best for Small Businesses More suitable due to lower initial costs and on-premises compatibility. Less suitable due to initial cloud infrastructure costs. ETL Best for Large Businesses May not scale well for large data volumes. Highly scalable for big data environments. ELT Ease of Maintenance More complex due to external transformation. Lower maintenance due to automation. ELT Overall Ease More initial setup and infrastructure management. Simpler with cloud automation and scalability. ELT Better for Future, Unexpected Analysis Less flexible for unforeseen analyses. More flexible, as raw data remains for various analyses. ELT Community Support Strong community support with established tools. Growing community with focus on cloud solutions. Tie (ETL has tradition; ELT has innovation)","title":"4. Criteria-Based Comparison"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#5-choosing-between-etl-and-elt","text":"When deciding between ETL and ELT, consider the following: Data Volume : ELT is preferable for large datasets that require cloud scalability. Transformation Needs : ETL is ideal when complex data transformations are necessary before analysis. Infrastructure : ETL suits on-premises setups, whereas ELT leverages cloud computing. Budget : While ETL is often more cost-effective for smaller projects, ELT can provide greater flexibility and scalability over time.","title":"5. Choosing Between ETL and ELT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#6-python-tooling-for-etl-and-elt-projects","text":"","title":"6. Python Tooling for ETL and ELT Projects"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#etl-tools","text":"In an ETL project, transformations occur before data reaches the destination. Common tools include: Python : General-purpose scripting. Pandas : Data manipulation. SQLAlchemy : Database interaction. Airflow : Pipeline orchestration and scheduling.","title":"ETL Tools"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#elt-tools","text":"In an ELT project, data is loaded first, and transformations happen within the destination, leveraging cloud power: Snowflake/BigQuery : Cloud data warehouses. DBT (Data Build Tool) : Transformation in the warehouse. Airflow or Prefect : ELT pipeline orchestration.","title":"ELT Tools"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part1/#7-summary-of-etl-vs-elt","text":"ETL : Commonly used with traditional databases where transformations occur before the final load. ELT : Suited for modern data warehouses, allowing in-database transformations for scalability and flexibility. Both ETL and ELT are essential in different scenarios. While ETL offers control and is suitable for structured data, ELT thrives in cloud environments with scalability and flexibility for various data types.","title":"7. Summary of ETL vs ELT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/","text":"2. Documentation on ETL and ELT Pipelines - Part 2 Table of Contents Purpose of this Documentation Types of ETL and ELT ETL Pipeline with Kafka, Spark, and Airflow ELT Pipeline with Kafka, Data Warehouse, and DBT Conclusion Purpose of this Documentation The goal of this documentation is to help you understand ETL and ELT pipelines in a practical and memorable way . Each section uses ETL/ELT abbreviations directly with real tools (Kafka, Spark, Airflow, Snowflake, DBT) to make each step easy to remember and apply. Types of ETL and ELT Both ETL and ELT processes can operate in two primary modes, each suited for specific use cases: Batch Processing : Processes large volumes of data at set intervals. Ideal for datasets that do not need real-time updates. Streaming Processing (real time) : Processes data in real time or near-real time, allowing for instant insights. Essential in cases where data freshness is critical, such as monitoring or alerting systems. ETL Pipeline with Kafka, Spark, and Airflow This section explains how to set up an ETL pipeline using Kafka, Spark, and Airflow , along with a database or data lake for storage. Here\u2019s how each component fits into the ETL abbreviation: Step Abbreviation Tool Purpose Extract E Kafka Kafka extract data from multiple sources (DB, API, Files, Webscraping etc) in real-time or near real-time and accept loads of different programming languages (Python, Scala, Java, Go, C++ etc) Transform T Spark Spark transforms data by cleaning, aggregating, and structuring it for analysis Load L Database/Data Lake Stores transformed data, often as raw data in data lakes for reprocessing or analysis Orchestration ETL Airflow orchestrate ETL/ELT pipelines for any data source or destination, managing dependencies, execution order and scheduling Each tool in this setup is chosen to highlight its specific role in the ETL pipeline, making it easier to associate the tools with each ETL step. ELT Pipeline with Kafka, Data Warehouse, and DBT In an ELT pipeline , data loading happens before transformation, leveraging the processing power of the Data Warehouse . Below is the breakdown for an ELT setup using Kafka, a data warehouse, and DBT. Step Abbreviation Tool Purpose Extract E Kafka Kafka extract data from multiple sources (DB, API, Files, Webscraping etc) in real-time or near real-time and accept loads of different programming languages (Python, Scala, Java, Go, C++ etc) Load L Data Warehouse (e.g., Snowflake) Loads raw data directly into a data warehouse like Snowflake for scalable storage Transform T DBT DBT (Data Build Tool) transforms data within the warehouse , leveraging its computation power Orchestration ELT Airflow Airflow The use of the ETL and ELT abbreviations here provides an intuitive guide to the function of each component. Conclusion Choosing between ETL and ELT pipelines depends on data requirements, infrastructure, and the specific use case. ETL pipelines are traditionally better for pre-processed data, while ELT pipelines benefit from modern data warehouses' scalability and in-place processing. Through this case study, the ETL and ELT abbreviations help structure each step, making the components memorable and their roles clear in both types of pipelines.","title":"02 - ETL vs ELT - part 2"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#2-documentation-on-etl-and-elt-pipelines-part-2","text":"","title":"2. Documentation on ETL and ELT Pipelines - Part 2"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#table-of-contents","text":"Purpose of this Documentation Types of ETL and ELT ETL Pipeline with Kafka, Spark, and Airflow ELT Pipeline with Kafka, Data Warehouse, and DBT Conclusion","title":"Table of Contents"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#purpose-of-this-documentation","text":"The goal of this documentation is to help you understand ETL and ELT pipelines in a practical and memorable way . Each section uses ETL/ELT abbreviations directly with real tools (Kafka, Spark, Airflow, Snowflake, DBT) to make each step easy to remember and apply.","title":"Purpose of this Documentation"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#types-of-etl-and-elt","text":"Both ETL and ELT processes can operate in two primary modes, each suited for specific use cases: Batch Processing : Processes large volumes of data at set intervals. Ideal for datasets that do not need real-time updates. Streaming Processing (real time) : Processes data in real time or near-real time, allowing for instant insights. Essential in cases where data freshness is critical, such as monitoring or alerting systems.","title":"Types of ETL and ELT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#etl-pipeline-with-kafka-spark-and-airflow","text":"This section explains how to set up an ETL pipeline using Kafka, Spark, and Airflow , along with a database or data lake for storage. Here\u2019s how each component fits into the ETL abbreviation: Step Abbreviation Tool Purpose Extract E Kafka Kafka extract data from multiple sources (DB, API, Files, Webscraping etc) in real-time or near real-time and accept loads of different programming languages (Python, Scala, Java, Go, C++ etc) Transform T Spark Spark transforms data by cleaning, aggregating, and structuring it for analysis Load L Database/Data Lake Stores transformed data, often as raw data in data lakes for reprocessing or analysis Orchestration ETL Airflow orchestrate ETL/ELT pipelines for any data source or destination, managing dependencies, execution order and scheduling Each tool in this setup is chosen to highlight its specific role in the ETL pipeline, making it easier to associate the tools with each ETL step.","title":"ETL Pipeline with Kafka, Spark, and Airflow"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#elt-pipeline-with-kafka-data-warehouse-and-dbt","text":"In an ELT pipeline , data loading happens before transformation, leveraging the processing power of the Data Warehouse . Below is the breakdown for an ELT setup using Kafka, a data warehouse, and DBT. Step Abbreviation Tool Purpose Extract E Kafka Kafka extract data from multiple sources (DB, API, Files, Webscraping etc) in real-time or near real-time and accept loads of different programming languages (Python, Scala, Java, Go, C++ etc) Load L Data Warehouse (e.g., Snowflake) Loads raw data directly into a data warehouse like Snowflake for scalable storage Transform T DBT DBT (Data Build Tool) transforms data within the warehouse , leveraging its computation power Orchestration ELT Airflow Airflow The use of the ETL and ELT abbreviations here provides an intuitive guide to the function of each component.","title":"ELT Pipeline with Kafka, Data Warehouse, and DBT"},{"location":"concepts/data_engineering/02_ETL_vs_ELT_part2/#conclusion","text":"Choosing between ETL and ELT pipelines depends on data requirements, infrastructure, and the specific use case. ETL pipelines are traditionally better for pre-processed data, while ELT pipelines benefit from modern data warehouses' scalability and in-place processing. Through this case study, the ETL and ELT abbreviations help structure each step, making the components memorable and their roles clear in both types of pipelines.","title":"Conclusion"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/","text":"3. Data Warehouse vs. Database vs. Data Lake Overview Data Warehouses, Databases, and Data Lakes each serve as storage solutions, but they cater to different data management needs within an organization: Data Lake : Optimized for storing unstructured and semi-structured data , often in its raw form, supporting large-scale data exploration, machine learning, and advanced analytics. Data Warehouse : Primarily focused on structured data , optimized for ELT processes where data transformations occur to enable business intelligence (BI) and decision-making. Tools like Snowflake are examples. Database : Used for structured data / semi-structured data (e.g., JSON in NoSQL databases). Databases are designed for fast retrieval and updating of transactional data, better for ETL Tips: ETL is related to Database, because the transformation come before loading. ELT is related to DataWarehouse, because the transformation occours after loading (for example, snowflake makes the transformation for your inside the plataform). Structured data , is everything that fit well in a TABLE - RELATIONAL ( SQL ). Semi-Structured data , is everthing that fit well in a DOCUMENT - NO RELATIONAL ( NoSQL ). Unstructured data , is a file, for instance, video, images, text file, audio file etc. If you think about RAW data it is related to DataLake , because the data haven't had any kind of TRANSFORMATION it also is related to DataWarehouse , because even after the TRANSFORMATION (ELT) the raw data are also storage in the Plataform. Key Differences Feature Data Lake Data Warehouse Database Purpose Storing raw, unstructured , and semi-structured data for analysis Analysis and historical data storage through ELT processes Real-time data storage and retrieval Processing Supports batch processing and big data analytics OLAP (Online Analytical Processing) OLTP (Online Transactional Processing) Data Structure Schema-on-read, stores data in raw format Subject-oriented, often star or snowflake schemas Application-oriented, typically relational Query Types Allows ad-hoc and advanced analytical queries Complex analytical queries Simple transaction queries Data Types Unstructured and semi-structured Structured historical data Primarily structured ; some can handle semi-structured data Data Storage Highly scalable, designed for big data and cloud environments Stored in a highly organized, optimized structure Organized for quick access by applications Scalability Designed for storing massive volumes of data, typically in cloud Scales based on analytical needs Scales according to application demands What is a Data Lake? A Data Lake is a centralized repository optimized for storing unstructured and semi-structured data in its native, unprocessed format. It is a highly scalable solution, ideal for data exploration, machine learning, and advanced analytics. Examples include AWS S3 buckets which store raw data without requiring it to be structured or transformed beforehand. Key Features of a Data Lake Scalability : Designed to handle massive data volumes, often in a cloud environment. Data Variety : Stores structured, semi-structured, and unstructured data (e.g., text, images, sensor data). Schema-on-Read : Applies schemas when data is read, allowing for flexibility in analysis. Data Exploration : Useful for data scientists and analysts to explore data for insights. Machine Learning and AI : Often used for big data analytics, machine learning, and AI model training. Examples of Data Lake Solutions Amazon S3 with AWS Lake Formation : AWS service to build and manage Data Lakes. Azure Data Lake Storage : A highly scalable storage service on Microsoft Azure. Google Cloud Storage : Supports big data storage with integration for analytics and machine learning. Apache Hadoop HDFS : Open-source solution for large-scale data storage and processing. What is a Data Warehouse? A Data Warehouse is a centralized repository optimized for storing structured data , typically involving ETL/ELT processes. Data is cleansed, transformed, and integrated from multiple sources to enable business intelligence (BI) and decision-making. Snowflake and other solutions support these processes, enabling data to be prepared and optimized for analytical queries. Key Features of a Data Warehouse Historical Data Storage : Consolidates historical data from various sources over time. Data Transformation : Processes data for structured analysis. Business Intelligence Support : Enables complex analytical queries and reporting. Data Integration : Combines data from multiple sources for a unified view. Decision Support : Assists in trend analysis and data-driven decision-making. Examples of Data Warehouse Solutions Amazon Redshift : Fully managed on AWS, supports SQL queries. Google BigQuery : Serverless and scalable, designed for big data analysis. Snowflake : Cloud-based, with separate scalable storage and compute resources. Microsoft Azure Synapse : Integrated with Azure for big data and data warehousing. What is a Database? A Database is a system designed primarily for storing and retrieving structured data , though some databases (like NoSQL) can handle semi-structured data such as JSON documents. Databases are crucial for transactional operations, ensuring fast data access and update capabilities for real-time applications. Key Features of a Database Transactional Processing : Manages real-time data transactions, such as in e-commerce or CRM. Data Storage : Organized in tables for structured data retrieval. Real-time Queries : Supports high-speed read and write operations. Consistency : Ensures data accuracy and reliability in transactions. Types of Databases Relational Databases (SQL) : Store data in structured tables (e.g., MySQL, PostgreSQL). NoSQL Databases : Handle large volumes of semi-structured data, offering flexibility (e.g., MongoDB, Cassandra). Use Cases Data Lake Use Cases IoT Data : Storing and processing sensor data in raw format for analysis. Machine Learning : Aggregating large volumes of training data for AI and ML models. Data Exploration : Flexible storage for exploratory analysis, enabling data scientists to uncover insights. Data Warehouse Use Cases Healthcare : Analyzing patient data to support treatment decisions. Marketing : Tracking campaign performance and customer interactions over time. Finance : Analyzing historical financial data for forecasting and trend analysis. Database Use Cases Retail : Managing real-time inventory and order transactions. CRM Systems : Storing and accessing customer information quickly for sales and support. Banking : Processing real-time transactions with data consistency and security. Data Management Professionals Data Lake Engineer : Manages and maintains the Data Lake infrastructure, ensuring efficient storage and retrieval of raw data. Data Warehouse Analyst : Analyzes data and evaluates systems to recommend improvements for better insights. BI Analyst : Develops insights through data visualization and reporting, primarily from structured data. Database Administrator : Manages databases, ensuring data integrity, security, and performance. Conclusion Data Lakes , Data Warehouses , and Databases serve distinct roles in data management: Data Lake : Best for storing and exploring raw data for machine learning, big data analytics, and flexible data exploration. Data Warehouse : Best for structured historical data analysis, supporting business intelligence and strategic decision-making through ETL/ELT processes. Database : Best for real-time data storage and quick transactional operations, essential for applications requiring consistent, structured data . Organizations often leverage a combination of these solutions to meet comprehensive data management and analysis needs.","title":"03 - Data Warehouse vs Database vs Datalake"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#3-data-warehouse-vs-database-vs-data-lake","text":"","title":"3. Data Warehouse vs. Database vs. Data Lake"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#overview","text":"Data Warehouses, Databases, and Data Lakes each serve as storage solutions, but they cater to different data management needs within an organization: Data Lake : Optimized for storing unstructured and semi-structured data , often in its raw form, supporting large-scale data exploration, machine learning, and advanced analytics. Data Warehouse : Primarily focused on structured data , optimized for ELT processes where data transformations occur to enable business intelligence (BI) and decision-making. Tools like Snowflake are examples. Database : Used for structured data / semi-structured data (e.g., JSON in NoSQL databases). Databases are designed for fast retrieval and updating of transactional data, better for ETL Tips: ETL is related to Database, because the transformation come before loading. ELT is related to DataWarehouse, because the transformation occours after loading (for example, snowflake makes the transformation for your inside the plataform). Structured data , is everything that fit well in a TABLE - RELATIONAL ( SQL ). Semi-Structured data , is everthing that fit well in a DOCUMENT - NO RELATIONAL ( NoSQL ). Unstructured data , is a file, for instance, video, images, text file, audio file etc. If you think about RAW data it is related to DataLake , because the data haven't had any kind of TRANSFORMATION it also is related to DataWarehouse , because even after the TRANSFORMATION (ELT) the raw data are also storage in the Plataform.","title":"Overview"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#key-differences","text":"Feature Data Lake Data Warehouse Database Purpose Storing raw, unstructured , and semi-structured data for analysis Analysis and historical data storage through ELT processes Real-time data storage and retrieval Processing Supports batch processing and big data analytics OLAP (Online Analytical Processing) OLTP (Online Transactional Processing) Data Structure Schema-on-read, stores data in raw format Subject-oriented, often star or snowflake schemas Application-oriented, typically relational Query Types Allows ad-hoc and advanced analytical queries Complex analytical queries Simple transaction queries Data Types Unstructured and semi-structured Structured historical data Primarily structured ; some can handle semi-structured data Data Storage Highly scalable, designed for big data and cloud environments Stored in a highly organized, optimized structure Organized for quick access by applications Scalability Designed for storing massive volumes of data, typically in cloud Scales based on analytical needs Scales according to application demands","title":"Key Differences"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#what-is-a-data-lake","text":"A Data Lake is a centralized repository optimized for storing unstructured and semi-structured data in its native, unprocessed format. It is a highly scalable solution, ideal for data exploration, machine learning, and advanced analytics. Examples include AWS S3 buckets which store raw data without requiring it to be structured or transformed beforehand.","title":"What is a Data Lake?"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#key-features-of-a-data-lake","text":"Scalability : Designed to handle massive data volumes, often in a cloud environment. Data Variety : Stores structured, semi-structured, and unstructured data (e.g., text, images, sensor data). Schema-on-Read : Applies schemas when data is read, allowing for flexibility in analysis. Data Exploration : Useful for data scientists and analysts to explore data for insights. Machine Learning and AI : Often used for big data analytics, machine learning, and AI model training.","title":"Key Features of a Data Lake"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#examples-of-data-lake-solutions","text":"Amazon S3 with AWS Lake Formation : AWS service to build and manage Data Lakes. Azure Data Lake Storage : A highly scalable storage service on Microsoft Azure. Google Cloud Storage : Supports big data storage with integration for analytics and machine learning. Apache Hadoop HDFS : Open-source solution for large-scale data storage and processing.","title":"Examples of Data Lake Solutions"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#what-is-a-data-warehouse","text":"A Data Warehouse is a centralized repository optimized for storing structured data , typically involving ETL/ELT processes. Data is cleansed, transformed, and integrated from multiple sources to enable business intelligence (BI) and decision-making. Snowflake and other solutions support these processes, enabling data to be prepared and optimized for analytical queries.","title":"What is a Data Warehouse?"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#key-features-of-a-data-warehouse","text":"Historical Data Storage : Consolidates historical data from various sources over time. Data Transformation : Processes data for structured analysis. Business Intelligence Support : Enables complex analytical queries and reporting. Data Integration : Combines data from multiple sources for a unified view. Decision Support : Assists in trend analysis and data-driven decision-making.","title":"Key Features of a Data Warehouse"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#examples-of-data-warehouse-solutions","text":"Amazon Redshift : Fully managed on AWS, supports SQL queries. Google BigQuery : Serverless and scalable, designed for big data analysis. Snowflake : Cloud-based, with separate scalable storage and compute resources. Microsoft Azure Synapse : Integrated with Azure for big data and data warehousing.","title":"Examples of Data Warehouse Solutions"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#what-is-a-database","text":"A Database is a system designed primarily for storing and retrieving structured data , though some databases (like NoSQL) can handle semi-structured data such as JSON documents. Databases are crucial for transactional operations, ensuring fast data access and update capabilities for real-time applications.","title":"What is a Database?"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#key-features-of-a-database","text":"Transactional Processing : Manages real-time data transactions, such as in e-commerce or CRM. Data Storage : Organized in tables for structured data retrieval. Real-time Queries : Supports high-speed read and write operations. Consistency : Ensures data accuracy and reliability in transactions.","title":"Key Features of a Database"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#types-of-databases","text":"Relational Databases (SQL) : Store data in structured tables (e.g., MySQL, PostgreSQL). NoSQL Databases : Handle large volumes of semi-structured data, offering flexibility (e.g., MongoDB, Cassandra).","title":"Types of Databases"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#use-cases","text":"","title":"Use Cases"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#data-lake-use-cases","text":"IoT Data : Storing and processing sensor data in raw format for analysis. Machine Learning : Aggregating large volumes of training data for AI and ML models. Data Exploration : Flexible storage for exploratory analysis, enabling data scientists to uncover insights.","title":"Data Lake Use Cases"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#data-warehouse-use-cases","text":"Healthcare : Analyzing patient data to support treatment decisions. Marketing : Tracking campaign performance and customer interactions over time. Finance : Analyzing historical financial data for forecasting and trend analysis.","title":"Data Warehouse Use Cases"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#database-use-cases","text":"Retail : Managing real-time inventory and order transactions. CRM Systems : Storing and accessing customer information quickly for sales and support. Banking : Processing real-time transactions with data consistency and security.","title":"Database Use Cases"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#data-management-professionals","text":"Data Lake Engineer : Manages and maintains the Data Lake infrastructure, ensuring efficient storage and retrieval of raw data. Data Warehouse Analyst : Analyzes data and evaluates systems to recommend improvements for better insights. BI Analyst : Develops insights through data visualization and reporting, primarily from structured data. Database Administrator : Manages databases, ensuring data integrity, security, and performance.","title":"Data Management Professionals"},{"location":"concepts/data_engineering/03_warehouse_database_datalake/#conclusion","text":"Data Lakes , Data Warehouses , and Databases serve distinct roles in data management: Data Lake : Best for storing and exploring raw data for machine learning, big data analytics, and flexible data exploration. Data Warehouse : Best for structured historical data analysis, supporting business intelligence and strategic decision-making through ETL/ELT processes. Database : Best for real-time data storage and quick transactional operations, essential for applications requiring consistent, structured data . Organizations often leverage a combination of these solutions to meet comprehensive data management and analysis needs.","title":"Conclusion"},{"location":"concepts/data_engineering/04_scalability/","text":"4. Scalability in IT Systems Scalability refers to a system's ability to handle increased load or demand WITHOUT compromising PERFORMANCE or EFFICIENCY . As data volumes and user requests grow, a scalable system adapts seamlessly, ensuring stability and responsiveness. To achieve scalability, systems should embody specific characteristics, adhere to best practices, and utilize appropriate technologies. This documentation outlines these elements and explores various scalability types. Characteristics of a Scalable System Characteristic Description Flexibility Ability to integrate new components or modules without significant restructuring, allowing the system to expand as needed. Automation Reduction of manual processes, enhancing efficiency and reducing the potential for human error. Microservices Architecture Structuring the system into small, independent services, each responsible for a specific functionality. This design facilitates easier scaling and maintenance. Best Practices for Achieving Scalability Implement a Microservices Architecture Decompose the system into independent services that can be developed, deployed, and scaled individually. This approach enhances flexibility and resilience. Automate Data Pipelines Streamline data ingestion, cleaning, and transformation processes through automation to handle increasing data volumes efficiently. Utilize Load Balancing Distribute incoming network traffic across multiple servers to prevent any single server from becoming a bottleneck, thereby improving system reliability and performance. Monitor System Performance Continuously track system metrics to identify bottlenecks and anticipate scaling needs, ensuring proactive management of resources. Key Technologies Supporting Scalability Apache Kafka A distributed streaming platform capable of handling real-time data feeds, facilitating high-throughput and low-latency data processing. Apache Spark An open-source unified analytics engine for large-scale data processing, known for its speed and ease of use in big data applications. Amazon S3 A scalable object storage service offering high availability and durability, suitable for storing and retrieving any amount of data. NoSQL Databases (e.g., MongoDB, Cassandra) Designed for horizontal scalability, these databases handle large volumes of unstructured data across distributed systems. Kubernetes An open-source platform for automating deployment, scaling, and management of containerized applications, promoting efficient resource utilization. Rancher A complete container management platform that simplifies deploying and managing Kubernetes clusters, enhancing scalability and operational efficiency. Types of Scalability In short, there are two types of scalability: horizontal and vertical . Vertical Scalability : This is the easiest type. You increase the machine's RESOURCES so that it can continue to perform tasks without compromising performance, for example, by increasing RAM , HDD (Hard Disk Drive) memory , improving the CPU , improving the internet connection, etc. The major disadvantage is that resources are finite, so there will come a time when there will be no way to improve and if demand continues to be high, the system will not be able to handle it. Horizontal Scalability : Horizontal scalability works with the concept of MICROSERVICES , where you connect clusters/separate machines and they work together, dividing activities. For example, with Kafka, you can create several clusters on different machines and Zookeeper will manage the demand for tasks that arrive, dividing the tasks on different machines. Horizontal scalability is the process of add more machines to process a task. Type of Scalability Description Key Advantage Limitation Vertical Scalability Increases capacity by adding resources (CPU, RAM, storage) to a single server. Simpler to implement for smaller systems Limited by the maximum capacity of a single server Horizontal Scalability Expands capacity by adding more servers to distribute the workload. Enables nearly limitless growth and fault tolerance More complex setup, requiring distributed systems and load balancing Elastic Scalability Combines vertical and horizontal scaling, usually leveraging cloud resources to adjust resources automatically based on demand. Flexible, cost-effective (pay-as-you-go) Requires advanced configuration and cloud-based infrastructure How to Idenfity if your application is Scalable. Question Explanation Can the system handle more users without slowing down? If your application can support an increasing number of users without performance degradation, it likely has good scalability. Is the system modular, with separate components or services? Modular applications, often built with a microservices architecture, allow individual components to scale independently as needed. Does the application automatically allocate resources based on demand? Systems with elastic scalability, often cloud-based, can automatically add or reduce resources, optimizing performance and cost. Is there load balancing implemented? Load balancing distributes user requests across multiple servers, preventing overload on any single server, which is essential for scalability. Can the application support horizontal scaling? If you can add more servers to manage increased demand, your application can scale horizontally, which is ideal for handling larger loads. Are monitoring and automation tools in place? Systems with monitoring and automated processes are better prepared to scale since they can detect and respond to issues proactively. Is your database designed for distributed or NoSQL architecture? Distributed and NoSQL databases are generally more scalable, handling large data volumes and high transaction rates more effectively. Does your application support asynchronous processing? Asynchronous processing allows tasks to run independently, improving responsiveness and scalability, especially under heavy loads. Conclusion Achieving scalability is crucial for systems expected to grow in data volume and user demand. By incorporating flexible architectures, automating processes, and employing appropriate technologies, organizations can build systems that maintain performance and reliability as they scale. Continuous monitoring and proactive management are essential to ensure scalability remains effective and cost-efficient over time.","title":"04 - Scalability"},{"location":"concepts/data_engineering/04_scalability/#4-scalability-in-it-systems","text":"Scalability refers to a system's ability to handle increased load or demand WITHOUT compromising PERFORMANCE or EFFICIENCY . As data volumes and user requests grow, a scalable system adapts seamlessly, ensuring stability and responsiveness. To achieve scalability, systems should embody specific characteristics, adhere to best practices, and utilize appropriate technologies. This documentation outlines these elements and explores various scalability types.","title":"4. Scalability in IT Systems"},{"location":"concepts/data_engineering/04_scalability/#characteristics-of-a-scalable-system","text":"Characteristic Description Flexibility Ability to integrate new components or modules without significant restructuring, allowing the system to expand as needed. Automation Reduction of manual processes, enhancing efficiency and reducing the potential for human error. Microservices Architecture Structuring the system into small, independent services, each responsible for a specific functionality. This design facilitates easier scaling and maintenance.","title":"Characteristics of a Scalable System"},{"location":"concepts/data_engineering/04_scalability/#best-practices-for-achieving-scalability","text":"Implement a Microservices Architecture Decompose the system into independent services that can be developed, deployed, and scaled individually. This approach enhances flexibility and resilience. Automate Data Pipelines Streamline data ingestion, cleaning, and transformation processes through automation to handle increasing data volumes efficiently. Utilize Load Balancing Distribute incoming network traffic across multiple servers to prevent any single server from becoming a bottleneck, thereby improving system reliability and performance. Monitor System Performance Continuously track system metrics to identify bottlenecks and anticipate scaling needs, ensuring proactive management of resources.","title":"Best Practices for Achieving Scalability"},{"location":"concepts/data_engineering/04_scalability/#key-technologies-supporting-scalability","text":"Apache Kafka A distributed streaming platform capable of handling real-time data feeds, facilitating high-throughput and low-latency data processing. Apache Spark An open-source unified analytics engine for large-scale data processing, known for its speed and ease of use in big data applications. Amazon S3 A scalable object storage service offering high availability and durability, suitable for storing and retrieving any amount of data. NoSQL Databases (e.g., MongoDB, Cassandra) Designed for horizontal scalability, these databases handle large volumes of unstructured data across distributed systems. Kubernetes An open-source platform for automating deployment, scaling, and management of containerized applications, promoting efficient resource utilization. Rancher A complete container management platform that simplifies deploying and managing Kubernetes clusters, enhancing scalability and operational efficiency.","title":"Key Technologies Supporting Scalability"},{"location":"concepts/data_engineering/04_scalability/#types-of-scalability","text":"In short, there are two types of scalability: horizontal and vertical . Vertical Scalability : This is the easiest type. You increase the machine's RESOURCES so that it can continue to perform tasks without compromising performance, for example, by increasing RAM , HDD (Hard Disk Drive) memory , improving the CPU , improving the internet connection, etc. The major disadvantage is that resources are finite, so there will come a time when there will be no way to improve and if demand continues to be high, the system will not be able to handle it. Horizontal Scalability : Horizontal scalability works with the concept of MICROSERVICES , where you connect clusters/separate machines and they work together, dividing activities. For example, with Kafka, you can create several clusters on different machines and Zookeeper will manage the demand for tasks that arrive, dividing the tasks on different machines. Horizontal scalability is the process of add more machines to process a task. Type of Scalability Description Key Advantage Limitation Vertical Scalability Increases capacity by adding resources (CPU, RAM, storage) to a single server. Simpler to implement for smaller systems Limited by the maximum capacity of a single server Horizontal Scalability Expands capacity by adding more servers to distribute the workload. Enables nearly limitless growth and fault tolerance More complex setup, requiring distributed systems and load balancing Elastic Scalability Combines vertical and horizontal scaling, usually leveraging cloud resources to adjust resources automatically based on demand. Flexible, cost-effective (pay-as-you-go) Requires advanced configuration and cloud-based infrastructure","title":"Types of Scalability"},{"location":"concepts/data_engineering/04_scalability/#how-to-idenfity-if-your-application-is-scalable","text":"Question Explanation Can the system handle more users without slowing down? If your application can support an increasing number of users without performance degradation, it likely has good scalability. Is the system modular, with separate components or services? Modular applications, often built with a microservices architecture, allow individual components to scale independently as needed. Does the application automatically allocate resources based on demand? Systems with elastic scalability, often cloud-based, can automatically add or reduce resources, optimizing performance and cost. Is there load balancing implemented? Load balancing distributes user requests across multiple servers, preventing overload on any single server, which is essential for scalability. Can the application support horizontal scaling? If you can add more servers to manage increased demand, your application can scale horizontally, which is ideal for handling larger loads. Are monitoring and automation tools in place? Systems with monitoring and automated processes are better prepared to scale since they can detect and respond to issues proactively. Is your database designed for distributed or NoSQL architecture? Distributed and NoSQL databases are generally more scalable, handling large data volumes and high transaction rates more effectively. Does your application support asynchronous processing? Asynchronous processing allows tasks to run independently, improving responsiveness and scalability, especially under heavy loads.","title":"How to Idenfity if your application is Scalable."},{"location":"concepts/data_engineering/04_scalability/#conclusion","text":"Achieving scalability is crucial for systems expected to grow in data volume and user demand. By incorporating flexible architectures, automating processes, and employing appropriate technologies, organizations can build systems that maintain performance and reliability as they scale. Continuous monitoring and proactive management are essential to ensure scalability remains effective and cost-efficient over time.","title":"Conclusion"},{"location":"data_career_guide/01_Introduction_data_career/","text":"Comprehensive Data Career Guide: Data Analyst, Data Scientist, Data Engineer, and Machine Learning Engineer Introduction to the Data World The data field is one of the most dynamic and rapidly growing sectors, offering promising careers in areas such as data analysis, data engineering, data science, and machine learning. Professionals in these fields play crucial roles in companies looking to leverage data to generate insights and drive strategic decisions. Each of these roles has specific characteristics and responsibilities, although there are frequent overlaps and collaborations. Understanding the nuances of each role is essential for those looking to enter or advance in a data career. Key Roles in the Data Field 1. Data Analyst Main Function: The Data Analyst is responsible for collecting, cleaning, and analyzing data to create reports and dashboards that help companies monitor metrics and make informed decisions. While focused on data interpretation and visualization, Data Analysts play a crucial role in translating data into understandable and actionable narratives. Key Responsibilities: - Identifying patterns and trends in data. - Developing reports and dashboards for stakeholders. - Collaborating with other teams to understand metric needs and provide data-driven insights. Required Skills: Strong SQL skills, Excel, and data visualization tools (such as Tableau or Power BI), along with a solid understanding of business concepts. 2. Data Engineer Main Function: The Data Engineer designs and maintains data infrastructure, ensuring that data is available and organized for use by Data Analysts and Data Scientists. Their role involves creating data pipelines and storage systems that support large volumes of data, commonly known as big data. Key Responsibilities: - Building and maintaining data pipelines. - Integrating data from various sources and storing it in data warehouses. - Implementing governance and security solutions to protect data. Required Skills: Knowledge of SQL, Python, ETL tools, and cloud platforms like AWS, Google Cloud, or Azure. 3. Data Scientist Main Function: The Data Scientist explores, models, and extracts insights from data using advanced statistical and machine learning techniques. This professional focuses on creating predictive algorithms and models to solve complex problems and anticipate trends. Key Responsibilities: - Developing machine learning models and algorithms. - Statistical analysis and modeling to forecast future patterns. - Collaborating with Data Engineers to access and prepare data for modeling. Required Skills: Advanced skills in statistics, programming (Python or R), machine learning, and familiarity with frameworks like TensorFlow or PyTorch. 4. Machine Learning Engineer Main Function: The Machine Learning Engineer is responsible for deploying models developed by Data Scientists into production. They ensure these models are scalable and properly integrated into the company\u2019s systems. This professional operates at the intersection of machine learning and software engineering. P.S. The role of an ML Engineer varies significantly by company and isn\u2019t yet clearly defined\u2014it depends on the project the company is working on. Generally, though, it\u2019s a profession that provides support for trained models, meaning ML Engineers handle the deployment of applications that use models created by data scientists. Key Responsibilities: - Implementing and automating machine learning pipelines. - Integrating models into production systems using CI/CD. - Scaling and continuously monitoring models to ensure robustness. Required Skills: A strong foundation in machine learning, experience with DevOps, programming languages (Python, Java), and cloud computing platforms. Differences and Similarities Between Data Scientist and Machine Learning Engineer While Data Scientists and Machine Learning Engineers frequently collaborate, their roles diverge. The Data Scientist creates and trains the model, while the ML Engineer is responsible for integrating and monitoring it in a production environment. Both require machine learning skills, but the ML Engineer focuses more on scalability and the practical operation of models. Data Scientist vs. Machine Learning Engineer: - Data Scientist: Focuses on analysis and model creation. - ML Engineer: Focuses on implementation, scalability, and model monitoring in production environments. Data Team Structure and Professional Ratios The number of data professionals in an organization can vary depending on the size and focus of the company: Data Analysts generally make up the largest portion, with ratios that can reach up to 10 Analysts for every Data Scientist or Data Engineer. Data Engineers constitute a moderate number, often in ratios of 1:3 or 1:5 relative to Data Analysts, especially in large companies that handle big data. Data Scientists are typically less numerous, with approximately 1 Scientist for every 5 Data Analysts. ML Engineers are often even more specialized, with a ratio that can be 1 for every 5 Data Scientists, depending on the volume and complexity of models in production. This table provides an overview of each role in the data field, detailing their primary functions, responsibilities, necessary skills, typical team proportion, and areas of specialization. Area Main Function Key Responsibilities Required Skills Team Proportion Area of Specialization Data Analyst Transform data into actionable insights through analysis and visualizations. Data analysis, report and dashboard creation, pattern identification. SQL, Excel, Power BI/Tableau, business knowledge. High - up to 10 Analysts for each Scientist/Engineer. Data Analysis and Visualization Data Engineer Create and maintain infrastructure for data collection, storage, and integration. Pipeline development, data integration, data warehouse maintenance. SQL, Python, ETL, cloud computing (AWS, Google Cloud, Azure). Moderate - generally 1 for every 3 to 5 Analysts. Data Warehousing and Big Data Data Scientist Develop predictive models and analyze data for advanced insights. ML model creation, statistical analysis, data exploration. Statistics, Python/R, machine learning, TensorFlow/PyTorch. Moderate - about 1 for every 5 to 10 Analysts. Advanced Statistics and Machine Learning Machine Learning Engineer Implement, scale, and monitor machine learning models in production environments. ML pipeline automation, model integration in production, continuous monitoring. Machine learning, DevOps, Python/Java, cloud computing. Low - approximately 1 for every 5 Data Scientists. Scalability and Machine Learning Integration Career Paths and Areas of Specialization Career Paths and Levels of Difficulty in Data Roles Starting a career in data typically follows a progression from roles with broader, foundational skills to those with deeper specialization. Here\u2019s an overview of how you can transition between these core data roles, leveraging transferable skills along the way: 1. Data Analyst (Beginner Level) Starting Point: The Data Analyst role is often the most accessible entry point into the data field. It requires foundational skills in data analysis, visualization, and understanding basic business metrics. Key Skills: SQL, data visualization tools (e.g., Tableau, Power BI), Excel, and a general understanding of data concepts. Transferable Experience: Skills in data wrangling, visualization, and interpreting trends can be directly applied to more specialized roles. 2. Data Engineer (Intermediate Level) Next Step: Transitioning from a Data Analyst to a Data Engineer involves expanding technical skills in data pipeline development, cloud infrastructure, and data architecture. Key Skills: Proficiency in SQL, Python, ETL processes, and experience with cloud platforms (AWS, Google Cloud, or Azure). Transferable Experience: Understanding data workflows and how data is consumed by analysts can be highly beneficial as you build infrastructure to support data usage across the organization. 3. Data Scientist (Advanced Level) Further Specialization: Moving into Data Science from Data Engineering or Data Analysis involves a deeper dive into statistical analysis, machine learning, and predictive modeling. Key Skills: Advanced knowledge of Python or R, machine learning frameworks (e.g., TensorFlow, PyTorch), and statistical analysis. Transferable Experience: Experience with data wrangling and understanding how data is structured in pipelines helps in developing and implementing complex models. 4. Machine Learning Engineer (Expert Level) Highest Specialization: The role of a Machine Learning Engineer is often pursued after significant experience in Data Science or Data Engineering. It requires expertise in deploying, scaling, and monitoring machine learning models in production environments. Key Skills: Strong understanding of machine learning algorithms, DevOps, cloud computing, and programming (Python, Java). Transferable Experience: Familiarity with machine learning models from Data Science roles and the ability to handle data at scale from Data Engineering roles are critical as you implement these models in production. Transitioning Through Roles While each role has its own set of challenges, starting as a Data Analyst allows you to build a solid foundation in data skills, which are applicable across more advanced roles. Transitioning into Data Engineering or Data Science leverages both technical and analytical skills gained as an Analyst. Finally, moving into Machine Learning Engineering requires a blend of experiences from previous roles, combining model-building with the engineering skills necessary for scalable deployment. By progressing through these roles, you can build a well-rounded data career, each step enhancing your understanding and skillset for the next. Conclusion The data field is vibrant and full of opportunities for those seeking a tangible impact on the business world and beyond. Whether improving process efficiency or creating innovative solutions, data professionals play an essential role. Specializations continue to expand, allowing individuals to choose paths that align with their passions and skills while collaborating to shape the data-driven future.","title":"01 - introduction Data Career"},{"location":"data_career_guide/01_Introduction_data_career/#comprehensive-data-career-guide-data-analyst-data-scientist-data-engineer-and-machine-learning-engineer","text":"","title":"Comprehensive Data Career Guide: Data Analyst, Data Scientist, Data Engineer, and Machine Learning Engineer"},{"location":"data_career_guide/01_Introduction_data_career/#introduction-to-the-data-world","text":"The data field is one of the most dynamic and rapidly growing sectors, offering promising careers in areas such as data analysis, data engineering, data science, and machine learning. Professionals in these fields play crucial roles in companies looking to leverage data to generate insights and drive strategic decisions. Each of these roles has specific characteristics and responsibilities, although there are frequent overlaps and collaborations. Understanding the nuances of each role is essential for those looking to enter or advance in a data career.","title":"Introduction to the Data World"},{"location":"data_career_guide/01_Introduction_data_career/#key-roles-in-the-data-field","text":"","title":"Key Roles in the Data Field"},{"location":"data_career_guide/01_Introduction_data_career/#1-data-analyst","text":"Main Function: The Data Analyst is responsible for collecting, cleaning, and analyzing data to create reports and dashboards that help companies monitor metrics and make informed decisions. While focused on data interpretation and visualization, Data Analysts play a crucial role in translating data into understandable and actionable narratives. Key Responsibilities: - Identifying patterns and trends in data. - Developing reports and dashboards for stakeholders. - Collaborating with other teams to understand metric needs and provide data-driven insights. Required Skills: Strong SQL skills, Excel, and data visualization tools (such as Tableau or Power BI), along with a solid understanding of business concepts.","title":"1. Data Analyst"},{"location":"data_career_guide/01_Introduction_data_career/#2-data-engineer","text":"Main Function: The Data Engineer designs and maintains data infrastructure, ensuring that data is available and organized for use by Data Analysts and Data Scientists. Their role involves creating data pipelines and storage systems that support large volumes of data, commonly known as big data. Key Responsibilities: - Building and maintaining data pipelines. - Integrating data from various sources and storing it in data warehouses. - Implementing governance and security solutions to protect data. Required Skills: Knowledge of SQL, Python, ETL tools, and cloud platforms like AWS, Google Cloud, or Azure.","title":"2. Data Engineer"},{"location":"data_career_guide/01_Introduction_data_career/#3-data-scientist","text":"Main Function: The Data Scientist explores, models, and extracts insights from data using advanced statistical and machine learning techniques. This professional focuses on creating predictive algorithms and models to solve complex problems and anticipate trends. Key Responsibilities: - Developing machine learning models and algorithms. - Statistical analysis and modeling to forecast future patterns. - Collaborating with Data Engineers to access and prepare data for modeling. Required Skills: Advanced skills in statistics, programming (Python or R), machine learning, and familiarity with frameworks like TensorFlow or PyTorch.","title":"3. Data Scientist"},{"location":"data_career_guide/01_Introduction_data_career/#4-machine-learning-engineer","text":"Main Function: The Machine Learning Engineer is responsible for deploying models developed by Data Scientists into production. They ensure these models are scalable and properly integrated into the company\u2019s systems. This professional operates at the intersection of machine learning and software engineering. P.S. The role of an ML Engineer varies significantly by company and isn\u2019t yet clearly defined\u2014it depends on the project the company is working on. Generally, though, it\u2019s a profession that provides support for trained models, meaning ML Engineers handle the deployment of applications that use models created by data scientists. Key Responsibilities: - Implementing and automating machine learning pipelines. - Integrating models into production systems using CI/CD. - Scaling and continuously monitoring models to ensure robustness. Required Skills: A strong foundation in machine learning, experience with DevOps, programming languages (Python, Java), and cloud computing platforms.","title":"4. Machine Learning Engineer"},{"location":"data_career_guide/01_Introduction_data_career/#differences-and-similarities-between-data-scientist-and-machine-learning-engineer","text":"While Data Scientists and Machine Learning Engineers frequently collaborate, their roles diverge. The Data Scientist creates and trains the model, while the ML Engineer is responsible for integrating and monitoring it in a production environment. Both require machine learning skills, but the ML Engineer focuses more on scalability and the practical operation of models. Data Scientist vs. Machine Learning Engineer: - Data Scientist: Focuses on analysis and model creation. - ML Engineer: Focuses on implementation, scalability, and model monitoring in production environments.","title":"Differences and Similarities Between Data Scientist and Machine Learning Engineer"},{"location":"data_career_guide/01_Introduction_data_career/#data-team-structure-and-professional-ratios","text":"The number of data professionals in an organization can vary depending on the size and focus of the company: Data Analysts generally make up the largest portion, with ratios that can reach up to 10 Analysts for every Data Scientist or Data Engineer. Data Engineers constitute a moderate number, often in ratios of 1:3 or 1:5 relative to Data Analysts, especially in large companies that handle big data. Data Scientists are typically less numerous, with approximately 1 Scientist for every 5 Data Analysts. ML Engineers are often even more specialized, with a ratio that can be 1 for every 5 Data Scientists, depending on the volume and complexity of models in production. This table provides an overview of each role in the data field, detailing their primary functions, responsibilities, necessary skills, typical team proportion, and areas of specialization. Area Main Function Key Responsibilities Required Skills Team Proportion Area of Specialization Data Analyst Transform data into actionable insights through analysis and visualizations. Data analysis, report and dashboard creation, pattern identification. SQL, Excel, Power BI/Tableau, business knowledge. High - up to 10 Analysts for each Scientist/Engineer. Data Analysis and Visualization Data Engineer Create and maintain infrastructure for data collection, storage, and integration. Pipeline development, data integration, data warehouse maintenance. SQL, Python, ETL, cloud computing (AWS, Google Cloud, Azure). Moderate - generally 1 for every 3 to 5 Analysts. Data Warehousing and Big Data Data Scientist Develop predictive models and analyze data for advanced insights. ML model creation, statistical analysis, data exploration. Statistics, Python/R, machine learning, TensorFlow/PyTorch. Moderate - about 1 for every 5 to 10 Analysts. Advanced Statistics and Machine Learning Machine Learning Engineer Implement, scale, and monitor machine learning models in production environments. ML pipeline automation, model integration in production, continuous monitoring. Machine learning, DevOps, Python/Java, cloud computing. Low - approximately 1 for every 5 Data Scientists. Scalability and Machine Learning Integration","title":"Data Team Structure and Professional Ratios"},{"location":"data_career_guide/01_Introduction_data_career/#career-paths-and-areas-of-specialization","text":"","title":"Career Paths and Areas of Specialization"},{"location":"data_career_guide/01_Introduction_data_career/#career-paths-and-levels-of-difficulty-in-data-roles","text":"Starting a career in data typically follows a progression from roles with broader, foundational skills to those with deeper specialization. Here\u2019s an overview of how you can transition between these core data roles, leveraging transferable skills along the way:","title":"Career Paths and Levels of Difficulty in Data Roles"},{"location":"data_career_guide/01_Introduction_data_career/#1-data-analyst-beginner-level","text":"Starting Point: The Data Analyst role is often the most accessible entry point into the data field. It requires foundational skills in data analysis, visualization, and understanding basic business metrics. Key Skills: SQL, data visualization tools (e.g., Tableau, Power BI), Excel, and a general understanding of data concepts. Transferable Experience: Skills in data wrangling, visualization, and interpreting trends can be directly applied to more specialized roles.","title":"1. Data Analyst (Beginner Level)"},{"location":"data_career_guide/01_Introduction_data_career/#2-data-engineer-intermediate-level","text":"Next Step: Transitioning from a Data Analyst to a Data Engineer involves expanding technical skills in data pipeline development, cloud infrastructure, and data architecture. Key Skills: Proficiency in SQL, Python, ETL processes, and experience with cloud platforms (AWS, Google Cloud, or Azure). Transferable Experience: Understanding data workflows and how data is consumed by analysts can be highly beneficial as you build infrastructure to support data usage across the organization.","title":"2. Data Engineer (Intermediate Level)"},{"location":"data_career_guide/01_Introduction_data_career/#3-data-scientist-advanced-level","text":"Further Specialization: Moving into Data Science from Data Engineering or Data Analysis involves a deeper dive into statistical analysis, machine learning, and predictive modeling. Key Skills: Advanced knowledge of Python or R, machine learning frameworks (e.g., TensorFlow, PyTorch), and statistical analysis. Transferable Experience: Experience with data wrangling and understanding how data is structured in pipelines helps in developing and implementing complex models.","title":"3. Data Scientist (Advanced Level)"},{"location":"data_career_guide/01_Introduction_data_career/#4-machine-learning-engineer-expert-level","text":"Highest Specialization: The role of a Machine Learning Engineer is often pursued after significant experience in Data Science or Data Engineering. It requires expertise in deploying, scaling, and monitoring machine learning models in production environments. Key Skills: Strong understanding of machine learning algorithms, DevOps, cloud computing, and programming (Python, Java). Transferable Experience: Familiarity with machine learning models from Data Science roles and the ability to handle data at scale from Data Engineering roles are critical as you implement these models in production.","title":"4. Machine Learning Engineer (Expert Level)"},{"location":"data_career_guide/01_Introduction_data_career/#transitioning-through-roles","text":"While each role has its own set of challenges, starting as a Data Analyst allows you to build a solid foundation in data skills, which are applicable across more advanced roles. Transitioning into Data Engineering or Data Science leverages both technical and analytical skills gained as an Analyst. Finally, moving into Machine Learning Engineering requires a blend of experiences from previous roles, combining model-building with the engineering skills necessary for scalable deployment. By progressing through these roles, you can build a well-rounded data career, each step enhancing your understanding and skillset for the next.","title":"Transitioning Through Roles"},{"location":"data_career_guide/01_Introduction_data_career/#conclusion","text":"The data field is vibrant and full of opportunities for those seeking a tangible impact on the business world and beyond. Whether improving process efficiency or creating innovative solutions, data professionals play an essential role. Specializations continue to expand, allowing individuals to choose paths that align with their passions and skills while collaborating to shape the data-driven future.","title":"Conclusion"},{"location":"data_career_guide/02_data_scientist/","text":"","title":"02 data scientist"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/","text":"1. Deploying a Docker Container on AWS EC2 from Scratch This guide will walk you through the steps to deploy your Docker container on an AWS EC2 instance. Prerequisites AWS Account Docker installed on your local machine An application to deploy (Dockerized) 1. Build Your Application First, create and test your application locally. Once the app is ready, move to the next steps to containerize it using Docker. 2. Create a Docker Image Once your app is built, you need to create a Docker image for it, for example: docker-user/image-name:tag sudo docker build -t miguelzeph/flask-news:latest . 2.1 Option A: Create a .tar Image for SSH Transfer If you plan to transfer the image to your EC2 instance via SSH, create a tarball ( .tar ) of your Docker image by running: docker save -o <image-name>.tar <image-name> 2.2 Option B: Push to Docker Hub Alternatively, you can push your image to Docker Hub: 1.Login: sudo docker login 2.Push your image: docker push <dockerhub-username>/<repository>:<tag> # ex: docker push miguelzeph/flask-news:latest Once your image is on Docker Hub, you can pull it from the EC2 instance later. 3. Launch an EC2 Instance Go to the AWS Management Console and create an EC2 instance. Choose the appropriate AMI (Amazon Machine Image) and instance type (e.g., t2.micro). 3.1 Modify Storage Volume (Optional) Increase the default storage volume size up to 30GB \u2014 this is covered under the free tier. 3.2 Save the SSH Key During instance creation, generate and download a key pair (.pem file). You'll need this to SSH into your instance. 3.3 Access the EC2 Instance Once the EC2 instance is running, you can access it in two ways: SSH into the EC2 Instance If you are accessing from your terminal, use SSH with the .pem key to connect: ssh -i your-key.pem ec2-user@your-ec2-public-ip Or You can also connect directly through the AWS Console: connect via AWS Console (easier) In the EC2 Dashboard, find your instance. Click Connect. Choose the EC2 Instance Connect tab and click Connect. 3.3 Install Docker on EC2 Once the instance is running, SSH into (or access from AWS console clicking in \"Connect\") the EC2 instance and install Docker. 3.4 Load the Docker Image on EC2 Now, you can either: Option A : Copy the .tar file using scp scp -i <your-key>.pem <image-name>.tar ubuntu@<ec2-public-ip>:~/ Then load the image: docker load -i <image-name>.tar Option B : Pull the image directly from Docker Hub: docker login docker pull <dockerhub-username>/<repository>:<tag> 3.5 Run the Container Run your Docker container on the EC2 instance: docker run -d -p <host-port>:<container-port> <image-name> # Example sudo docker run -d -p 8000:8000 miguelzeph/flask-news:latest gunicorn -w 4 -b 0.0.0.0:8000 app:app P.S -> In case you have chosen the B process (using DockerHub), create a file to configure your own app (because the conf file used to create the image was just an example to not push sensitive information to the repository). # Create a file for your configuration according to your project nano config_flask_news.yml Copy this file to the Container that is running with the application (get the container-id using ps command). sudo docker cp config_flask_news.yml <container-id>:/app/config_example.yml Restart the Container if necessary sudo docker stop <container-id> sudo docker start <container-id> 4. Configure EC2 Security Group Create or modify a security group for your EC2 instance to allow incoming traffic on the ports needed by your application (e.g., HTTP: 80 or a custom port). Go to the EC2 Dashboard > Security Groups. Select the security group associated with your EC2 instance. Click Edit Inbound Rules and add the following rules SSH (to allow SSH access): bash Type: SSH Protocol: TCP Port Range: 22 Source: 0.0.0.0/0 Custom TCP Rule (to allow access to your application) for the necessary ports and set the source to 0.0.0.0/0 or restrict it to specific IPs: bash Type: Custom TCP Rule Protocol: TCP Port Range: <your-app-port> Source: 0.0.0.0/0 Make sure the port you're exposing is open and accessible from external sources. This will allow you to access both your application and SSH into your EC2 instance. 5. Assign an Elastic IP To ensure your public IP remains static (doesn\u2019t change when you stop/start your instance): Navigate to Elastic IPs in the EC2 Console. Allocate a new Elastic IP. Associate the Elastic IP with your running EC2 instance. Note : Some cloud databases (like AWS RDS) require you to whitelist this static IP so that your application can access the database. 6. Test Your Application Make sure the necessary ports are open in the security group, and access your application in the browser: http://<your-static-ip>:<host-port>","title":"01 - EC2 Instance - Deploying Docker"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#1-deploying-a-docker-container-on-aws-ec2-from-scratch","text":"This guide will walk you through the steps to deploy your Docker container on an AWS EC2 instance.","title":"1. Deploying a Docker Container on AWS EC2 from Scratch"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#prerequisites","text":"AWS Account Docker installed on your local machine An application to deploy (Dockerized)","title":"Prerequisites"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#1-build-your-application","text":"First, create and test your application locally. Once the app is ready, move to the next steps to containerize it using Docker.","title":"1. Build Your Application"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#2-create-a-docker-image","text":"Once your app is built, you need to create a Docker image for it, for example: docker-user/image-name:tag sudo docker build -t miguelzeph/flask-news:latest .","title":"2. Create a Docker Image"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#21-option-a-create-a-tar-image-for-ssh-transfer","text":"If you plan to transfer the image to your EC2 instance via SSH, create a tarball ( .tar ) of your Docker image by running: docker save -o <image-name>.tar <image-name>","title":"2.1 Option A: Create a .tar Image for SSH Transfer"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#22-option-b-push-to-docker-hub","text":"Alternatively, you can push your image to Docker Hub: 1.Login: sudo docker login 2.Push your image: docker push <dockerhub-username>/<repository>:<tag> # ex: docker push miguelzeph/flask-news:latest Once your image is on Docker Hub, you can pull it from the EC2 instance later.","title":"2.2 Option B: Push to Docker Hub"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#3-launch-an-ec2-instance","text":"Go to the AWS Management Console and create an EC2 instance. Choose the appropriate AMI (Amazon Machine Image) and instance type (e.g., t2.micro).","title":"3. Launch an EC2 Instance"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#31-modify-storage-volume-optional","text":"Increase the default storage volume size up to 30GB \u2014 this is covered under the free tier.","title":"3.1 Modify Storage Volume (Optional)"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#32-save-the-ssh-key","text":"During instance creation, generate and download a key pair (.pem file). You'll need this to SSH into your instance.","title":"3.2 Save the SSH Key"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#33-access-the-ec2-instance","text":"Once the EC2 instance is running, you can access it in two ways: SSH into the EC2 Instance If you are accessing from your terminal, use SSH with the .pem key to connect: ssh -i your-key.pem ec2-user@your-ec2-public-ip Or You can also connect directly through the AWS Console: connect via AWS Console (easier) In the EC2 Dashboard, find your instance. Click Connect. Choose the EC2 Instance Connect tab and click Connect.","title":"3.3 Access the EC2 Instance"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#33-install-docker-on-ec2","text":"Once the instance is running, SSH into (or access from AWS console clicking in \"Connect\") the EC2 instance and install Docker.","title":"3.3 Install Docker on EC2"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#34-load-the-docker-image-on-ec2","text":"Now, you can either: Option A : Copy the .tar file using scp scp -i <your-key>.pem <image-name>.tar ubuntu@<ec2-public-ip>:~/ Then load the image: docker load -i <image-name>.tar Option B : Pull the image directly from Docker Hub: docker login docker pull <dockerhub-username>/<repository>:<tag>","title":"3.4 Load the Docker Image on EC2"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#35-run-the-container","text":"Run your Docker container on the EC2 instance: docker run -d -p <host-port>:<container-port> <image-name> # Example sudo docker run -d -p 8000:8000 miguelzeph/flask-news:latest gunicorn -w 4 -b 0.0.0.0:8000 app:app P.S -> In case you have chosen the B process (using DockerHub), create a file to configure your own app (because the conf file used to create the image was just an example to not push sensitive information to the repository). # Create a file for your configuration according to your project nano config_flask_news.yml Copy this file to the Container that is running with the application (get the container-id using ps command). sudo docker cp config_flask_news.yml <container-id>:/app/config_example.yml Restart the Container if necessary sudo docker stop <container-id> sudo docker start <container-id>","title":"3.5 Run the Container"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#4-configure-ec2-security-group","text":"Create or modify a security group for your EC2 instance to allow incoming traffic on the ports needed by your application (e.g., HTTP: 80 or a custom port). Go to the EC2 Dashboard > Security Groups. Select the security group associated with your EC2 instance. Click Edit Inbound Rules and add the following rules SSH (to allow SSH access): bash Type: SSH Protocol: TCP Port Range: 22 Source: 0.0.0.0/0 Custom TCP Rule (to allow access to your application) for the necessary ports and set the source to 0.0.0.0/0 or restrict it to specific IPs: bash Type: Custom TCP Rule Protocol: TCP Port Range: <your-app-port> Source: 0.0.0.0/0 Make sure the port you're exposing is open and accessible from external sources. This will allow you to access both your application and SSH into your EC2 instance.","title":"4. Configure EC2 Security Group"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#5-assign-an-elastic-ip","text":"To ensure your public IP remains static (doesn\u2019t change when you stop/start your instance): Navigate to Elastic IPs in the EC2 Console. Allocate a new Elastic IP. Associate the Elastic IP with your running EC2 instance. Note : Some cloud databases (like AWS RDS) require you to whitelist this static IP so that your application can access the database.","title":"5. Assign an Elastic IP"},{"location":"documentation/aws/01_deploy_docker_AWS_EC2/#6-test-your-application","text":"Make sure the necessary ports are open in the security group, and access your application in the browser: http://<your-static-ip>:<host-port>","title":"6. Test Your Application"},{"location":"documentation/aws/02_AWS_EC2_domain_http/","text":"2. Mapping an AWS EC2 Application to a Custom Domain over HTTP 1. Create Your Domain To access your application using a custom domain instead of the Elastic IP, you\u2019ll need to register a domain name. You can register a domain through AWS Route 53 or any other domain registrar (e.g., GoDaddy, Namecheap). After registering the domain, you'll configure the DNS settings to point to your EC2 instance\u2019s Elastic IP. 2. Register DNS A Record (IPv4) for Your Domain/Subdomain Once you have your domain, you need to configure the DNS settings to associate your domain with the EC2 instance. In your domain registrar's control panel (or Route 53 if using AWS), create an A record that points to your EC2 Elastic IP address. For example: Record Name Type Value/Route Traffic to TTL www.apps.com A <your-elastic-ip> 300 amaralapps.com A <your-elastic-ip> 300 If you're using subdomains, set the Name field to the appropriate subdomain (e.g., app.yourdomain.com ). 3. Configure Nginx for HTTP Access Now that your domain is linked to your EC2 instance, you\u2019ll need to configure Nginx to serve your application over HTTP. Install Nginx (if not installed yet): sudo apt update sudo apt install nginx Configure Nginx for your application: Create a configuration file in /etc/nginx/sites-available/your-domain : sudo nano /etc/nginx/sites-available/your-domain Add the following configuration to proxy traffic to your Docker container: server { listen 80; server_name yourdomain.com www.yourdomain.com; location / { proxy_pass http://localhost:<your-app-port>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Enable the configuration by creating a symbolic link from the sites-available directory to sites-enabled : sudo ln -s /etc/nginx/sites-available/your-domain /etc/nginx/sites-enabled/ Test the Nginx configuration to make sure there are no errors: sudo nginx -t Restart Nginx to apply the new configuration: sudo systemctl restart nginx At this point, Nginx should be serving your application via your domain over HTTP. 4. Access Your Application Once Nginx is configured and running, visit your application in the browser using your custom domain: http://yourdomain.com OBSERVATION - If it still not working Check DNS and create a A Record Record name : Fill with the domain name you want ex: www.covid.amaralapps.com Select the record type as A - IPv4 address . In the Value field, enter your server\u2019s IP address, , ex: 41.198.251.100. . Select Simple routing and click Create records to save. Verifique o Firewall no Servidor: sudo ufw status sudo ufw enable # if is enable you can activate sudo ufw allow 80 sudo ufw allow 8502 Check the security group TCP: check if you opened the port 80 (http) and 443 (https) Check again the file /etc/nginx/sites-available/your-domain restart nginx Summary of Commands Install Nginx: sudo apt update && sudo apt install nginx Create Nginx config: sudo nano /etc/nginx/sites-available/your-domain Add proxy settings in the config file: server { listen 80; server_name yourdomain.com www.yourdomain.com; location / { proxy_pass http://localhost:<your-app-port>; } } P.S. - Using streamlit the file is something like this server { listen 80; server_name covid.amaralapps.com www.covid.amaralapps.com; location / { proxy_pass http://localhost:8502; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Configura\u00e7\u00f5es adicionais para WebSocket proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } Enable the config: sudo ln -s /etc/nginx/sites-available/your-domain /etc/nginx/sites-enabled/ Test Nginx: sudo nginx -t Restart Nginx: sudo systemctl restart nginx Access your app: http://yourdomain.com","title":"02 - EC2 App - Custom Domain HTTP"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#2-mapping-an-aws-ec2-application-to-a-custom-domain-over-http","text":"","title":"2. Mapping an AWS EC2 Application to a Custom Domain over HTTP"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#1-create-your-domain","text":"To access your application using a custom domain instead of the Elastic IP, you\u2019ll need to register a domain name. You can register a domain through AWS Route 53 or any other domain registrar (e.g., GoDaddy, Namecheap). After registering the domain, you'll configure the DNS settings to point to your EC2 instance\u2019s Elastic IP.","title":"1. Create Your Domain"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#2-register-dns-a-record-ipv4-for-your-domainsubdomain","text":"Once you have your domain, you need to configure the DNS settings to associate your domain with the EC2 instance. In your domain registrar's control panel (or Route 53 if using AWS), create an A record that points to your EC2 Elastic IP address. For example: Record Name Type Value/Route Traffic to TTL www.apps.com A <your-elastic-ip> 300 amaralapps.com A <your-elastic-ip> 300 If you're using subdomains, set the Name field to the appropriate subdomain (e.g., app.yourdomain.com ).","title":"2. Register DNS A Record (IPv4) for Your Domain/Subdomain"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#3-configure-nginx-for-http-access","text":"Now that your domain is linked to your EC2 instance, you\u2019ll need to configure Nginx to serve your application over HTTP. Install Nginx (if not installed yet): sudo apt update sudo apt install nginx Configure Nginx for your application: Create a configuration file in /etc/nginx/sites-available/your-domain : sudo nano /etc/nginx/sites-available/your-domain Add the following configuration to proxy traffic to your Docker container: server { listen 80; server_name yourdomain.com www.yourdomain.com; location / { proxy_pass http://localhost:<your-app-port>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Enable the configuration by creating a symbolic link from the sites-available directory to sites-enabled : sudo ln -s /etc/nginx/sites-available/your-domain /etc/nginx/sites-enabled/ Test the Nginx configuration to make sure there are no errors: sudo nginx -t Restart Nginx to apply the new configuration: sudo systemctl restart nginx At this point, Nginx should be serving your application via your domain over HTTP.","title":"3. Configure Nginx for HTTP Access"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#4-access-your-application","text":"Once Nginx is configured and running, visit your application in the browser using your custom domain: http://yourdomain.com","title":"4. Access Your Application"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#observation-if-it-still-not-working","text":"Check DNS and create a A Record Record name : Fill with the domain name you want ex: www.covid.amaralapps.com Select the record type as A - IPv4 address . In the Value field, enter your server\u2019s IP address, , ex: 41.198.251.100. . Select Simple routing and click Create records to save. Verifique o Firewall no Servidor: sudo ufw status sudo ufw enable # if is enable you can activate sudo ufw allow 80 sudo ufw allow 8502 Check the security group TCP: check if you opened the port 80 (http) and 443 (https) Check again the file /etc/nginx/sites-available/your-domain restart nginx","title":"OBSERVATION - If it still not working"},{"location":"documentation/aws/02_AWS_EC2_domain_http/#summary-of-commands","text":"Install Nginx: sudo apt update && sudo apt install nginx Create Nginx config: sudo nano /etc/nginx/sites-available/your-domain Add proxy settings in the config file: server { listen 80; server_name yourdomain.com www.yourdomain.com; location / { proxy_pass http://localhost:<your-app-port>; } } P.S. - Using streamlit the file is something like this server { listen 80; server_name covid.amaralapps.com www.covid.amaralapps.com; location / { proxy_pass http://localhost:8502; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Configura\u00e7\u00f5es adicionais para WebSocket proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } Enable the config: sudo ln -s /etc/nginx/sites-available/your-domain /etc/nginx/sites-enabled/ Test Nginx: sudo nginx -t Restart Nginx: sudo systemctl restart nginx Access your app: http://yourdomain.com","title":"Summary of Commands"},{"location":"documentation/aws/03_AWS_EC2_domain_https/","text":"3. Enable HTTPS for AWS EC2 Application In this guide, you will learn how to secure your application hosted on AWS EC2 by enabling HTTPS with free SSL certificates from Let's Encrypt using Certbot and configuring Nginx . Prerequisites You should already have your domain configured and your application running over HTTP (as described in the previous guide). Nginx installed and configured for your domain. Certbot installed (for generating SSL certificates). 1. Install Certbot and Generate SSL Certificates First, ensure Certbot is installed. On Ubuntu, run: sudo apt update sudo apt install certbot python3-certbot-nginx After installation, use Certbot to automatically generate and configure the SSL certificates for your domain: sudo certbot --nginx Certbot will automatically detect your Nginx configuration and list the domains configured. When prompted, choose the domain number that corresponds to the domain you want to secure. Certbot will handle the certificate generation and automatically update your Nginx configuration to use SSL. 2. Verify and Update Nginx Configuration Certbot automatically modifies your Nginx configuration to enable HTTPS. However, it's good to verify or manually adjust the configuration if needed. Open your Nginx configuration file for your domain: sudo nano /etc/nginx/sites-available/your-domain Ensure that the Nginx configuration has been updated to redirect HTTP traffic to HTTPS and use the SSL certificates. It should look something like this:Ensure that the Nginx configuration has been updated to redirect HTTP traffic to HTTPS and use the SSL certificates. It should look something like this: server { listen 80; server_name yourdomain.com www.yourdomain.com; return 301 https://$host$request_uri; } server { listen 443 ssl; server_name yourdomain.com www.yourdomain.com; ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; location / { proxy_pass http://localhost:<your-app-port>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Save and close the file. 3. Restart Nginx After generating the certificates and updating the configuration, restart Nginx to apply the changes: sudo systemctl restart nginx 4. Test the Application Over HTTPS Now that Nginx is serving your application over HTTPS: Open your web browser and navigate to: https://yourdomain.com Ensure that your application is running and that there\u2019s a padlock icon next to your URL in the browser, indicating the SSL certificate is active. To confirm that HTTP traffic is being properly redirected to HTTPS, try accessing the site using http://yourdomain.com . It should automatically redirect to the secure https:// version. Set Up Automatic Certificate Renewal Let\u2019s Encrypt certificates expire every 90 days, but Certbot includes a built-in automatic renewal system. To ensure your certificates renew automatically, you can set up a cron job. Run the following command to test the automatic renewal process: sudo certbot renew --dry-run Certbot installs a cron job to automatically renew certificates, so no further action should be needed. Summary of Commands Install Certbot: sudo apt install certbot python3-certbot-nginx Generate and configure SSL certificates: sudo certbot --nginx (Optional) Edit Nginx config: sudo nano /etc/nginx/sites-available/your-domain Restart Nginx: sudo systemctl restart nginx Test HTTPS: https://yourdomain.com","title":"03 - EC2 App - Custom Domain HTTPS"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#3-enable-https-for-aws-ec2-application","text":"In this guide, you will learn how to secure your application hosted on AWS EC2 by enabling HTTPS with free SSL certificates from Let's Encrypt using Certbot and configuring Nginx .","title":"3. Enable HTTPS for AWS EC2 Application"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#prerequisites","text":"You should already have your domain configured and your application running over HTTP (as described in the previous guide). Nginx installed and configured for your domain. Certbot installed (for generating SSL certificates).","title":"Prerequisites"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#1-install-certbot-and-generate-ssl-certificates","text":"First, ensure Certbot is installed. On Ubuntu, run: sudo apt update sudo apt install certbot python3-certbot-nginx After installation, use Certbot to automatically generate and configure the SSL certificates for your domain: sudo certbot --nginx Certbot will automatically detect your Nginx configuration and list the domains configured. When prompted, choose the domain number that corresponds to the domain you want to secure. Certbot will handle the certificate generation and automatically update your Nginx configuration to use SSL.","title":"1. Install Certbot and Generate SSL Certificates"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#2-verify-and-update-nginx-configuration","text":"Certbot automatically modifies your Nginx configuration to enable HTTPS. However, it's good to verify or manually adjust the configuration if needed. Open your Nginx configuration file for your domain: sudo nano /etc/nginx/sites-available/your-domain Ensure that the Nginx configuration has been updated to redirect HTTP traffic to HTTPS and use the SSL certificates. It should look something like this:Ensure that the Nginx configuration has been updated to redirect HTTP traffic to HTTPS and use the SSL certificates. It should look something like this: server { listen 80; server_name yourdomain.com www.yourdomain.com; return 301 https://$host$request_uri; } server { listen 443 ssl; server_name yourdomain.com www.yourdomain.com; ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; location / { proxy_pass http://localhost:<your-app-port>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Save and close the file.","title":"2. Verify and Update Nginx Configuration"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#3-restart-nginx","text":"After generating the certificates and updating the configuration, restart Nginx to apply the changes: sudo systemctl restart nginx","title":"3. Restart Nginx"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#4-test-the-application-over-https","text":"Now that Nginx is serving your application over HTTPS: Open your web browser and navigate to: https://yourdomain.com Ensure that your application is running and that there\u2019s a padlock icon next to your URL in the browser, indicating the SSL certificate is active. To confirm that HTTP traffic is being properly redirected to HTTPS, try accessing the site using http://yourdomain.com . It should automatically redirect to the secure https:// version.","title":"4. Test the Application Over HTTPS"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#set-up-automatic-certificate-renewal","text":"Let\u2019s Encrypt certificates expire every 90 days, but Certbot includes a built-in automatic renewal system. To ensure your certificates renew automatically, you can set up a cron job. Run the following command to test the automatic renewal process: sudo certbot renew --dry-run Certbot installs a cron job to automatically renew certificates, so no further action should be needed.","title":"Set Up Automatic Certificate Renewal"},{"location":"documentation/aws/03_AWS_EC2_domain_https/#summary-of-commands","text":"Install Certbot: sudo apt install certbot python3-certbot-nginx Generate and configure SSL certificates: sudo certbot --nginx (Optional) Edit Nginx config: sudo nano /etc/nginx/sites-available/your-domain Restart Nginx: sudo systemctl restart nginx Test HTTPS: https://yourdomain.com","title":"Summary of Commands"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/","text":"4. Enable HTTPS for AWS EC2 Application with Streamlit This guide explains how to set up HTTPS for a Streamlit application running in Docker using SSL certificates. 1. Create the Streamlit Configuration Folder First, create the configuration directory for Streamlit, where you will save the config.toml file. mkdir -p ~/.streamlit/ nano ~/.streamlit/config.toml 2. Configure the config.toml File Edit the config.toml file to include HTTPS settings, specifying the paths to your SSL certificates. Add the following content to the file: [server] enableCORS = false enableXsrfProtection = false port = 8502 sslCertFile = \"/app/fullchain.pem\" sslKeyFile = \"/app/privkey.pem\" 3. Obtain SSL Certificates If you don\u2019t have SSL certificates yet, obtain them using Certbot or another certification service. This example assumes the certificates are located at /etc/letsencrypt/live/covid.amaralapps.com/ . 4. Run the Docker Container With the certificates in place, start the Docker container for Streamlit, mounting the necessary files (certificates and config.toml): docker run -d --name covid-dashboard \\ -p 8502:8502 \\ -v /etc/letsencrypt/live/covid.amaralapps.com/fullchain.pem:/app/fullchain.pem \\ -v /etc/letsencrypt/live/covid.amaralapps.com/privkey.pem:/app/privkey.pem \\ -v ~/.streamlit/config.toml:/root/.streamlit/config.toml \\ miguelzeph/covid-dashboard:latest","title":"04 - EC2 App - (STREAMLIT) Custom Domain HTTPS"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/#4-enable-https-for-aws-ec2-application-with-streamlit","text":"This guide explains how to set up HTTPS for a Streamlit application running in Docker using SSL certificates.","title":"4. Enable HTTPS for AWS EC2 Application with Streamlit"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/#1-create-the-streamlit-configuration-folder","text":"First, create the configuration directory for Streamlit, where you will save the config.toml file. mkdir -p ~/.streamlit/ nano ~/.streamlit/config.toml","title":"1. Create the Streamlit Configuration Folder"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/#2-configure-the-configtoml-file","text":"Edit the config.toml file to include HTTPS settings, specifying the paths to your SSL certificates. Add the following content to the file: [server] enableCORS = false enableXsrfProtection = false port = 8502 sslCertFile = \"/app/fullchain.pem\" sslKeyFile = \"/app/privkey.pem\"","title":"2. Configure the config.toml File"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/#3-obtain-ssl-certificates","text":"If you don\u2019t have SSL certificates yet, obtain them using Certbot or another certification service. This example assumes the certificates are located at /etc/letsencrypt/live/covid.amaralapps.com/ .","title":"3. Obtain SSL Certificates"},{"location":"documentation/aws/04_AWS_EC2_domain_https_streamlit/#4-run-the-docker-container","text":"With the certificates in place, start the Docker container for Streamlit, mounting the necessary files (certificates and config.toml): docker run -d --name covid-dashboard \\ -p 8502:8502 \\ -v /etc/letsencrypt/live/covid.amaralapps.com/fullchain.pem:/app/fullchain.pem \\ -v /etc/letsencrypt/live/covid.amaralapps.com/privkey.pem:/app/privkey.pem \\ -v ~/.streamlit/config.toml:/root/.streamlit/config.toml \\ miguelzeph/covid-dashboard:latest","title":"4. Run the Docker Container"},{"location":"documentation/aws/05_AWS_workmail_SES/","text":"5. Setting Up AWS WorkMail and SES for Email Sending This guide will walk you through the steps to set up an AWS WorkMail email account for your custom domain (e.g., amaralapps.com ) and configure AWS SES (Simple Email Service) to send emails through Python using the boto3 library. Prerequisites Access to the AWS Management Console An existing custom domain (e.g., amaralapps.com ) Step 1: Setting Up AWS WorkMail with Your Domain Log in to the AWS Management Console and go to WorkMail . Choose Get Started or Create Organization if it\u2019s your first time. Add your custom domain : Go to Domains in the WorkMail console. Click Add Domain and enter your custom domain (e.g., amaral.com ). Follow the instructions to verify your domain through DNS by adding the provided TXT record in your domain registrar\u2019s DNS settings. Create an Email Account : Navigate to the Organization tab and select Create User . Enter the email address you want to create (e.g., contact@amaralapps.com ). Accessing the workmail: Visite the page: https://YOUR-WORKMAIL-DOMAIN-NAME.awsapps.com / mail (P.S. Never forget the /mail ) Step 2: Setting Up IAM for AWS SES Access To use AWS SES with Python's boto3 , you need to create an IAM user with SES permissions: Go to the IAM Console in AWS and choose Users . Click Add user , name your user (e.g., ses-user ), and select Programmatic access . Attach the AmazonSESFullAccess policy to grant permissions. Finish the setup and save the Access Key ID and Secret Access Key . You will need these keys to authenticate from Python. P.S. Save the Secret Access Key because it will appear just once Step 3: Configuring AWS SES 1. Register Your Email in AWS SES: Go to the SES Console . Choose Email Addresses and then Verify a New Email Address . Enter the email you created in WorkMail (e.g., contact@amaralapps.com ) and verify it. After verifying, you may now use this email to send and receive emails. 2. Generate AWS SES Credentials: Your Access Key ID and Secret Access Key from the IAM setup will be used in the code to authenticate SES requests. Step 4: Sending an Email with Python's boto3 Library The following example code shows how to send an email using AWS SES with boto3 : import boto3 from botocore.exceptions import ClientError from config import config # Configuring AWS SES AWS_REGION = config.get(\"email.aws_region\") SENDER_EMAIL = config.get(\"email.sender_email\") RECIPIENT_EMAIL = config.get(\"email.recipient_email\") AWS_ACCESS_KEY_ID = config.get(\"email.aws_access_key_id\") AWS_SECRET_ACCESS_KEY = config.get(\"email.aws_secret_access_key\") def send_email_to_recipient(name, email, subject, message): ses_client = boto3.client( 'ses', region_name=AWS_REGION, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, ) body_text = f\"Name: {name}\\nEmail: {email}\\n\\nMessage:\\n{message}\" body_html = f\"\"\"<html> <head></head> <body> <h1>Contacting</h1> <p><strong>Name:</strong> {name}</p> <p><strong>Email:</strong> {email}</p> <p><strong>Message:</strong></p> <p>{message}</p> </body> </html> \"\"\" try: response = ses_client.send_email( Destination={'ToAddresses': [RECIPIENT_EMAIL]}, Message={ 'Body': { 'Html': {'Charset': 'UTF-8', 'Data': body_html}, 'Text': {'Charset': 'UTF-8', 'Data': body_text}, }, 'Subject': {'Charset': 'UTF-8', 'Data': subject}, }, Source=SENDER_EMAIL, ) return response['MessageId'] except ClientError as e: return str(e.response['Error']['Message']) Explanation of the Code Configuration : The config module holds AWS credentials and configuration settings like AWS_REGION , SENDER_EMAIL , etc. SES Client : Created using boto3.client with SES-specific parameters and AWS credentials. Email Content : Configured as plain text and HTML. Customize body_text and body_html as needed. Send Email : Using send_email from the SES client, which includes Destination , Message , and Source parameters. By following these steps, you should be able to create a WorkMail email, register it with AWS SES, and send emails programmatically using Python.","title":"05 - Workmail & SES"},{"location":"documentation/aws/05_AWS_workmail_SES/#5-setting-up-aws-workmail-and-ses-for-email-sending","text":"This guide will walk you through the steps to set up an AWS WorkMail email account for your custom domain (e.g., amaralapps.com ) and configure AWS SES (Simple Email Service) to send emails through Python using the boto3 library.","title":"5. Setting Up AWS WorkMail and SES for Email Sending"},{"location":"documentation/aws/05_AWS_workmail_SES/#prerequisites","text":"Access to the AWS Management Console An existing custom domain (e.g., amaralapps.com )","title":"Prerequisites"},{"location":"documentation/aws/05_AWS_workmail_SES/#step-1-setting-up-aws-workmail-with-your-domain","text":"Log in to the AWS Management Console and go to WorkMail . Choose Get Started or Create Organization if it\u2019s your first time. Add your custom domain : Go to Domains in the WorkMail console. Click Add Domain and enter your custom domain (e.g., amaral.com ). Follow the instructions to verify your domain through DNS by adding the provided TXT record in your domain registrar\u2019s DNS settings. Create an Email Account : Navigate to the Organization tab and select Create User . Enter the email address you want to create (e.g., contact@amaralapps.com ). Accessing the workmail: Visite the page: https://YOUR-WORKMAIL-DOMAIN-NAME.awsapps.com / mail (P.S. Never forget the /mail )","title":"Step 1: Setting Up AWS WorkMail with Your Domain"},{"location":"documentation/aws/05_AWS_workmail_SES/#step-2-setting-up-iam-for-aws-ses-access","text":"To use AWS SES with Python's boto3 , you need to create an IAM user with SES permissions: Go to the IAM Console in AWS and choose Users . Click Add user , name your user (e.g., ses-user ), and select Programmatic access . Attach the AmazonSESFullAccess policy to grant permissions. Finish the setup and save the Access Key ID and Secret Access Key . You will need these keys to authenticate from Python. P.S. Save the Secret Access Key because it will appear just once","title":"Step 2: Setting Up IAM for AWS SES Access"},{"location":"documentation/aws/05_AWS_workmail_SES/#step-3-configuring-aws-ses","text":"","title":"Step 3: Configuring AWS SES"},{"location":"documentation/aws/05_AWS_workmail_SES/#1-register-your-email-in-aws-ses","text":"Go to the SES Console . Choose Email Addresses and then Verify a New Email Address . Enter the email you created in WorkMail (e.g., contact@amaralapps.com ) and verify it. After verifying, you may now use this email to send and receive emails.","title":"1. Register Your Email in AWS SES:"},{"location":"documentation/aws/05_AWS_workmail_SES/#2-generate-aws-ses-credentials","text":"Your Access Key ID and Secret Access Key from the IAM setup will be used in the code to authenticate SES requests.","title":"2. Generate AWS SES Credentials:"},{"location":"documentation/aws/05_AWS_workmail_SES/#step-4-sending-an-email-with-pythons-boto3-library","text":"The following example code shows how to send an email using AWS SES with boto3 : import boto3 from botocore.exceptions import ClientError from config import config # Configuring AWS SES AWS_REGION = config.get(\"email.aws_region\") SENDER_EMAIL = config.get(\"email.sender_email\") RECIPIENT_EMAIL = config.get(\"email.recipient_email\") AWS_ACCESS_KEY_ID = config.get(\"email.aws_access_key_id\") AWS_SECRET_ACCESS_KEY = config.get(\"email.aws_secret_access_key\") def send_email_to_recipient(name, email, subject, message): ses_client = boto3.client( 'ses', region_name=AWS_REGION, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, ) body_text = f\"Name: {name}\\nEmail: {email}\\n\\nMessage:\\n{message}\" body_html = f\"\"\"<html> <head></head> <body> <h1>Contacting</h1> <p><strong>Name:</strong> {name}</p> <p><strong>Email:</strong> {email}</p> <p><strong>Message:</strong></p> <p>{message}</p> </body> </html> \"\"\" try: response = ses_client.send_email( Destination={'ToAddresses': [RECIPIENT_EMAIL]}, Message={ 'Body': { 'Html': {'Charset': 'UTF-8', 'Data': body_html}, 'Text': {'Charset': 'UTF-8', 'Data': body_text}, }, 'Subject': {'Charset': 'UTF-8', 'Data': subject}, }, Source=SENDER_EMAIL, ) return response['MessageId'] except ClientError as e: return str(e.response['Error']['Message'])","title":"Step 4: Sending an Email with Python's boto3 Library"},{"location":"documentation/aws/05_AWS_workmail_SES/#explanation-of-the-code","text":"Configuration : The config module holds AWS credentials and configuration settings like AWS_REGION , SENDER_EMAIL , etc. SES Client : Created using boto3.client with SES-specific parameters and AWS credentials. Email Content : Configured as plain text and HTML. Customize body_text and body_html as needed. Send Email : Using send_email from the SES client, which includes Destination , Message , and Source parameters. By following these steps, you should be able to create a WorkMail email, register it with AWS SES, and send emails programmatically using Python.","title":"Explanation of the Code"},{"location":"documentation/docker/01_docker/","text":"Docker Introduction To get started with Docker, it's essential to understand three basic concepts: Container - Containers are lightweight, isolated virtual machines . They include everything your application needs to run (such as code , libraries , and dependencies ) but share the operating system kernel , making them lightweight and fast. Images - An image is a \"blueprint\" used to create containers. It's a kind of \"snapshot\" of the system with everything needed to run an application. Images are immutable and can be versioned . Dockerfile - The Dockerfile is where you define how the image will be built. In this file, you specify the base system , dependencies, and configurations required for the application. Commands docker ps This command displays a list of containers that are currently running. docker ps # shows the running containers docker ps -a # shows all containers, including those that are stopped docker images The docker images command lists all Docker images stored locally on your system. docker images docker run What the docker run command does: - Uses a specified image - Creates a container from the image - Runs the container docker run [options] <image> docker run -d <image> # runs the container in detached mode (background) docker run -p <local_port>:<container_port> <image> docker run -d -p <local_port>:<container_port> <image> # Very useful for running on an AWS instance docker login This command is used to log in to a Docker registry (such as Docker Hub). docker login docker push This command is used to push an image from your local system to a Docker registry. docker push <repository_name>/<image_name>:<tag> docker pull This command is used to pull an image from a Docker registry to your local system. docker pull <repository_name>/<image_name>:<tag>","title":"01 - Docker Base"},{"location":"documentation/docker/01_docker/#docker-introduction","text":"To get started with Docker, it's essential to understand three basic concepts: Container - Containers are lightweight, isolated virtual machines . They include everything your application needs to run (such as code , libraries , and dependencies ) but share the operating system kernel , making them lightweight and fast. Images - An image is a \"blueprint\" used to create containers. It's a kind of \"snapshot\" of the system with everything needed to run an application. Images are immutable and can be versioned . Dockerfile - The Dockerfile is where you define how the image will be built. In this file, you specify the base system , dependencies, and configurations required for the application.","title":"Docker Introduction"},{"location":"documentation/docker/01_docker/#commands","text":"","title":"Commands"},{"location":"documentation/docker/01_docker/#docker-ps","text":"This command displays a list of containers that are currently running. docker ps # shows the running containers docker ps -a # shows all containers, including those that are stopped","title":"docker ps"},{"location":"documentation/docker/01_docker/#docker-images","text":"The docker images command lists all Docker images stored locally on your system. docker images","title":"docker images"},{"location":"documentation/docker/01_docker/#docker-run","text":"What the docker run command does: - Uses a specified image - Creates a container from the image - Runs the container docker run [options] <image> docker run -d <image> # runs the container in detached mode (background) docker run -p <local_port>:<container_port> <image> docker run -d -p <local_port>:<container_port> <image> # Very useful for running on an AWS instance","title":"docker run"},{"location":"documentation/docker/01_docker/#docker-login","text":"This command is used to log in to a Docker registry (such as Docker Hub). docker login","title":"docker login"},{"location":"documentation/docker/01_docker/#docker-push","text":"This command is used to push an image from your local system to a Docker registry. docker push <repository_name>/<image_name>:<tag>","title":"docker push"},{"location":"documentation/docker/01_docker/#docker-pull","text":"This command is used to pull an image from a Docker registry to your local system. docker pull <repository_name>/<image_name>:<tag>","title":"docker pull"},{"location":"documentation/docker/02_dockerfile/","text":"Dockerfile A Dockerfile is a text file with instructions that Docker uses to build an image . Each line in the Dockerfile represents an instruction that adds, modifies, or configures the image. Dockerfile Guide Summary Step Command/Instruction Purpose Example 1 FROM Defines the base image for your application, such as the OS or language environment FROM python:latest 2 LABEL Adds metadata like maintainer info and descriptions LABEL maintainer=\"myemail@example.com\" description=\"Image for my Python application\" 3 WORKDIR Sets the working directory inside the container WORKDIR /app 4 COPY / ADD Transfers files from the local system to the container COPY . /app 5 RUN Runs commands to install dependencies or configure the environment RUN pip install --upgrade pip && pip install -r requirements.txt 6 ENV Sets environment variables within the container ENV APP_ENV=production 7 EXPOSE Opens specific ports for external access EXPOSE 5000 8 CMD / ENTRYPOINT Defines the startup command that runs when the container starts CMD [\"python\", \"app.py\"] 9 .dockerignore Excludes files/folders from the container, optimizing size and efficiency .dockerignore (add __pycache__, .venv, etc.) 2. Choose the Base Image ( FROM ) Every Docker image starts with a base image, defined by the FROM command. The base image is the operating system or environment on which your application will be built. How to Choose: Choose a base image that has the minimum system required for your application. Use official Docker Hub images (e.g., ubuntu , node , python ) for security and reliability. # Example: using the latest Python version as a base FROM python:latest 3. Add Author and Description Information ( LABEL ) For documentation and organization, use the LABEL instruction to add author information or purposes for the image. LABEL maintainer=\"myemail@example.com\" LABEL description=\"Image for my Python application\" This helps with image identification and future maintenance. 4. Set the Working Directory ( WORKDIR ) Defining the working directory with WORKDIR helps organize where files and commands will be executed. All subsequent commands will use this directory as their base. WORKDIR /app If you need to move to an directory, you also can use the command WORKDIR WORKDIR /app/src 5. Copy Necessary Files ( COPY and ADD ) Use COPY or ADD to transfer files from the local system to the container. COPY : simple and ideal for copying local files. ADD : allows copying local files and downloading from URLs. Example: COPY . /app This command copies all files from the current directory to the /app directory in the container. 6. Install Dependencies and Environment Configurations ( RUN and ENV ) Use RUN to install dependencies and configure the environment. With ENV , set environment variables. RUN pip install --upgrade pip RUN pip install -r requirements.txt ENV APP_ENV=production These commands are essential to prepare the application's environment. Here, it upgrades pip and installs all dependencies listed in requirements.txt . 7. Expose Ports for External Access ( EXPOSE ) To let Docker know which ports should be open to the external world, use EXPOSE . EXPOSE 5000 This sets up port 5000 to be accessible, which is commonly used for Flask applications, for example. 8. Configure the Startup Command ( CMD or ENTRYPOINT ) The startup command defines what the container will run when it starts. CMD and ENTRYPOINT are used here. CMD : default command but can be overridden. ENTRYPOINT : sets a fixed command that generally cannot be overridden. Example with CMD : CMD [\"python\", \"app.py\"] This command starts the Python application by running app.py . 9. Organize and Optimize the Dockerfile Minimize the number of layers : Combine RUN instructions to reduce layers and image size. Use .dockerignore files to ignore files that do not need to be in the container (like .venv or __pycache__ folders in Python projects). # Combining RUN instructions RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/* Complete Dockerfile Example Here's a complete Dockerfile example for a Python application: # Choosing the base image FROM python:3.9 # Adding maintenance and description labels LABEL maintainer=\"myemail@example.com\" LABEL description=\"Image for my Python application\" # Setting up the working directory WORKDIR /app # Copying application files COPY . /app # Installing dependencies RUN pip install --upgrade pip RUN pip install -r requirements.txt # Setting environment variables ENV APP_ENV=production # Exposing the port EXPOSE 5000 # Setting the startup command CMD [\"python\", \"app.py\"] Conclusion This guide provides a logical and organized approach to understanding and creating a Dockerfile. Practice is essential, so try creating Dockerfiles for different applications, testing instructions, and adjusting as necessary.","title":"02 - DockerFile Guide"},{"location":"documentation/docker/02_dockerfile/#dockerfile","text":"A Dockerfile is a text file with instructions that Docker uses to build an image . Each line in the Dockerfile represents an instruction that adds, modifies, or configures the image.","title":"Dockerfile"},{"location":"documentation/docker/02_dockerfile/#dockerfile-guide-summary","text":"Step Command/Instruction Purpose Example 1 FROM Defines the base image for your application, such as the OS or language environment FROM python:latest 2 LABEL Adds metadata like maintainer info and descriptions LABEL maintainer=\"myemail@example.com\" description=\"Image for my Python application\" 3 WORKDIR Sets the working directory inside the container WORKDIR /app 4 COPY / ADD Transfers files from the local system to the container COPY . /app 5 RUN Runs commands to install dependencies or configure the environment RUN pip install --upgrade pip && pip install -r requirements.txt 6 ENV Sets environment variables within the container ENV APP_ENV=production 7 EXPOSE Opens specific ports for external access EXPOSE 5000 8 CMD / ENTRYPOINT Defines the startup command that runs when the container starts CMD [\"python\", \"app.py\"] 9 .dockerignore Excludes files/folders from the container, optimizing size and efficiency .dockerignore (add __pycache__, .venv, etc.)","title":"Dockerfile Guide Summary"},{"location":"documentation/docker/02_dockerfile/#2-choose-the-base-image-from","text":"Every Docker image starts with a base image, defined by the FROM command. The base image is the operating system or environment on which your application will be built.","title":"2. Choose the Base Image (FROM)"},{"location":"documentation/docker/02_dockerfile/#how-to-choose","text":"Choose a base image that has the minimum system required for your application. Use official Docker Hub images (e.g., ubuntu , node , python ) for security and reliability. # Example: using the latest Python version as a base FROM python:latest","title":"How to Choose:"},{"location":"documentation/docker/02_dockerfile/#3-add-author-and-description-information-label","text":"For documentation and organization, use the LABEL instruction to add author information or purposes for the image. LABEL maintainer=\"myemail@example.com\" LABEL description=\"Image for my Python application\" This helps with image identification and future maintenance.","title":"3. Add Author and Description Information (LABEL)"},{"location":"documentation/docker/02_dockerfile/#4-set-the-working-directory-workdir","text":"Defining the working directory with WORKDIR helps organize where files and commands will be executed. All subsequent commands will use this directory as their base. WORKDIR /app If you need to move to an directory, you also can use the command WORKDIR WORKDIR /app/src","title":"4. Set the Working Directory (WORKDIR)"},{"location":"documentation/docker/02_dockerfile/#5-copy-necessary-files-copy-and-add","text":"Use COPY or ADD to transfer files from the local system to the container. COPY : simple and ideal for copying local files. ADD : allows copying local files and downloading from URLs.","title":"5. Copy Necessary Files (COPY and ADD)"},{"location":"documentation/docker/02_dockerfile/#example","text":"COPY . /app This command copies all files from the current directory to the /app directory in the container.","title":"Example:"},{"location":"documentation/docker/02_dockerfile/#6-install-dependencies-and-environment-configurations-run-and-env","text":"Use RUN to install dependencies and configure the environment. With ENV , set environment variables. RUN pip install --upgrade pip RUN pip install -r requirements.txt ENV APP_ENV=production These commands are essential to prepare the application's environment. Here, it upgrades pip and installs all dependencies listed in requirements.txt .","title":"6. Install Dependencies and Environment Configurations (RUN and ENV)"},{"location":"documentation/docker/02_dockerfile/#7-expose-ports-for-external-access-expose","text":"To let Docker know which ports should be open to the external world, use EXPOSE . EXPOSE 5000 This sets up port 5000 to be accessible, which is commonly used for Flask applications, for example.","title":"7. Expose Ports for External Access (EXPOSE)"},{"location":"documentation/docker/02_dockerfile/#8-configure-the-startup-command-cmd-or-entrypoint","text":"The startup command defines what the container will run when it starts. CMD and ENTRYPOINT are used here. CMD : default command but can be overridden. ENTRYPOINT : sets a fixed command that generally cannot be overridden.","title":"8. Configure the Startup Command (CMD or ENTRYPOINT)"},{"location":"documentation/docker/02_dockerfile/#example-with-cmd","text":"CMD [\"python\", \"app.py\"] This command starts the Python application by running app.py .","title":"Example with CMD:"},{"location":"documentation/docker/02_dockerfile/#9-organize-and-optimize-the-dockerfile","text":"Minimize the number of layers : Combine RUN instructions to reduce layers and image size. Use .dockerignore files to ignore files that do not need to be in the container (like .venv or __pycache__ folders in Python projects). # Combining RUN instructions RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*","title":"9. Organize and Optimize the Dockerfile"},{"location":"documentation/docker/02_dockerfile/#complete-dockerfile-example","text":"Here's a complete Dockerfile example for a Python application: # Choosing the base image FROM python:3.9 # Adding maintenance and description labels LABEL maintainer=\"myemail@example.com\" LABEL description=\"Image for my Python application\" # Setting up the working directory WORKDIR /app # Copying application files COPY . /app # Installing dependencies RUN pip install --upgrade pip RUN pip install -r requirements.txt # Setting environment variables ENV APP_ENV=production # Exposing the port EXPOSE 5000 # Setting the startup command CMD [\"python\", \"app.py\"]","title":"Complete Dockerfile Example"},{"location":"documentation/docker/02_dockerfile/#conclusion","text":"This guide provides a logical and organized approach to understanding and creating a Dockerfile. Practice is essential, so try creating Dockerfiles for different applications, testing instructions, and adjusting as necessary.","title":"Conclusion"},{"location":"documentation/docker/03_docker_restart_env_completely/","text":"Restarting Docker Environment Completely Stop everything and remove containers and networks: docker-compose down Stop everything and remove containers, networks, and volumes: docker-compose down -v Stop everything, remove containers, networks, and images: docker-compose down --rmi all Rebuild and start everything from scratch: docker-compose up --build With these steps, you should be able to \"restart\" your Docker environment completely, starting from scratch.","title":"03 - Restarting Env Completely"},{"location":"documentation/docker/03_docker_restart_env_completely/#restarting-docker-environment-completely","text":"Stop everything and remove containers and networks: docker-compose down Stop everything and remove containers, networks, and volumes: docker-compose down -v Stop everything, remove containers, networks, and images: docker-compose down --rmi all Rebuild and start everything from scratch: docker-compose up --build With these steps, you should be able to \"restart\" your Docker environment completely, starting from scratch.","title":"Restarting Docker Environment Completely"},{"location":"documentation/kafka/01_kafka_introduction/","text":"Kafka: Minimum Necessary to Run This guide provides a step-by-step approach to setting up and operating Kafka , including an overview of its components and practical instructions for managing topics, producing, and consuming messages. Table of Contents Overview What is Kafka? What is the difference between Kafka and RabitMQ? Setting Up Kafka 1. Start Zookeeper Server 2. Start Kafka Server 3. Execute Docker Compose Basic Kafka Operations 4. Access Kafka Container 5. Create a Topic 6. Produce Messages 7. Consume Messages Graphical Interface Managing Persistent Data Remove Old Volumes Create a Custom Consumer 1. Install Required Library 2. Develop the Consumer 3. Add Consumer to Docker Compose Testing and Handling Failures Stopping a Broker Restarting the Broker Commands Summary Overview What is Kafka? Kafka is a Distributed Data Streaming Plataform where have loads of different services, so it also can be classified in sub-groups as: Message Broker ETL Log System Temporary Event Storage (data rentention) Let's have a look the Kafka architecture What is the difference between Kafka and RabitMQ? Feature Apache Kafka RabbitMQ Architecture Kafka uses a partitioned log model, combining message queue and publish-subscribe approaches. RabbitMQ uses a message queue model. Scalability Kafka scales by allowing partitions to be distributed across different servers. Scale by increasing the number of consumers in the queue to distribute processing among concurrent consumers. Message Retention Based on policies, for example, messages can be stored for one day. The retention window can be configured by the user. Based on acknowledgment, meaning messages are deleted as they are consumed. Multiple Consumers Multiple consumers can subscribe to the same topic, as Kafka allows the same message to be replayed for a certain period. Not possible for multiple consumers to receive the same message, as messages are removed once consumed. Replication Topics are replicated automatically, but users can manually configure topics to not be replicated. Messages are not replicated automatically, but users can manually configure replication. Message Ordering Each consumer receives messages in order due to the partitioned log architecture. Messages are delivered to consumers in the order they arrive in the queue. If there are concurrent consumers, each consumer will process a subset of those messages. Protocols Kafka uses a binary protocol over TCP. Advanced Message Queuing Protocol (AMQP) with support via plugins: MQTT, STOMP. CLUSTER (Scalability): Manages a group of BROKERS in Kafka. Can use Zookeeper or Kafka's built-in capabilities for cluster management. BROKER (Storage/Message Management): Recive messages from PRODUCER A Kafka server that stores messages and manages TOPICS . Distributes messages to CONSUMERS . TOPIC (Organization): A named channel for organizing and storing messages. CONSUMER (Transforme/Processing): Reads messages from TOPICS and processes/transform them, possibly forwarding data to other systems. PRODUCER (Create messages/Sending): Sends messages to TOPICS . P.S: Zookeeper: Assigns an Active Control Broker (AC) to manage tasks like topic creation, partition management, and leader redistribution. Automatically reassigns roles in case of broker failures. Offset: A unique identifier for each message within a topic. In summary: The PRODUCER sends messages to a TOPIC , the BROKER manages the topics and retains the messages for a period of time, the CLUSTER manages the brokers , and the CONSUMER pulls the data from the TOPIC , then performs transformations and/or sends the data to another system or database . Setting Up Kafka 1. Start Zookeeper Server Use the official Docker image for Zookeeper: Zookeeper Docker Hub Image name: zookeeper 2. Start Kafka Server Use the recommended Kafka Docker image: apache/kafka 3. Execute docker-compose Start the services using: bash sudo docker-compose up --build --build : Ensures images are rebuilt. Example of docker-compose.yml file version: '3' services: zookeeper: image: wurstmeister/zookeeper container_name: zookeeper ports: - \"2181:2181\" #----------------------------------------- BROKERS -------------------------------- # Broker 1 kafka: image: wurstmeister/kafka container_name: kafka ports: - \"9092:9092\" environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_LOG_RETENTION_HOURS: 168 # Broker 2 kafka-broker-2: image: wurstmeister/kafka container_name: kafka-broker-2 ports: - \"9093:9092\" environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_LOG_RETENTION_HOURS: 168 #----------------------------------------- CONSUMERS -------------------------------- # Consumer 1 consumer-process-articles: build: context: . dockerfile: Dockerfile_python container_name: consumer-articles command: python consumer_articles.py depends_on: - kafka - kafka-broker-2 # Consumer 2 consumer-send-message-to-topic: build: context: . dockerfile: Dockerfile_python container_name: consumer-send-message-to-topic command: python consumer_send_message_to_topic.py depends_on: - kafka - kafka-broker-2 #------------------------------------- GRAPHICAL INTERFACE --------------------------- kafka-ui: image: provectuslabs/kafka-ui container_name: kafka-ui ports: - \"8080:8080\" depends_on: - kafka - kafka-broker-2 - zookeeper environment: KAFKA_CLUSTERS_0_NAME: local KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092,kafka-broker-2:9093 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 DOCKERFILE_PYTHON (Consumer), example: FROM python:3.9-slim # Install necessary dependencies RUN pip install confluent-kafka # Copy the scripts to the container in the correct folder COPY ./consumers /app/consumers # Working directory WORKDIR /app/consumers Basic Kafka Operations 4. Access Kafka Container Enter the Kafka container: sudo docker exec -it kafka /bin/sh Navigate to Kafka scripts directory: cd /opt/kafka_2.13-2.8.1/bin/ Use find to locate scripts if needed: find / -name \"kafka-topics.sh\" 5. Create a Topic Command: kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic helloworld Explanation of Parameters: - --bootstrap-server : Kafka server address. - --replication-factor : Number of replicas (1 for single broker setup). - --partitions : Number of partitions for the topic. - --topic : Name of the topic. 6. Produce Messages Send Simple Messages Command: kafka-console-producer.sh --topic helloworld --bootstrap-server localhost:9092 Type messages and press ENTER to send. Send Key-Value Messages Command: kafka-console-producer.sh --topic helloworld --bootstrap-server localhost:9092 --property parse.key=true --property key.separator=: Example format: key1:{\"title\":\"Message Title\",\"content\":\"Message Content\"} 7. Consume Messages Command: kafka-console-consumer.sh --topic helloworld --bootstrap-server localhost:9092 --from-beginning Explanation: - --from-beginning : Reads all messages from the topic's start. Graphical Interface Access Kafka's UI in the browser: Navigate to localhost:8080 . Managing Persistent Data Remove Old Volumes Clear existing volumes to prevent data conflicts: bash docker-compose down --volumes Create a Custom Consumer 1. Install Required Library Use the confluent-kafka library: bash pip install confluent-kafka 2. Develop the Consumer Write a Python script to act as a custom consumer. 3. Add Consumer to Docker Compose Integrate the consumer into the docker-compose.yml file and start the service. Testing and Handling Failures Stopping a Broker Simulate a broker failure: bash sudo docker-compose stop kafka-broker-2 Observe message behavior. Restarting the Broker Restart the stopped broker: bash sudo docker-compose up kafka-broker-2 Commands Summary Command Description kafka-topics.sh --create Create a new topic. kafka-console-producer.sh Send messages to a topic. kafka-console-consumer.sh Consume messages from a topic. docker-compose up --build Start services with Docker Compose. docker-compose down --volumes Remove services and associated volumes. sudo docker exec -it kafka Access Kafka container shell.","title":"01 - Introduction"},{"location":"documentation/kafka/01_kafka_introduction/#kafka-minimum-necessary-to-run","text":"This guide provides a step-by-step approach to setting up and operating Kafka , including an overview of its components and practical instructions for managing topics, producing, and consuming messages.","title":"Kafka: Minimum Necessary to Run"},{"location":"documentation/kafka/01_kafka_introduction/#table-of-contents","text":"Overview What is Kafka? What is the difference between Kafka and RabitMQ? Setting Up Kafka 1. Start Zookeeper Server 2. Start Kafka Server 3. Execute Docker Compose Basic Kafka Operations 4. Access Kafka Container 5. Create a Topic 6. Produce Messages 7. Consume Messages Graphical Interface Managing Persistent Data Remove Old Volumes Create a Custom Consumer 1. Install Required Library 2. Develop the Consumer 3. Add Consumer to Docker Compose Testing and Handling Failures Stopping a Broker Restarting the Broker Commands Summary","title":"Table of Contents"},{"location":"documentation/kafka/01_kafka_introduction/#overview","text":"","title":"Overview"},{"location":"documentation/kafka/01_kafka_introduction/#what-is-kafka","text":"Kafka is a Distributed Data Streaming Plataform where have loads of different services, so it also can be classified in sub-groups as: Message Broker ETL Log System Temporary Event Storage (data rentention) Let's have a look the Kafka architecture","title":"What is Kafka?"},{"location":"documentation/kafka/01_kafka_introduction/#what-is-the-difference-between-kafka-and-rabitmq","text":"Feature Apache Kafka RabbitMQ Architecture Kafka uses a partitioned log model, combining message queue and publish-subscribe approaches. RabbitMQ uses a message queue model. Scalability Kafka scales by allowing partitions to be distributed across different servers. Scale by increasing the number of consumers in the queue to distribute processing among concurrent consumers. Message Retention Based on policies, for example, messages can be stored for one day. The retention window can be configured by the user. Based on acknowledgment, meaning messages are deleted as they are consumed. Multiple Consumers Multiple consumers can subscribe to the same topic, as Kafka allows the same message to be replayed for a certain period. Not possible for multiple consumers to receive the same message, as messages are removed once consumed. Replication Topics are replicated automatically, but users can manually configure topics to not be replicated. Messages are not replicated automatically, but users can manually configure replication. Message Ordering Each consumer receives messages in order due to the partitioned log architecture. Messages are delivered to consumers in the order they arrive in the queue. If there are concurrent consumers, each consumer will process a subset of those messages. Protocols Kafka uses a binary protocol over TCP. Advanced Message Queuing Protocol (AMQP) with support via plugins: MQTT, STOMP. CLUSTER (Scalability): Manages a group of BROKERS in Kafka. Can use Zookeeper or Kafka's built-in capabilities for cluster management. BROKER (Storage/Message Management): Recive messages from PRODUCER A Kafka server that stores messages and manages TOPICS . Distributes messages to CONSUMERS . TOPIC (Organization): A named channel for organizing and storing messages. CONSUMER (Transforme/Processing): Reads messages from TOPICS and processes/transform them, possibly forwarding data to other systems. PRODUCER (Create messages/Sending): Sends messages to TOPICS . P.S: Zookeeper: Assigns an Active Control Broker (AC) to manage tasks like topic creation, partition management, and leader redistribution. Automatically reassigns roles in case of broker failures. Offset: A unique identifier for each message within a topic. In summary: The PRODUCER sends messages to a TOPIC , the BROKER manages the topics and retains the messages for a period of time, the CLUSTER manages the brokers , and the CONSUMER pulls the data from the TOPIC , then performs transformations and/or sends the data to another system or database .","title":"What is the difference between Kafka and RabitMQ?"},{"location":"documentation/kafka/01_kafka_introduction/#setting-up-kafka","text":"","title":"Setting Up Kafka"},{"location":"documentation/kafka/01_kafka_introduction/#1-start-zookeeper-server","text":"Use the official Docker image for Zookeeper: Zookeeper Docker Hub Image name: zookeeper","title":"1. Start Zookeeper Server"},{"location":"documentation/kafka/01_kafka_introduction/#2-start-kafka-server","text":"Use the recommended Kafka Docker image: apache/kafka","title":"2. Start Kafka Server"},{"location":"documentation/kafka/01_kafka_introduction/#3-execute-docker-compose","text":"Start the services using: bash sudo docker-compose up --build --build : Ensures images are rebuilt. Example of docker-compose.yml file version: '3' services: zookeeper: image: wurstmeister/zookeeper container_name: zookeeper ports: - \"2181:2181\" #----------------------------------------- BROKERS -------------------------------- # Broker 1 kafka: image: wurstmeister/kafka container_name: kafka ports: - \"9092:9092\" environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_LOG_RETENTION_HOURS: 168 # Broker 2 kafka-broker-2: image: wurstmeister/kafka container_name: kafka-broker-2 ports: - \"9093:9092\" environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_LOG_RETENTION_HOURS: 168 #----------------------------------------- CONSUMERS -------------------------------- # Consumer 1 consumer-process-articles: build: context: . dockerfile: Dockerfile_python container_name: consumer-articles command: python consumer_articles.py depends_on: - kafka - kafka-broker-2 # Consumer 2 consumer-send-message-to-topic: build: context: . dockerfile: Dockerfile_python container_name: consumer-send-message-to-topic command: python consumer_send_message_to_topic.py depends_on: - kafka - kafka-broker-2 #------------------------------------- GRAPHICAL INTERFACE --------------------------- kafka-ui: image: provectuslabs/kafka-ui container_name: kafka-ui ports: - \"8080:8080\" depends_on: - kafka - kafka-broker-2 - zookeeper environment: KAFKA_CLUSTERS_0_NAME: local KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092,kafka-broker-2:9093 KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181 DOCKERFILE_PYTHON (Consumer), example: FROM python:3.9-slim # Install necessary dependencies RUN pip install confluent-kafka # Copy the scripts to the container in the correct folder COPY ./consumers /app/consumers # Working directory WORKDIR /app/consumers","title":"3. Execute docker-compose"},{"location":"documentation/kafka/01_kafka_introduction/#basic-kafka-operations","text":"","title":"Basic Kafka Operations"},{"location":"documentation/kafka/01_kafka_introduction/#4-access-kafka-container","text":"Enter the Kafka container: sudo docker exec -it kafka /bin/sh Navigate to Kafka scripts directory: cd /opt/kafka_2.13-2.8.1/bin/ Use find to locate scripts if needed: find / -name \"kafka-topics.sh\"","title":"4. Access Kafka Container"},{"location":"documentation/kafka/01_kafka_introduction/#5-create-a-topic","text":"Command: kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic helloworld Explanation of Parameters: - --bootstrap-server : Kafka server address. - --replication-factor : Number of replicas (1 for single broker setup). - --partitions : Number of partitions for the topic. - --topic : Name of the topic.","title":"5. Create a Topic"},{"location":"documentation/kafka/01_kafka_introduction/#6-produce-messages","text":"Send Simple Messages Command: kafka-console-producer.sh --topic helloworld --bootstrap-server localhost:9092 Type messages and press ENTER to send. Send Key-Value Messages Command: kafka-console-producer.sh --topic helloworld --bootstrap-server localhost:9092 --property parse.key=true --property key.separator=: Example format: key1:{\"title\":\"Message Title\",\"content\":\"Message Content\"}","title":"6. Produce Messages"},{"location":"documentation/kafka/01_kafka_introduction/#7-consume-messages","text":"Command: kafka-console-consumer.sh --topic helloworld --bootstrap-server localhost:9092 --from-beginning Explanation: - --from-beginning : Reads all messages from the topic's start.","title":"7. Consume Messages"},{"location":"documentation/kafka/01_kafka_introduction/#graphical-interface","text":"Access Kafka's UI in the browser: Navigate to localhost:8080 .","title":"Graphical Interface"},{"location":"documentation/kafka/01_kafka_introduction/#managing-persistent-data","text":"","title":"Managing Persistent Data"},{"location":"documentation/kafka/01_kafka_introduction/#remove-old-volumes","text":"Clear existing volumes to prevent data conflicts: bash docker-compose down --volumes","title":"Remove Old Volumes"},{"location":"documentation/kafka/01_kafka_introduction/#create-a-custom-consumer","text":"","title":"Create a Custom Consumer"},{"location":"documentation/kafka/01_kafka_introduction/#1-install-required-library","text":"Use the confluent-kafka library: bash pip install confluent-kafka","title":"1. Install Required Library"},{"location":"documentation/kafka/01_kafka_introduction/#2-develop-the-consumer","text":"Write a Python script to act as a custom consumer.","title":"2. Develop the Consumer"},{"location":"documentation/kafka/01_kafka_introduction/#3-add-consumer-to-docker-compose","text":"Integrate the consumer into the docker-compose.yml file and start the service.","title":"3. Add Consumer to Docker Compose"},{"location":"documentation/kafka/01_kafka_introduction/#testing-and-handling-failures","text":"","title":"Testing and Handling Failures"},{"location":"documentation/kafka/01_kafka_introduction/#stopping-a-broker","text":"Simulate a broker failure: bash sudo docker-compose stop kafka-broker-2 Observe message behavior.","title":"Stopping a Broker"},{"location":"documentation/kafka/01_kafka_introduction/#restarting-the-broker","text":"Restart the stopped broker: bash sudo docker-compose up kafka-broker-2","title":"Restarting the Broker"},{"location":"documentation/kafka/01_kafka_introduction/#commands-summary","text":"Command Description kafka-topics.sh --create Create a new topic. kafka-console-producer.sh Send messages to a topic. kafka-console-consumer.sh Consume messages from a topic. docker-compose up --build Start services with Docker Compose. docker-compose down --volumes Remove services and associated volumes. sudo docker exec -it kafka Access Kafka container shell.","title":"Commands Summary"},{"location":"documentation/python/01_python_good_starting/","text":"Python Good Starting When beginning your journey with Python, it\u2019s crucial to set up your development environment properly to avoid potential conflicts and ensure a smooth experience. This guide will walk you through the steps of installing Python correctly using pyenv , managing different Python versions, and setting up a virtual environment for your projects. Table of Contents Python Good Starting Why Learn Python Correctly from Scratch? Pyenv 1. To install pyenv 2. Configure Shell for pyenv 3. Restart Shell 4. Check Available Python Versions 5. Install a Python Version with pyenv 6. Check Installed Python Versions 7. Select a Python Version VirtualEnv 1. Install virtualenv 2. Create a Virtual Environment with virtualenv 3. Activate the Virtual Environment Install Dependencies Project Structure Overview Why Learn Python Correctly from Scratch? Consistency Across Environments : Setting up a consistent Python environment ensures that your code runs the same way on different systems . Avoid System Conflicts : Using tools like pyenv and virtual environments prevents conflicts with system-installed Python versions and other projects. Manage Dependencies : Virtual environments help you manage project-specific dependencies without affecting your global Python installation (useful for Linux users). Pyenv pyenv is a tool that allows you to easily install and manage multiple versions of Python. It helps you avoid conflicts with the system Python and manage different versions for different projects. 1. To install pyenv : On macOS/Linux: curl https://pyenv.run | bash 2. Configure Shell for pyenv After running the command, add the following lines to your shell startup file ( .bashrc , .zshrc , etc.): # Pyenv configuration export PATH=\"$HOME/.pyenv/bin:$PATH\" eval \"$(pyenv init --path)\" eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" This step is also important to avoid pip conflicts. Create an alias in .bashrc , .zshrc , etc., to ensure you are using pip from the correct pyenv installation: # Configure your PIP as well alias pip='python -m pip' 3. Restart Shell Restart your shell or run source ~/.bashrc (or the corresponding file) to apply the changes. 4. Check Available Python Versions: To see the available Python versions that you can install with pyenv , use: pyenv install --list 5. Install a Python Version with pyenv To install a specific version of Python, use: # Replace <version> with the desired version number (e.g., 3.9.1). pyenv install <version> 6. Check Installed Python Versions To see which Python versions are installed on your system and identify the active version: pyenv versions # Example Output: system # Python from your system 3.7.17 * 3.11.9 # (the * represents the active version) 3.12.0 7. Select a Python Version pyenv global <python-version> VirtualEnv virtualenv is a tool to create isolated Python environments, ensuring that dependencies for one project don\u2019t interfere with those of another. 1. Install virtualenv pip install virtualenv 2. Create a Virtual Environment with virtualenv Select the directory where you will start your project (typically a new folder), then run: # You don't need to specify the python version because pyenv is doing it when you # choose the global version that you want to use on the previous steps. Of course # if you want to change the python version, you have to do it on pyenv first before # execute this command. virtualenv --python=python <python-env-name> # Recommended to use \"env\" as the environment name 3. Activate the Virtual Environment source <python-env-name>/bin/activate When the environment is active, its name appears at the beginning of your terminal prompt: (<python-env-name>) user@machine:~/<project-directory>$ Install Dependencies After creating and activating the virtual environment, it\u2019s a good practice to create a requirements.txt file listing the Python libraries required for the project: # requirements.txt file requests==2.25.1 selenium==4.4.1 bs4==0.0.2 klein_config==4.0.2 Install all dependencies at once from requirements.txt : pip install -r requirements.txt Project Structure Overview Here\u2019s an example of a simple, well-organized Python project structure: my_project/ \u251c\u2500\u2500 app/ # Your application \u251c\u2500\u2500 env/ # Virtual environment directory (exclude from version control) \u251c\u2500\u2500 requirements.txt # List of project dependencies \u251c\u2500\u2500 .gitignore # Files and directories to ignore in Git \u2514\u2500\u2500 README.md # Project overview and documentation","title":"01 - Python Good Starting"},{"location":"documentation/python/01_python_good_starting/#python-good-starting","text":"When beginning your journey with Python, it\u2019s crucial to set up your development environment properly to avoid potential conflicts and ensure a smooth experience. This guide will walk you through the steps of installing Python correctly using pyenv , managing different Python versions, and setting up a virtual environment for your projects.","title":"Python Good Starting"},{"location":"documentation/python/01_python_good_starting/#table-of-contents","text":"Python Good Starting Why Learn Python Correctly from Scratch? Pyenv 1. To install pyenv 2. Configure Shell for pyenv 3. Restart Shell 4. Check Available Python Versions 5. Install a Python Version with pyenv 6. Check Installed Python Versions 7. Select a Python Version VirtualEnv 1. Install virtualenv 2. Create a Virtual Environment with virtualenv 3. Activate the Virtual Environment Install Dependencies Project Structure Overview","title":"Table of Contents"},{"location":"documentation/python/01_python_good_starting/#why-learn-python-correctly-from-scratch","text":"Consistency Across Environments : Setting up a consistent Python environment ensures that your code runs the same way on different systems . Avoid System Conflicts : Using tools like pyenv and virtual environments prevents conflicts with system-installed Python versions and other projects. Manage Dependencies : Virtual environments help you manage project-specific dependencies without affecting your global Python installation (useful for Linux users).","title":"Why Learn Python Correctly from Scratch?"},{"location":"documentation/python/01_python_good_starting/#pyenv","text":"pyenv is a tool that allows you to easily install and manage multiple versions of Python. It helps you avoid conflicts with the system Python and manage different versions for different projects.","title":"Pyenv"},{"location":"documentation/python/01_python_good_starting/#1-to-install-pyenv","text":"On macOS/Linux: curl https://pyenv.run | bash","title":"1. To install pyenv:"},{"location":"documentation/python/01_python_good_starting/#2-configure-shell-for-pyenv","text":"After running the command, add the following lines to your shell startup file ( .bashrc , .zshrc , etc.): # Pyenv configuration export PATH=\"$HOME/.pyenv/bin:$PATH\" eval \"$(pyenv init --path)\" eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" This step is also important to avoid pip conflicts. Create an alias in .bashrc , .zshrc , etc., to ensure you are using pip from the correct pyenv installation: # Configure your PIP as well alias pip='python -m pip'","title":"2. Configure Shell for pyenv"},{"location":"documentation/python/01_python_good_starting/#3-restart-shell","text":"Restart your shell or run source ~/.bashrc (or the corresponding file) to apply the changes.","title":"3. Restart Shell"},{"location":"documentation/python/01_python_good_starting/#4-check-available-python-versions","text":"To see the available Python versions that you can install with pyenv , use: pyenv install --list","title":"4. Check Available Python Versions:"},{"location":"documentation/python/01_python_good_starting/#5-install-a-python-version-with-pyenv","text":"To install a specific version of Python, use: # Replace <version> with the desired version number (e.g., 3.9.1). pyenv install <version>","title":"5. Install a Python Version with pyenv"},{"location":"documentation/python/01_python_good_starting/#6-check-installed-python-versions","text":"To see which Python versions are installed on your system and identify the active version: pyenv versions # Example Output: system # Python from your system 3.7.17 * 3.11.9 # (the * represents the active version) 3.12.0","title":"6. Check Installed Python Versions"},{"location":"documentation/python/01_python_good_starting/#7-select-a-python-version","text":"pyenv global <python-version>","title":"7. Select a Python Version"},{"location":"documentation/python/01_python_good_starting/#virtualenv","text":"virtualenv is a tool to create isolated Python environments, ensuring that dependencies for one project don\u2019t interfere with those of another.","title":"VirtualEnv"},{"location":"documentation/python/01_python_good_starting/#1-install-virtualenv","text":"pip install virtualenv","title":"1. Install virtualenv"},{"location":"documentation/python/01_python_good_starting/#2-create-a-virtual-environment-with-virtualenv","text":"Select the directory where you will start your project (typically a new folder), then run: # You don't need to specify the python version because pyenv is doing it when you # choose the global version that you want to use on the previous steps. Of course # if you want to change the python version, you have to do it on pyenv first before # execute this command. virtualenv --python=python <python-env-name> # Recommended to use \"env\" as the environment name","title":"2. Create a Virtual Environment with virtualenv"},{"location":"documentation/python/01_python_good_starting/#3-activate-the-virtual-environment","text":"source <python-env-name>/bin/activate When the environment is active, its name appears at the beginning of your terminal prompt: (<python-env-name>) user@machine:~/<project-directory>$","title":"3. Activate the Virtual Environment"},{"location":"documentation/python/01_python_good_starting/#install-dependencies","text":"After creating and activating the virtual environment, it\u2019s a good practice to create a requirements.txt file listing the Python libraries required for the project: # requirements.txt file requests==2.25.1 selenium==4.4.1 bs4==0.0.2 klein_config==4.0.2 Install all dependencies at once from requirements.txt : pip install -r requirements.txt","title":"Install Dependencies"},{"location":"documentation/python/01_python_good_starting/#project-structure-overview","text":"Here\u2019s an example of a simple, well-organized Python project structure: my_project/ \u251c\u2500\u2500 app/ # Your application \u251c\u2500\u2500 env/ # Virtual environment directory (exclude from version control) \u251c\u2500\u2500 requirements.txt # List of project dependencies \u251c\u2500\u2500 .gitignore # Files and directories to ignore in Git \u2514\u2500\u2500 README.md # Project overview and documentation","title":"Project Structure Overview"},{"location":"documentation/python/02_python_organize_modules/","text":"02 - Organizing into Modules Introduction In Python, organizing your code into small, manageable modules is crucial for maintaining clean, readable, and reusable code . Instead of writing all your functions and logic in a single file, it's best practice to divide your code into multiple modules (i.e., separate Python files) that each focus on a specific aspect of your program . This approach not only improves code readability but also allows for easier debugging, testing, and reuse of functions across different projects . Briefly, Instead of cramming all your code into a single file, break it down into smaller, focused pieces. Organize these pieces into readable folders based on their functionality. This makes your code easier to manage, understand, and reuse. How to Organize Code into Modules Identify the Functionality : Start by identifying distinct functionalities in your project. For example, if you are building a web scraper, you might have functionalities like HTTP requests, data parsing, and data storage. Create Separate Python Files (Modules) : Create separate Python files for each functionality. Write Functions and Classes : In each module, write functions and classes specific to the functionality of that module. Make sure the functions are generic enough to be reusable. Import and Use the Modules : In your main script or other modules, import the necessary functions or classes from your modules. Example of Well Structured Project my_project/ \u2502 \u251c\u2500\u2500 README.md # Project overview and instructions \u251c\u2500\u2500 requirements.txt # Dependencies for the project \u251c\u2500\u2500 setup.py # Installation and packaging script \u251c\u2500\u2500 config.yml # YAML file for environment variables and application settings \u251c\u2500\u2500 .gitignore # Git ignore rules \u2514\u2500\u2500 my_project/ # Main source code directory \u251c\u2500\u2500 __init__.py # Initializes the package \u251c\u2500\u2500 main.py # Application entry point \u251c\u2500\u2500 settings.py # Settings APP \u251c\u2500\u2500 utils/ # Utility functions \u2502 \u251c\u2500\u2500 __init__.py # Initializes the utils package \u2502 \u251c\u2500\u2500 file_utils.py # File operation utilities \u2502 \u2514\u2500\u2500 data_utils.py # Data manipulation utilities \u251c\u2500\u2500 modules/ # Core modules \u2502 \u251c\u2500\u2500 __init__.py # Initializes the modules package \u2502 \u251c\u2500\u2500 data_processing.py # Data processing logic \u2502 \u2514\u2500\u2500 model.py # Business logic or machine learning model \u2514\u2500\u2500 tests/ # Test cases \u251c\u2500\u2500 __init__.py # Initializes the tests package \u251c\u2500\u2500 test_data_processing.py # Unit tests for data_processing \u2514\u2500\u2500 test_model.py # Unit tests for the model Conclusion Organizing your Python code into well-structured modules is a critical practice for maintaining a clean, efficient, and scalable project. By separating your code into distinct modules based on functionality, you can enhance code readability, make debugging easier, and increase the reusability of your functions across different projects. This modular approach not only streamlines development but also makes collaboration with other developers more straightforward, as the codebase becomes more organized and understandable. Following the example provided, you can see how a well-structured project allows for clear separation of concerns, making it easy to navigate, maintain, and extend your code. Adopting these practices will lead to more robust and maintainable Python applications in the long run.","title":"02 - Organazing into Modules"},{"location":"documentation/python/02_python_organize_modules/#02-organizing-into-modules","text":"","title":"02 - Organizing into Modules"},{"location":"documentation/python/02_python_organize_modules/#introduction","text":"In Python, organizing your code into small, manageable modules is crucial for maintaining clean, readable, and reusable code . Instead of writing all your functions and logic in a single file, it's best practice to divide your code into multiple modules (i.e., separate Python files) that each focus on a specific aspect of your program . This approach not only improves code readability but also allows for easier debugging, testing, and reuse of functions across different projects . Briefly, Instead of cramming all your code into a single file, break it down into smaller, focused pieces. Organize these pieces into readable folders based on their functionality. This makes your code easier to manage, understand, and reuse.","title":"Introduction"},{"location":"documentation/python/02_python_organize_modules/#how-to-organize-code-into-modules","text":"Identify the Functionality : Start by identifying distinct functionalities in your project. For example, if you are building a web scraper, you might have functionalities like HTTP requests, data parsing, and data storage. Create Separate Python Files (Modules) : Create separate Python files for each functionality. Write Functions and Classes : In each module, write functions and classes specific to the functionality of that module. Make sure the functions are generic enough to be reusable. Import and Use the Modules : In your main script or other modules, import the necessary functions or classes from your modules.","title":"How to Organize Code into Modules"},{"location":"documentation/python/02_python_organize_modules/#example-of-well-structured-project","text":"my_project/ \u2502 \u251c\u2500\u2500 README.md # Project overview and instructions \u251c\u2500\u2500 requirements.txt # Dependencies for the project \u251c\u2500\u2500 setup.py # Installation and packaging script \u251c\u2500\u2500 config.yml # YAML file for environment variables and application settings \u251c\u2500\u2500 .gitignore # Git ignore rules \u2514\u2500\u2500 my_project/ # Main source code directory \u251c\u2500\u2500 __init__.py # Initializes the package \u251c\u2500\u2500 main.py # Application entry point \u251c\u2500\u2500 settings.py # Settings APP \u251c\u2500\u2500 utils/ # Utility functions \u2502 \u251c\u2500\u2500 __init__.py # Initializes the utils package \u2502 \u251c\u2500\u2500 file_utils.py # File operation utilities \u2502 \u2514\u2500\u2500 data_utils.py # Data manipulation utilities \u251c\u2500\u2500 modules/ # Core modules \u2502 \u251c\u2500\u2500 __init__.py # Initializes the modules package \u2502 \u251c\u2500\u2500 data_processing.py # Data processing logic \u2502 \u2514\u2500\u2500 model.py # Business logic or machine learning model \u2514\u2500\u2500 tests/ # Test cases \u251c\u2500\u2500 __init__.py # Initializes the tests package \u251c\u2500\u2500 test_data_processing.py # Unit tests for data_processing \u2514\u2500\u2500 test_model.py # Unit tests for the model","title":"Example of Well Structured Project"},{"location":"documentation/python/02_python_organize_modules/#conclusion","text":"Organizing your Python code into well-structured modules is a critical practice for maintaining a clean, efficient, and scalable project. By separating your code into distinct modules based on functionality, you can enhance code readability, make debugging easier, and increase the reusability of your functions across different projects. This modular approach not only streamlines development but also makes collaboration with other developers more straightforward, as the codebase becomes more organized and understandable. Following the example provided, you can see how a well-structured project allows for clear separation of concerns, making it easy to navigate, maintain, and extend your code. Adopting these practices will lead to more robust and maintainable Python applications in the long run.","title":"Conclusion"},{"location":"documentation/python/03_configuration_management/","text":"03 Configuration Management Guide with klein-config This guide explains how to manage configuration settings in a Python project using the klein-config library. klein-config supports configurations from various sources like configuration files, environment variables, and command-line arguments, offering a flexible approach to configuration management in Python projects. Table of Contents 1. Installing klein-config 2. Creating a Configuration File 2.1 Using YAML 2.2 Using JSON 3. Setting the Configuration File Path 4. Creating a settings.py File 5. Practical Example: Using Imported Configurations 1. Installing klein-config Before you start, ensure you have Python and pip installed on your machine. Install klein-config using pip: pip install klein-config 2. Creating a Configuration File You can use either a YAML file or a JSON file to store your configuration settings. Below are examples of each. 2.1 Using YAML Create a configuration file named config.yml with your settings in YAML format: logger: level: INFO mongo: hostname: mongodb port: 27017 database: task_management collection: task 2.2 Using JSON Alternatively, create a JSON configuration file named config.json with the same structure in JSON format: { \"logger\": { \"level\": \"INFO\" }, \"mongo\": { \"hostname\": \"mongodb\", \"port\": 27017, \"database\": \"task_management\", \"collection\": \"task\" } } 3. Setting the Configuration File Path Set the path to your config.yml or config.json file in the KLEIN_CONFIG environment variable: export KLEIN_CONFIG=./my-path/config.yml # Or if using JSON: export KLEIN_CONFIG=./my-path/config.json 4. Creating a settings.py File In your Python project, create a settings.py file to import and use configuration settings. Example: # settings.py import logging from klein_config import get_config # Get config from the environment variable config = get_config() ####### MongoDB Settings ######## MONGO_HOSTNAME = config.get(\"mongo.hostname\") MONGO_PORT = config.get(\"mongo.port\") MONGO_DATABASE = config.get(\"mongo.database\") MONGO_COLLECTION = config.get(\"mongo.collection\") ############ Logger ############# logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) 5. Practical Example: Using Imported Configurations After setting up environment variables, you can import these variables or instances anywhere in your Python code. This approach avoids hardcoding or duplicating code, enhancing readability and maintainability. Project Structure . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 mongo \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 mongo_connection.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 settings.py \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 requirements.txt MongoDB Connection in mongo_connection.py In mongo_connection.py , import the MongoDB settings from settings.py and establish a connection without repeating configuration code: # File: mongo_connection.py import pymongo from settings import ( MONGO_HOSTNAME, MONGO_COLLECTION, MONGO_DATABASE, MONGO_PORT ) mongo_client = pymongo.MongoClient(MONGO_HOSTNAME, MONGO_PORT) db = mongo_client[MONGO_DATABASE] mongo_collection = db[MONGO_COLLECTION] Now, you can simply import the mongo_collection variable wherever needed without worrying about the connection or configuration details. Example Usage in main.py In main.py , you can use mongo_collection to interact with your database: # main.py from app.mongo.mongo_connection import mongo_collection if __name__ == \"__main__\": fake_data = { \"name\": \"Task 1\", \"description\": \"This is a sample task\", \"status\": \"pending\" } # Insert data into mongo collection mongo_collection.insert_one(fake_data) This approach creates modular, organized pipelines in your code, allowing configuration imports only when necessary, which enhances readability and maintainability. Author Miguel Angelo do Amaral Junior","title":"03 - Configuration Management"},{"location":"documentation/python/03_configuration_management/#03-configuration-management-guide-with-klein-config","text":"This guide explains how to manage configuration settings in a Python project using the klein-config library. klein-config supports configurations from various sources like configuration files, environment variables, and command-line arguments, offering a flexible approach to configuration management in Python projects.","title":"03 Configuration Management Guide with klein-config"},{"location":"documentation/python/03_configuration_management/#table-of-contents","text":"1. Installing klein-config 2. Creating a Configuration File 2.1 Using YAML 2.2 Using JSON 3. Setting the Configuration File Path 4. Creating a settings.py File 5. Practical Example: Using Imported Configurations","title":"Table of Contents"},{"location":"documentation/python/03_configuration_management/#1-installing-klein-config","text":"Before you start, ensure you have Python and pip installed on your machine. Install klein-config using pip: pip install klein-config","title":"1. Installing klein-config"},{"location":"documentation/python/03_configuration_management/#2-creating-a-configuration-file","text":"You can use either a YAML file or a JSON file to store your configuration settings. Below are examples of each.","title":"2. Creating a Configuration File"},{"location":"documentation/python/03_configuration_management/#21-using-yaml","text":"Create a configuration file named config.yml with your settings in YAML format: logger: level: INFO mongo: hostname: mongodb port: 27017 database: task_management collection: task","title":"2.1 Using YAML"},{"location":"documentation/python/03_configuration_management/#22-using-json","text":"Alternatively, create a JSON configuration file named config.json with the same structure in JSON format: { \"logger\": { \"level\": \"INFO\" }, \"mongo\": { \"hostname\": \"mongodb\", \"port\": 27017, \"database\": \"task_management\", \"collection\": \"task\" } }","title":"2.2 Using JSON"},{"location":"documentation/python/03_configuration_management/#3-setting-the-configuration-file-path","text":"Set the path to your config.yml or config.json file in the KLEIN_CONFIG environment variable: export KLEIN_CONFIG=./my-path/config.yml # Or if using JSON: export KLEIN_CONFIG=./my-path/config.json","title":"3. Setting the Configuration File Path"},{"location":"documentation/python/03_configuration_management/#4-creating-a-settingspy-file","text":"In your Python project, create a settings.py file to import and use configuration settings. Example: # settings.py import logging from klein_config import get_config # Get config from the environment variable config = get_config() ####### MongoDB Settings ######## MONGO_HOSTNAME = config.get(\"mongo.hostname\") MONGO_PORT = config.get(\"mongo.port\") MONGO_DATABASE = config.get(\"mongo.database\") MONGO_COLLECTION = config.get(\"mongo.collection\") ############ Logger ############# logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__)","title":"4. Creating a settings.py File"},{"location":"documentation/python/03_configuration_management/#5-practical-example-using-imported-configurations","text":"After setting up environment variables, you can import these variables or instances anywhere in your Python code. This approach avoids hardcoding or duplicating code, enhancing readability and maintainability.","title":"5. Practical Example: Using Imported Configurations"},{"location":"documentation/python/03_configuration_management/#project-structure","text":". \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 mongo \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 mongo_connection.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 settings.py \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 requirements.txt","title":"Project Structure"},{"location":"documentation/python/03_configuration_management/#mongodb-connection-in-mongo_connectionpy","text":"In mongo_connection.py , import the MongoDB settings from settings.py and establish a connection without repeating configuration code: # File: mongo_connection.py import pymongo from settings import ( MONGO_HOSTNAME, MONGO_COLLECTION, MONGO_DATABASE, MONGO_PORT ) mongo_client = pymongo.MongoClient(MONGO_HOSTNAME, MONGO_PORT) db = mongo_client[MONGO_DATABASE] mongo_collection = db[MONGO_COLLECTION] Now, you can simply import the mongo_collection variable wherever needed without worrying about the connection or configuration details.","title":"MongoDB Connection in mongo_connection.py"},{"location":"documentation/python/03_configuration_management/#example-usage-in-mainpy","text":"In main.py , you can use mongo_collection to interact with your database: # main.py from app.mongo.mongo_connection import mongo_collection if __name__ == \"__main__\": fake_data = { \"name\": \"Task 1\", \"description\": \"This is a sample task\", \"status\": \"pending\" } # Insert data into mongo collection mongo_collection.insert_one(fake_data) This approach creates modular, organized pipelines in your code, allowing configuration imports only when necessary, which enhances readability and maintainability.","title":"Example Usage in main.py"},{"location":"documentation/python/03_configuration_management/#author","text":"Miguel Angelo do Amaral Junior","title":"Author"},{"location":"documentation/python/04_typing_docstring/","text":"04 - The Importance of Typing and Docstrings in Python Overview In Python, the use of typing & docstrings is critical for maintaining high-quality, maintainable, and readable code. These practices not only improve the development process but also enhance collaboration within the team . This document explains the importance of typing and docstrings and how they contribute to a more efficient and robust codebase. Importance of Typing What is Typing? Typing in Python refers to the practice of explicitly specifying the data types of variables, function arguments, and return values. Python is a dynamically-typed language, meaning that the interpreter determines the type of a variable at runtime. However, using type hints (introduced in Python 3.5 ) allows developers to indicate the expected data types. Benefits of Typing Code Clarity: Type hints make the code more understandable by clearly indicating what types of data are expected. This reduces ambiguity and makes it easier to follow the logic of the code. Error Detection: Typing helps catch potential bugs early in the development process. Static type checkers, like mypy , can be used to identify type-related errors before the code is run, reducing runtime errors. Improved IDE Support: Modern IDEs and editors provide better autocomplete and linting features when typing is used. This results in faster development and fewer mistakes. Ease of Refactoring: With type hints, refactoring code becomes safer and more manageable. Developers can confidently change code, knowing that the type checker will flag any inconsistencies. Enhanced Collaboration: Typing provides clear documentation for other developers on the team, making it easier for them to understand and work with the code. This is especially important in larger projects with multiple contributors. Examples of Typing def add(a: int, b: int) -> int: return a + b In this example, the function add explicitly states that it expects two integers as arguments and will return an integer. This clarity is beneficial for anyone reading the code even if it is a simple function, it is always good practice to use typing annotations in your code . Importance of Docstrings What are Docstrings? Docstrings are a form of documentation embedded within Python code. They are string literals placed in specific locations in the code (such as at the beginning of modules, functions, classes, or methods) to describe what the code does. Benefits of Docstrings Self-Documenting Code : Docstrings provide immediate documentation that is accessible directly in the code. This helps developers understand the purpose and usage of different parts of the code without needing to refer to external documentation. Consistency : Using a standard format for docstrings (such as PEP 257) ensures that documentation is consistent across the codebase, making it easier for developers to find and understand information. API Documentation : Tools like Sphinx can automatically generate API documentation from docstrings, making it easier to maintain up-to-date and comprehensive documentation for end-users or developers using the API. Improved Code Quality : Well-documented code often correlates with well-thought-out and structured code. Writing docstrings forces developers to think through their code's design and functionality, often leading to higher-quality code. General Format Location : Docstrings should be immediately after the definition of modules, classes, functions, or methods, and should be enclosed in triple double quotes ( \"\"\" ). Style : The content of the docstring should start with a concise description of what the module, class, function, or method does. For functions and methods, the description should begin with a verb in the infinitive form. Structure of the Docstring Short Description : Start with a brief and descriptive line about the main purpose. This line should be a complete sentence and should not exceed 72 characters per line. Detailed Description : If necessary, add a blank line after the short description and provide a more detailed description of the behavior or functionality, if it helps to understand better. Parameters and Returns : For functions and methods, include sections for Parameters and Returns that describe the input parameters and the returned value, respectively. The Parameters section should list each parameter with its name, type, and a brief description. The Returns section should describe the type and purpose of the returned value. Exceptions : If the function or method can raise exceptions, add a Raises section to document which exceptions can be raised and under what conditions. Examples of Docstrings def add(a: int, b: int) -> int: \"\"\" Adds two integers together. Parameters: a (int): The first integer. b (int): The second integer. Returns: int: The sum of the two integers. \"\"\" return a + b In this example, the docstring provides a clear and concise explanation of what the function does, its parameters, and its return value. Best Practices Use Typing Consistently : Always use typing where applicable. This includes function arguments, return types, and variable annotations. Write Clear Docstrings : Follow the conventions outlined in PEP 257. Be concise but thorough in describing what the code does. Review and Update : Regularly review and update typings and docstrings to ensure they remain accurate as the code evolves. Leverage Tools : Utilize tools like mypy for type checking and Sphinx for generating documentation from docstrings. Conclusion Incorporating typing and docstrings into your development workflow significantly enhances the readability, maintainability, and reliability of your code. These practices not only help individual developers but also foster better teamwork and collaboration by providing clear and consistent documentation. By adhering to these standards, the team can ensure a more efficient and error-free development process. Author Miguel Angelo Do Amaral Junior","title":"04 - Typing & Docstring"},{"location":"documentation/python/04_typing_docstring/#04-the-importance-of-typing-and-docstrings-in-python","text":"","title":"04 - The Importance of Typing and Docstrings in Python"},{"location":"documentation/python/04_typing_docstring/#overview","text":"In Python, the use of typing & docstrings is critical for maintaining high-quality, maintainable, and readable code. These practices not only improve the development process but also enhance collaboration within the team . This document explains the importance of typing and docstrings and how they contribute to a more efficient and robust codebase.","title":"Overview"},{"location":"documentation/python/04_typing_docstring/#importance-of-typing","text":"","title":"Importance of Typing"},{"location":"documentation/python/04_typing_docstring/#what-is-typing","text":"Typing in Python refers to the practice of explicitly specifying the data types of variables, function arguments, and return values. Python is a dynamically-typed language, meaning that the interpreter determines the type of a variable at runtime. However, using type hints (introduced in Python 3.5 ) allows developers to indicate the expected data types.","title":"What is Typing?"},{"location":"documentation/python/04_typing_docstring/#benefits-of-typing","text":"Code Clarity: Type hints make the code more understandable by clearly indicating what types of data are expected. This reduces ambiguity and makes it easier to follow the logic of the code. Error Detection: Typing helps catch potential bugs early in the development process. Static type checkers, like mypy , can be used to identify type-related errors before the code is run, reducing runtime errors. Improved IDE Support: Modern IDEs and editors provide better autocomplete and linting features when typing is used. This results in faster development and fewer mistakes. Ease of Refactoring: With type hints, refactoring code becomes safer and more manageable. Developers can confidently change code, knowing that the type checker will flag any inconsistencies. Enhanced Collaboration: Typing provides clear documentation for other developers on the team, making it easier for them to understand and work with the code. This is especially important in larger projects with multiple contributors.","title":"Benefits of Typing"},{"location":"documentation/python/04_typing_docstring/#examples-of-typing","text":"def add(a: int, b: int) -> int: return a + b In this example, the function add explicitly states that it expects two integers as arguments and will return an integer. This clarity is beneficial for anyone reading the code even if it is a simple function, it is always good practice to use typing annotations in your code .","title":"Examples of Typing"},{"location":"documentation/python/04_typing_docstring/#importance-of-docstrings","text":"","title":"Importance of Docstrings"},{"location":"documentation/python/04_typing_docstring/#what-are-docstrings","text":"Docstrings are a form of documentation embedded within Python code. They are string literals placed in specific locations in the code (such as at the beginning of modules, functions, classes, or methods) to describe what the code does.","title":"What are Docstrings?"},{"location":"documentation/python/04_typing_docstring/#benefits-of-docstrings","text":"Self-Documenting Code : Docstrings provide immediate documentation that is accessible directly in the code. This helps developers understand the purpose and usage of different parts of the code without needing to refer to external documentation. Consistency : Using a standard format for docstrings (such as PEP 257) ensures that documentation is consistent across the codebase, making it easier for developers to find and understand information. API Documentation : Tools like Sphinx can automatically generate API documentation from docstrings, making it easier to maintain up-to-date and comprehensive documentation for end-users or developers using the API. Improved Code Quality : Well-documented code often correlates with well-thought-out and structured code. Writing docstrings forces developers to think through their code's design and functionality, often leading to higher-quality code.","title":"Benefits of Docstrings"},{"location":"documentation/python/04_typing_docstring/#general-format","text":"Location : Docstrings should be immediately after the definition of modules, classes, functions, or methods, and should be enclosed in triple double quotes ( \"\"\" ). Style : The content of the docstring should start with a concise description of what the module, class, function, or method does. For functions and methods, the description should begin with a verb in the infinitive form.","title":"General Format"},{"location":"documentation/python/04_typing_docstring/#structure-of-the-docstring","text":"Short Description : Start with a brief and descriptive line about the main purpose. This line should be a complete sentence and should not exceed 72 characters per line. Detailed Description : If necessary, add a blank line after the short description and provide a more detailed description of the behavior or functionality, if it helps to understand better. Parameters and Returns : For functions and methods, include sections for Parameters and Returns that describe the input parameters and the returned value, respectively. The Parameters section should list each parameter with its name, type, and a brief description. The Returns section should describe the type and purpose of the returned value. Exceptions : If the function or method can raise exceptions, add a Raises section to document which exceptions can be raised and under what conditions.","title":"Structure of the Docstring"},{"location":"documentation/python/04_typing_docstring/#examples-of-docstrings","text":"def add(a: int, b: int) -> int: \"\"\" Adds two integers together. Parameters: a (int): The first integer. b (int): The second integer. Returns: int: The sum of the two integers. \"\"\" return a + b In this example, the docstring provides a clear and concise explanation of what the function does, its parameters, and its return value.","title":"Examples of Docstrings"},{"location":"documentation/python/04_typing_docstring/#best-practices","text":"Use Typing Consistently : Always use typing where applicable. This includes function arguments, return types, and variable annotations. Write Clear Docstrings : Follow the conventions outlined in PEP 257. Be concise but thorough in describing what the code does. Review and Update : Regularly review and update typings and docstrings to ensure they remain accurate as the code evolves. Leverage Tools : Utilize tools like mypy for type checking and Sphinx for generating documentation from docstrings.","title":"Best Practices"},{"location":"documentation/python/04_typing_docstring/#conclusion","text":"Incorporating typing and docstrings into your development workflow significantly enhances the readability, maintainability, and reliability of your code. These practices not only help individual developers but also foster better teamwork and collaboration by providing clear and consistent documentation. By adhering to these standards, the team can ensure a more efficient and error-free development process.","title":"Conclusion"},{"location":"documentation/python/04_typing_docstring/#author","text":"Miguel Angelo Do Amaral Junior","title":"Author"},{"location":"documentation/python/05_built-in_functions/","text":"05 - Python Built-in Functions Overview Python, a versatile and widely-used programming language, comes equipped with a robust set of built-in functions. These functions are an essential part of Python, as they allow developers to perform common tasks efficiently without the need to import additional libraries. As of Python 3.11, there are approximately 70 built-in functions available. Understanding these functions is crucial for any Python programmer, as they form the foundation for more complex coding tasks and help in writing concise, readable, and efficient code. Importance of Understanding Built-in Functions Efficiency : Built-in functions are optimized and written in C, making them faster than custom implementations. Readability : Using built-in functions makes code more readable and easier to understand, as they are widely recognized by the Python community. Maintainability : Code that relies on built-in functions is often easier to maintain and update. Reduced Complexity : By leveraging these functions, you can minimize the complexity of your code, focusing more on solving the problem rather than reinventing the wheel. Categories of Built-in Functions 1. Type Conversion Functions Purpose : Convert values from one type to another. Function Description Example Result int() Converts a value to an integer. int('5') 5 float() Converts a value to a floating-point number. float('3.14') 3.14 complex() Converts a value to a complex number. complex(2, 3) (2+3j) str() Converts a value to a string. str(123) '123' repr() Returns a string representation of an object. repr([1, 2, 3]) '[1, 2, 3]' bool() Converts a value to a boolean. bool(1) True list() Converts a value to a list. list('abc') ['a', 'b', 'c'] dict() Converts a value to a dictionary. dict(a=1, b=2) {'a': 1, 'b': 2} set() Converts a value to a set. set([1, 2, 3]) {1, 2, 3} tuple() Converts a value to a tuple. tuple([1, 2, 3]) (1, 2, 3) frozenset() Converts a value to a frozenset. frozenset([1, 2, 3]) frozenset({1, 2, 3}) chr() Converts an integer to a Unicode character. chr(65) 'A' ord() Converts a character to its Unicode code. ord('A') 65 bin() Converts an integer to a binary string. bin(10) '0b1010' oct() Converts an integer to an octal string. oct(10) '0o12' hex() Converts an integer to a hexadecimal string. hex(10) '0xa' bytes() Converts a value to a bytes object. bytes('abc', 'utf-8') b'abc' bytearray() Converts a value to a bytearray object. bytearray('abc', 'utf-8') bytearray(b'abc') memoryview() Creates a memory view object from a bytes-like object. memoryview(b'abc') <memoryview> 2. Sequence and Collection Manipulation Functions Purpose : Perform operations on sequences (lists, tuples, strings) and collections (sets, dictionaries). Function Description Example Result len() Returns the length of an object. len([1, 2, 3]) 3 min() Returns the smallest item in an iterable. min([1, 2, 3]) 1 max() Returns the largest item in an iterable. max([1, 2, 3]) 3 sum() Returns the sum of all items in an iterable. sum([1, 2, 3]) 6 sorted() Returns a sorted list from an iterable. sorted([3, 1, 2]) [1, 2, 3] reversed() Returns a reversed iterator for a sequence. reversed([1, 2, 3]) <reversed object> enumerate() Returns an enumerate object, yielding pairs of index and value. enumerate(['a', 'b']) [(0, 'a'), (1, 'b')] zip() Returns an iterator aggregating elements from iterables. zip([1, 2], ['a', 'b']) [(1, 'a'), (2, 'b')] map() Applies a function to all items in an iterable. map(str, [1, 2]) ['1', '2'] filter() Filters elements in an iterable based on a function. filter(bool, [0, 1, 2]) [1, 2] 3. Mathematical and Numeric Functions Purpose : Execute basic mathematical operations. Function Description Example Result abs() Returns the absolute value of a number. abs(-10) 10 round() Rounds a floating-point number to the nearest integer. round(3.14159, 2) 3.14 divmod() Returns a pair of quotient and remainder. divmod(10, 3) (3, 1) pow() Returns the value of a number raised to a power. pow(2, 3) 8 sum() Sums items of an iterable (optional start value). sum([1, 2, 3]) 6 max() Returns the maximum value in an iterable or arguments. max([1, 2, 3]) 3 min() Returns the minimum value in an iterable or arguments. min([1, 2, 3]) 1 4. Input/Output Functions Purpose : Interact with the user or manipulate files. Function Description Example Result print() Outputs a message to the console. print(\"Hello, World!\") Hello, World! input() Reads a string from user input. input(\"Enter name: \") Returns user input as a string. open() Opens a file and returns a corresponding file object. open('file.txt', 'r') Opens the file for reading. 5. Object Manipulation Functions Purpose : Work with objects and attributes. type() : Returns the type of an object. isinstance() : Checks if an object is an instance of a class or tuple of classes. id() : Returns the unique identifier of an object. dir() : Returns a list of valid attributes for an object. vars() : Returns the __dict__ attribute of an object. hasattr() : Checks if an object has a specified attribute. getattr() : Retrieves the value of a named attribute. setattr() : Sets the value of a named attribute. delattr() : Deletes a named attribute. Function Description Example Result type() Returns the type of an object. type(123) <class 'int'> isinstance() Checks if an object is an instance of a class or tuple of classes. isinstance(123, int) True id() Returns the unique identifier of an object. id(123) Some unique integer value dir() Returns a list of valid attributes for an object. dir([1, 2, 3]) List of attributes for the list object vars() Returns the __dict__ attribute of an object. vars({\"key\": \"value\"}) {'key': 'value'} hasattr() Checks if an object has a specified attribute. hasattr([], 'append') True getattr() Retrieves the value of a named attribute. getattr([], 'append') <built-in method append of list object> setattr() Sets the value of a named attribute. setattr([], 'new_attr', 42) No output, sets attribute new_attr to 42 delattr() Deletes a named attribute. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr 6. Memory and System Manipulation Functions Purpose : Work with memory and low-level system aspects. Function Description Example Result memoryview() Returns a memory view object. mv = memoryview(b'hello'); mv[0] 104 (the ASCII value of 'h') hash() Returns the hash value of an object. hash(\"hello\") A unique integer value (e.g., -1017891587 ) object() Returns a new featureless object. obj = object() A new object with no specific attributes del() Deletes an object. x = 10; del x No output, deletes variable x delattr() Deletes an attribute from an object. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr 7. Miscellaneous Functions Purpose : Functions that don't fit neatly into other categories. Function Description Example Result help() Invokes the built-in help system. help(str) Displays help information about the str class globals() Returns a dictionary representing the current global symbol table. globals() A dictionary of global variables and their values locals() Returns a dictionary representing the current local symbol table. locals() A dictionary of local variables and their values eval() Evaluates a Python expression from a string-based input. eval('2 + 3') 5 exec() Executes Python code dynamically from a string or compiled code. exec('x = 5') No output; creates variable x with value 5 compile() Compiles a source into a code or AST object. code = compile('print(\"Hello\")', '<string>', 'exec'); exec(code) Hello super() Returns a proxy object that delegates method calls to a parent or sibling class. class Parent: pass; class Child(Parent): pass; super(Child, Child()) <super object> staticmethod() Returns a static method for a function. class MyClass: @staticmethod def my_method(): pass No output; creates a static method in MyClass classmethod() Returns a class method for a function. class MyClass: @classmethod def my_method(cls): pass No output; creates a class method in MyClass property() Returns a property attribute for a class. class MyClass: def my_property(self): return 42; my_property = property(my_property) A property that returns 42 8. Exception Handling Functions Purpose : Handle exceptions and errors. Function/Class Description Example Result issubclass() Checks if a class is a subclass of another class. issubclass(bool, int) True BaseException The base class for all built-in exceptions. BaseException <class 'BaseException'> Exception The base class for most built-in exceptions. Exception <class 'Exception'> 9. Memory and System Manipulation Functions Purpose : Work with memory and system aspects. Function Description Example Result memoryview() Returns a memory view object. mv = memoryview(b'hello'); mv[0] 104 (the ASCII value of 'h') hash() Returns the hash value of an object. hash(\"hello\") A unique integer value (e.g., -1017891587 ) object() Returns a new featureless object. obj = object() A new object with no specific attributes del() Deletes an object. x = 10; del x No output, deletes variable x delattr() Deletes an attribute from an object. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr 10. Input/Output Functions Purpose : Interact with the user or manipulate files. print() : Outputs a message to the console. input() : Reads a string from user input. open() : Opens a file and returns a corresponding file object. Conclusion Mastering Python's built-in functions is a critical step in becoming a proficient Python developer. These functions provide the building blocks for creating efficient, readable, and maintainable code. By leveraging these functions, developers can save time, reduce errors, and produce higher-quality software. Author Miguel Angelo Do Amaral Junior","title":"05 - Built-in functions"},{"location":"documentation/python/05_built-in_functions/#05-python-built-in-functions","text":"","title":"05 - Python Built-in Functions"},{"location":"documentation/python/05_built-in_functions/#overview","text":"Python, a versatile and widely-used programming language, comes equipped with a robust set of built-in functions. These functions are an essential part of Python, as they allow developers to perform common tasks efficiently without the need to import additional libraries. As of Python 3.11, there are approximately 70 built-in functions available. Understanding these functions is crucial for any Python programmer, as they form the foundation for more complex coding tasks and help in writing concise, readable, and efficient code.","title":"Overview"},{"location":"documentation/python/05_built-in_functions/#importance-of-understanding-built-in-functions","text":"Efficiency : Built-in functions are optimized and written in C, making them faster than custom implementations. Readability : Using built-in functions makes code more readable and easier to understand, as they are widely recognized by the Python community. Maintainability : Code that relies on built-in functions is often easier to maintain and update. Reduced Complexity : By leveraging these functions, you can minimize the complexity of your code, focusing more on solving the problem rather than reinventing the wheel.","title":"Importance of Understanding Built-in Functions"},{"location":"documentation/python/05_built-in_functions/#categories-of-built-in-functions","text":"","title":"Categories of Built-in Functions"},{"location":"documentation/python/05_built-in_functions/#1-type-conversion-functions","text":"Purpose : Convert values from one type to another. Function Description Example Result int() Converts a value to an integer. int('5') 5 float() Converts a value to a floating-point number. float('3.14') 3.14 complex() Converts a value to a complex number. complex(2, 3) (2+3j) str() Converts a value to a string. str(123) '123' repr() Returns a string representation of an object. repr([1, 2, 3]) '[1, 2, 3]' bool() Converts a value to a boolean. bool(1) True list() Converts a value to a list. list('abc') ['a', 'b', 'c'] dict() Converts a value to a dictionary. dict(a=1, b=2) {'a': 1, 'b': 2} set() Converts a value to a set. set([1, 2, 3]) {1, 2, 3} tuple() Converts a value to a tuple. tuple([1, 2, 3]) (1, 2, 3) frozenset() Converts a value to a frozenset. frozenset([1, 2, 3]) frozenset({1, 2, 3}) chr() Converts an integer to a Unicode character. chr(65) 'A' ord() Converts a character to its Unicode code. ord('A') 65 bin() Converts an integer to a binary string. bin(10) '0b1010' oct() Converts an integer to an octal string. oct(10) '0o12' hex() Converts an integer to a hexadecimal string. hex(10) '0xa' bytes() Converts a value to a bytes object. bytes('abc', 'utf-8') b'abc' bytearray() Converts a value to a bytearray object. bytearray('abc', 'utf-8') bytearray(b'abc') memoryview() Creates a memory view object from a bytes-like object. memoryview(b'abc') <memoryview>","title":"1. Type Conversion Functions"},{"location":"documentation/python/05_built-in_functions/#2-sequence-and-collection-manipulation-functions","text":"Purpose : Perform operations on sequences (lists, tuples, strings) and collections (sets, dictionaries). Function Description Example Result len() Returns the length of an object. len([1, 2, 3]) 3 min() Returns the smallest item in an iterable. min([1, 2, 3]) 1 max() Returns the largest item in an iterable. max([1, 2, 3]) 3 sum() Returns the sum of all items in an iterable. sum([1, 2, 3]) 6 sorted() Returns a sorted list from an iterable. sorted([3, 1, 2]) [1, 2, 3] reversed() Returns a reversed iterator for a sequence. reversed([1, 2, 3]) <reversed object> enumerate() Returns an enumerate object, yielding pairs of index and value. enumerate(['a', 'b']) [(0, 'a'), (1, 'b')] zip() Returns an iterator aggregating elements from iterables. zip([1, 2], ['a', 'b']) [(1, 'a'), (2, 'b')] map() Applies a function to all items in an iterable. map(str, [1, 2]) ['1', '2'] filter() Filters elements in an iterable based on a function. filter(bool, [0, 1, 2]) [1, 2]","title":"2. Sequence and Collection Manipulation Functions"},{"location":"documentation/python/05_built-in_functions/#3-mathematical-and-numeric-functions","text":"Purpose : Execute basic mathematical operations. Function Description Example Result abs() Returns the absolute value of a number. abs(-10) 10 round() Rounds a floating-point number to the nearest integer. round(3.14159, 2) 3.14 divmod() Returns a pair of quotient and remainder. divmod(10, 3) (3, 1) pow() Returns the value of a number raised to a power. pow(2, 3) 8 sum() Sums items of an iterable (optional start value). sum([1, 2, 3]) 6 max() Returns the maximum value in an iterable or arguments. max([1, 2, 3]) 3 min() Returns the minimum value in an iterable or arguments. min([1, 2, 3]) 1","title":"3. Mathematical and Numeric Functions"},{"location":"documentation/python/05_built-in_functions/#4-inputoutput-functions","text":"Purpose : Interact with the user or manipulate files. Function Description Example Result print() Outputs a message to the console. print(\"Hello, World!\") Hello, World! input() Reads a string from user input. input(\"Enter name: \") Returns user input as a string. open() Opens a file and returns a corresponding file object. open('file.txt', 'r') Opens the file for reading.","title":"4. Input/Output Functions"},{"location":"documentation/python/05_built-in_functions/#5-object-manipulation-functions","text":"Purpose : Work with objects and attributes. type() : Returns the type of an object. isinstance() : Checks if an object is an instance of a class or tuple of classes. id() : Returns the unique identifier of an object. dir() : Returns a list of valid attributes for an object. vars() : Returns the __dict__ attribute of an object. hasattr() : Checks if an object has a specified attribute. getattr() : Retrieves the value of a named attribute. setattr() : Sets the value of a named attribute. delattr() : Deletes a named attribute. Function Description Example Result type() Returns the type of an object. type(123) <class 'int'> isinstance() Checks if an object is an instance of a class or tuple of classes. isinstance(123, int) True id() Returns the unique identifier of an object. id(123) Some unique integer value dir() Returns a list of valid attributes for an object. dir([1, 2, 3]) List of attributes for the list object vars() Returns the __dict__ attribute of an object. vars({\"key\": \"value\"}) {'key': 'value'} hasattr() Checks if an object has a specified attribute. hasattr([], 'append') True getattr() Retrieves the value of a named attribute. getattr([], 'append') <built-in method append of list object> setattr() Sets the value of a named attribute. setattr([], 'new_attr', 42) No output, sets attribute new_attr to 42 delattr() Deletes a named attribute. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr","title":"5. Object Manipulation Functions"},{"location":"documentation/python/05_built-in_functions/#6-memory-and-system-manipulation-functions","text":"Purpose : Work with memory and low-level system aspects. Function Description Example Result memoryview() Returns a memory view object. mv = memoryview(b'hello'); mv[0] 104 (the ASCII value of 'h') hash() Returns the hash value of an object. hash(\"hello\") A unique integer value (e.g., -1017891587 ) object() Returns a new featureless object. obj = object() A new object with no specific attributes del() Deletes an object. x = 10; del x No output, deletes variable x delattr() Deletes an attribute from an object. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr","title":"6. Memory and System Manipulation Functions"},{"location":"documentation/python/05_built-in_functions/#7-miscellaneous-functions","text":"Purpose : Functions that don't fit neatly into other categories. Function Description Example Result help() Invokes the built-in help system. help(str) Displays help information about the str class globals() Returns a dictionary representing the current global symbol table. globals() A dictionary of global variables and their values locals() Returns a dictionary representing the current local symbol table. locals() A dictionary of local variables and their values eval() Evaluates a Python expression from a string-based input. eval('2 + 3') 5 exec() Executes Python code dynamically from a string or compiled code. exec('x = 5') No output; creates variable x with value 5 compile() Compiles a source into a code or AST object. code = compile('print(\"Hello\")', '<string>', 'exec'); exec(code) Hello super() Returns a proxy object that delegates method calls to a parent or sibling class. class Parent: pass; class Child(Parent): pass; super(Child, Child()) <super object> staticmethod() Returns a static method for a function. class MyClass: @staticmethod def my_method(): pass No output; creates a static method in MyClass classmethod() Returns a class method for a function. class MyClass: @classmethod def my_method(cls): pass No output; creates a class method in MyClass property() Returns a property attribute for a class. class MyClass: def my_property(self): return 42; my_property = property(my_property) A property that returns 42","title":"7. Miscellaneous Functions"},{"location":"documentation/python/05_built-in_functions/#8-exception-handling-functions","text":"Purpose : Handle exceptions and errors. Function/Class Description Example Result issubclass() Checks if a class is a subclass of another class. issubclass(bool, int) True BaseException The base class for all built-in exceptions. BaseException <class 'BaseException'> Exception The base class for most built-in exceptions. Exception <class 'Exception'>","title":"8. Exception Handling Functions"},{"location":"documentation/python/05_built-in_functions/#9-memory-and-system-manipulation-functions","text":"Purpose : Work with memory and system aspects. Function Description Example Result memoryview() Returns a memory view object. mv = memoryview(b'hello'); mv[0] 104 (the ASCII value of 'h') hash() Returns the hash value of an object. hash(\"hello\") A unique integer value (e.g., -1017891587 ) object() Returns a new featureless object. obj = object() A new object with no specific attributes del() Deletes an object. x = 10; del x No output, deletes variable x delattr() Deletes an attribute from an object. class Test: pass; t = Test(); setattr(t, 'attr', 1); delattr(t, 'attr') No output, deletes attribute attr","title":"9. Memory and System Manipulation Functions"},{"location":"documentation/python/05_built-in_functions/#10-inputoutput-functions","text":"Purpose : Interact with the user or manipulate files. print() : Outputs a message to the console. input() : Reads a string from user input. open() : Opens a file and returns a corresponding file object.","title":"10. Input/Output Functions"},{"location":"documentation/python/05_built-in_functions/#conclusion","text":"Mastering Python's built-in functions is a critical step in becoming a proficient Python developer. These functions provide the building blocks for creating efficient, readable, and maintainable code. By leveraging these functions, developers can save time, reduce errors, and produce higher-quality software.","title":"Conclusion"},{"location":"documentation/python/05_built-in_functions/#author","text":"Miguel Angelo Do Amaral Junior","title":"Author"},{"location":"documentation/python/06_built-in_methods/","text":"6. Python Methods Documentation This documentation covers several essential Python methods that are frequently used in day-to-day programming tasks. Whether you're sorting lists, manipulating strings, or working with dictionaries, these built-in methods are fundamental tools for efficient and effective coding. List Methods Method Description Example Result append() Adds an item to the end of the list. my_list.append(4) [1, 2, 3, 4] extend() Extends list by appending an iterable. my_list.extend([4, 5]) [1, 2, 3, 4, 5] insert() Inserts item at specified position. my_list.insert(1, 'a') [1, 'a', 2, 3] remove() Removes the first occurrence of an item. my_list.remove(2) [1, 3] pop() Removes and returns an item by index. my_list.pop(1) 2 , remaining: [1, 3] clear() Removes all items from the list. my_list.clear() [] index() Returns the index of the first match. my_list.index(3) 2 count() Counts the occurrences of a value. my_list.count(2) 1 sort() Sorts the list in ascending order. my_list.sort() [1, 2, 3] reverse() Reverses the order of the list. my_list.reverse() [3, 2, 1] copy() Returns a shallow copy of the list. new_list = my_list.copy() new_list = [1, 2, 3] String Methods Method Description Example Result upper() Converts all characters to uppercase. \"hello\".upper() \"HELLO\" lower() Converts all characters to lowercase. \"HELLO\".lower() \"hello\" capitalize() Capitalizes the first character. \"hello world\".capitalize() \"Hello world\" title() Capitalizes the first letter of each word. \"hello world\".title() \"Hello World\" strip() Removes leading and trailing spaces. \" hello \".strip() \"hello\" replace() Replaces occurrences of a substring. \"hello\".replace('e', 'a') \"hallo\" find() Finds the index of the first match. \"hello\".find('e') 1 split() Splits string into a list. \"a,b,c\".split(',') ['a', 'b', 'c'] join() Joins iterable into a string. \" \".join(['a', 'b', 'c']) \"a b c\" startswith() Checks if string starts with substring. \"hello\".startswith('h') True endswith() Checks if string ends with substring. \"hello\".endswith('o') True Dictionary Methods Method Description Example Result get() Returns the value for a given key. my_dict.get('age', 'Unknown') 30 or 'Unknown' keys() Returns a view of the dictionary\u2019s keys. my_dict.keys() dict_keys(['name', 'age']) values() Returns a view of the dictionary\u2019s values. my_dict.values() dict_values(['John', 30]) items() Returns a view of the dictionary\u2019s items. my_dict.items() dict_items([('name', 'John')]) update() Updates the dictionary with key-value pairs. my_dict.update({'age': 30}) {'name': 'John', 'age': 30} pop() Removes a key-value pair and returns its value. my_dict.pop('age') 30 , remaining: {'name': 'John'} clear() Removes all key-value pairs from the dictionary. my_dict.clear() {} Set Methods Method Description Example Result add() Adds an element to the set. my_set.add(4) {1, 2, 3, 4} update() Updates the set with multiple elements. my_set.update([5, 6]) {1, 2, 3, 5, 6} remove() Removes a specific element (raises error if not found). my_set.remove(2) {1, 3} discard() Removes a specific element (no error if not found). my_set.discard(4) {1, 2, 3} pop() Removes and returns an arbitrary element. my_set.pop() 1 (for example), remaining: {2, 3} clear() Removes all elements from the set. my_set.clear() set() Other Built-in Methods Method Description Example Result len() Returns the number of items. len([1, 2, 3]) 3 max() Returns the largest item. max([1, 2, 3]) 3 min() Returns the smallest item. min([1, 2, 3]) 1 sum() Returns the sum of items. sum([1, 2, 3]) 6 enumerate() Returns index and value pairs. list(enumerate(['a', 'b', 'c'])) [(0, 'a'), (1, 'b'), (2, 'c')] zip() Aggregates elements from iterables. list(zip([1, 2], ['a', 'b'])) [(1, 'a'), (2, 'b')] range() Generates a sequence of numbers. list(range(3)) [0, 1, 2] Conclusion These are some of the most commonly used Python methods in everyday programming. Having a strong grasp of these functions will greatly improve your efficiency and productivity as a Python programmer. Author Miguel Angelo Do Amaral Junior","title":"06 - Built-in methods"},{"location":"documentation/python/06_built-in_methods/#6-python-methods-documentation","text":"This documentation covers several essential Python methods that are frequently used in day-to-day programming tasks. Whether you're sorting lists, manipulating strings, or working with dictionaries, these built-in methods are fundamental tools for efficient and effective coding.","title":"6. Python Methods Documentation"},{"location":"documentation/python/06_built-in_methods/#list-methods","text":"Method Description Example Result append() Adds an item to the end of the list. my_list.append(4) [1, 2, 3, 4] extend() Extends list by appending an iterable. my_list.extend([4, 5]) [1, 2, 3, 4, 5] insert() Inserts item at specified position. my_list.insert(1, 'a') [1, 'a', 2, 3] remove() Removes the first occurrence of an item. my_list.remove(2) [1, 3] pop() Removes and returns an item by index. my_list.pop(1) 2 , remaining: [1, 3] clear() Removes all items from the list. my_list.clear() [] index() Returns the index of the first match. my_list.index(3) 2 count() Counts the occurrences of a value. my_list.count(2) 1 sort() Sorts the list in ascending order. my_list.sort() [1, 2, 3] reverse() Reverses the order of the list. my_list.reverse() [3, 2, 1] copy() Returns a shallow copy of the list. new_list = my_list.copy() new_list = [1, 2, 3]","title":"List Methods"},{"location":"documentation/python/06_built-in_methods/#string-methods","text":"Method Description Example Result upper() Converts all characters to uppercase. \"hello\".upper() \"HELLO\" lower() Converts all characters to lowercase. \"HELLO\".lower() \"hello\" capitalize() Capitalizes the first character. \"hello world\".capitalize() \"Hello world\" title() Capitalizes the first letter of each word. \"hello world\".title() \"Hello World\" strip() Removes leading and trailing spaces. \" hello \".strip() \"hello\" replace() Replaces occurrences of a substring. \"hello\".replace('e', 'a') \"hallo\" find() Finds the index of the first match. \"hello\".find('e') 1 split() Splits string into a list. \"a,b,c\".split(',') ['a', 'b', 'c'] join() Joins iterable into a string. \" \".join(['a', 'b', 'c']) \"a b c\" startswith() Checks if string starts with substring. \"hello\".startswith('h') True endswith() Checks if string ends with substring. \"hello\".endswith('o') True","title":"String Methods"},{"location":"documentation/python/06_built-in_methods/#dictionary-methods","text":"Method Description Example Result get() Returns the value for a given key. my_dict.get('age', 'Unknown') 30 or 'Unknown' keys() Returns a view of the dictionary\u2019s keys. my_dict.keys() dict_keys(['name', 'age']) values() Returns a view of the dictionary\u2019s values. my_dict.values() dict_values(['John', 30]) items() Returns a view of the dictionary\u2019s items. my_dict.items() dict_items([('name', 'John')]) update() Updates the dictionary with key-value pairs. my_dict.update({'age': 30}) {'name': 'John', 'age': 30} pop() Removes a key-value pair and returns its value. my_dict.pop('age') 30 , remaining: {'name': 'John'} clear() Removes all key-value pairs from the dictionary. my_dict.clear() {}","title":"Dictionary Methods"},{"location":"documentation/python/06_built-in_methods/#set-methods","text":"Method Description Example Result add() Adds an element to the set. my_set.add(4) {1, 2, 3, 4} update() Updates the set with multiple elements. my_set.update([5, 6]) {1, 2, 3, 5, 6} remove() Removes a specific element (raises error if not found). my_set.remove(2) {1, 3} discard() Removes a specific element (no error if not found). my_set.discard(4) {1, 2, 3} pop() Removes and returns an arbitrary element. my_set.pop() 1 (for example), remaining: {2, 3} clear() Removes all elements from the set. my_set.clear() set()","title":"Set Methods"},{"location":"documentation/python/06_built-in_methods/#other-built-in-methods","text":"Method Description Example Result len() Returns the number of items. len([1, 2, 3]) 3 max() Returns the largest item. max([1, 2, 3]) 3 min() Returns the smallest item. min([1, 2, 3]) 1 sum() Returns the sum of items. sum([1, 2, 3]) 6 enumerate() Returns index and value pairs. list(enumerate(['a', 'b', 'c'])) [(0, 'a'), (1, 'b'), (2, 'c')] zip() Aggregates elements from iterables. list(zip([1, 2], ['a', 'b'])) [(1, 'a'), (2, 'b')] range() Generates a sequence of numbers. list(range(3)) [0, 1, 2]","title":"Other Built-in Methods"},{"location":"documentation/python/06_built-in_methods/#conclusion","text":"These are some of the most commonly used Python methods in everyday programming. Having a strong grasp of these functions will greatly improve your efficiency and productivity as a Python programmer.","title":"Conclusion"},{"location":"documentation/python/06_built-in_methods/#author","text":"Miguel Angelo Do Amaral Junior","title":"Author"},{"location":"documentation/sql/01_sql_introduction/","text":"","title":"01 sql introduction"},{"location":"documentation/sql/02_sql_creating_tables/","text":"","title":"02 sql creating tables"},{"location":"documentation/sql/03_sql_queries/","text":"","title":"03 sql queries"},{"location":"documentation/version_control/01_gitflow/","text":"Overview of GitFlow GitFlow is a branching model that aims to streamline collaborative development and continuous delivery. The model introduces multiple branches with specific roles: Feature branches : For developing new features. Develop branch : An integration branch for feature work. Main (or Master) branch : A stable branch that always reflects production-ready code. Using GitFlow, teams can ensure code quality, reduce integration issues, and maintain a clear history of changes. Branching Strategy Feature Branches Feature branches are created to develop new features, bug fixes, or other significant changes. They branch off from the develop branch and are named descriptively to reflect the work being done. For example, a new login feature might be developed on a branch called feature/login-page . Development Branch The develop branch serves as the integration branch for feature branches. It contains the latest, bleeding-edge changes that are being prepared for the next release. It is essential to keep this branch stable to facilitate smooth integration of features. Main Branch The main (or master ) branch represents the stable, production-ready state of the project. It only contains code that has been thoroughly tested and reviewed. Releases are created by merging changes from develop into main . Creating a Feature Branch To create a new feature branch, follow these steps: Ensure Your Local Repository is Up-to-Date: git checkout develop git pull origin develop Create the Feature Branch: git checkout -b feature/your-feature-name Start Development: Begin coding on your feature branch. Commit your changes regularly with clear and concise commit messages. git add . git commit -m \"Added initial implementation of [your feature]\" Push the Feature Branch to Remote: git push origin feature/your-feature-name Merging a Feature Branch into Develop Once your feature is complete, it's time to merge it into the develop branch. Follow these steps: 1. Open a Pull Request Go to your repository on GitHub (or any other Git hosting service) and open a Pull Request (PR) to merge your feature branch into develop . Provide a clear title and description of the changes in the PR. 2. Code Review Process The PR should be reviewed by at least one or more team members. Reviewers should focus on code quality, functionality, and consistency with the project\u2019s coding standards. If there are any issues, feedback should be provided, and necessary changes should be made before approval. 3. Merging to Develop Once the PR is approved, the feature branch can be merged into develop. After merging, delete the feature branch both locally and remotely to keep the repository clean. git branch -d feature/your-feature-name git push origin --delete feature/your-feature-name Merging Develop into Main After a set of features or fixes are integrated into develop , the next step is to prepare for a release by merging into main . 1. Final Testing Ensure that all features on develop have been tested thoroughly. Run integration tests, unit tests, and any other relevant testing processes. 2. Preparing for Release Update the CHANGELOG.md to reflect all changes. Update the version number as needed according to semantic versioning practices. 3. Merging and Tagging Merge the develop branch into main: git checkout main git pull origin main git merge --no-ff develop Push the main branch: git push origin main Conclusion GitFlow provides a structured approach to managing complex development projects. By following these best practices, teams can ensure smooth collaboration, maintain high code quality, and deliver stable releases. Remember to communicate clearly, review code thoroughly, and test rigorously to make the most out of GitFlow. Author Miguel Angelo do Amaral Junior","title":"01 - GitFlow"},{"location":"documentation/version_control/01_gitflow/#overview-of-gitflow","text":"GitFlow is a branching model that aims to streamline collaborative development and continuous delivery. The model introduces multiple branches with specific roles: Feature branches : For developing new features. Develop branch : An integration branch for feature work. Main (or Master) branch : A stable branch that always reflects production-ready code. Using GitFlow, teams can ensure code quality, reduce integration issues, and maintain a clear history of changes.","title":"Overview of GitFlow"},{"location":"documentation/version_control/01_gitflow/#branching-strategy","text":"Feature Branches Feature branches are created to develop new features, bug fixes, or other significant changes. They branch off from the develop branch and are named descriptively to reflect the work being done. For example, a new login feature might be developed on a branch called feature/login-page . Development Branch The develop branch serves as the integration branch for feature branches. It contains the latest, bleeding-edge changes that are being prepared for the next release. It is essential to keep this branch stable to facilitate smooth integration of features. Main Branch The main (or master ) branch represents the stable, production-ready state of the project. It only contains code that has been thoroughly tested and reviewed. Releases are created by merging changes from develop into main .","title":"Branching Strategy"},{"location":"documentation/version_control/01_gitflow/#creating-a-feature-branch","text":"To create a new feature branch, follow these steps: Ensure Your Local Repository is Up-to-Date: git checkout develop git pull origin develop Create the Feature Branch: git checkout -b feature/your-feature-name Start Development: Begin coding on your feature branch. Commit your changes regularly with clear and concise commit messages. git add . git commit -m \"Added initial implementation of [your feature]\" Push the Feature Branch to Remote: git push origin feature/your-feature-name","title":"Creating a Feature Branch"},{"location":"documentation/version_control/01_gitflow/#merging-a-feature-branch-into-develop","text":"Once your feature is complete, it's time to merge it into the develop branch. Follow these steps:","title":"Merging a Feature Branch into Develop"},{"location":"documentation/version_control/01_gitflow/#1-open-a-pull-request","text":"Go to your repository on GitHub (or any other Git hosting service) and open a Pull Request (PR) to merge your feature branch into develop . Provide a clear title and description of the changes in the PR.","title":"1. Open a Pull Request"},{"location":"documentation/version_control/01_gitflow/#2-code-review-process","text":"The PR should be reviewed by at least one or more team members. Reviewers should focus on code quality, functionality, and consistency with the project\u2019s coding standards. If there are any issues, feedback should be provided, and necessary changes should be made before approval.","title":"2. Code Review Process"},{"location":"documentation/version_control/01_gitflow/#3-merging-to-develop","text":"Once the PR is approved, the feature branch can be merged into develop. After merging, delete the feature branch both locally and remotely to keep the repository clean. git branch -d feature/your-feature-name git push origin --delete feature/your-feature-name","title":"3. Merging to Develop"},{"location":"documentation/version_control/01_gitflow/#merging-develop-into-main","text":"After a set of features or fixes are integrated into develop , the next step is to prepare for a release by merging into main .","title":"Merging Develop into Main"},{"location":"documentation/version_control/01_gitflow/#1-final-testing","text":"Ensure that all features on develop have been tested thoroughly. Run integration tests, unit tests, and any other relevant testing processes.","title":"1. Final Testing"},{"location":"documentation/version_control/01_gitflow/#2-preparing-for-release","text":"Update the CHANGELOG.md to reflect all changes. Update the version number as needed according to semantic versioning practices.","title":"2. Preparing for Release"},{"location":"documentation/version_control/01_gitflow/#3-merging-and-tagging","text":"Merge the develop branch into main: git checkout main git pull origin main git merge --no-ff develop Push the main branch: git push origin main","title":"3. Merging and Tagging"},{"location":"documentation/version_control/01_gitflow/#conclusion","text":"GitFlow provides a structured approach to managing complex development projects. By following these best practices, teams can ensure smooth collaboration, maintain high code quality, and deliver stable releases. Remember to communicate clearly, review code thoroughly, and test rigorously to make the most out of GitFlow.","title":"Conclusion"},{"location":"documentation/version_control/01_gitflow/#author","text":"Miguel Angelo do Amaral Junior","title":"Author"},{"location":"documentation/version_control/02_commands/","text":"Git Commands Documentation This guide covers the most commonly used Git commands for managing and collaborating on repositories. These commands are essential for day-to-day work with Git, including staging, committing, and synchronizing code with remote repositories. 1. git init Initializes a new Git repository in the current directory. This command creates a hidden .git folder, setting up everything needed to start version controlling your files. git init 2. git clone <repository-url> Copies (clones) an existing Git repository from a remote source (e.g., GitHub, GitLab) to your local machine. git clone <repository-url> 3. git status Shows the current status of your repository. It displays untracked, modified, and staged files, helping you understand what changes need to be staged or committed. git status 4. git add <file> Stages changes in a specific file to be committed. You can stage all files with . instead of a specific filename. git add <file> To stage all files at once: git add . 5. git commit -m \"commit message\" Commits the staged changes with a descriptive message. Each commit creates a new entry in the repository\u2019s history, representing a snapshot of your project at that time. git commit -m \"Add a descriptive message\" 6. git push origin <branch> Uploads your local commits to a remote repository, making them available to others. Specify the branch you want to push to, typically main or master . git push origin <branch> Example: git push origin main 7. git pull origin <branch> Fetches and merges changes from the specified branch on a remote repository into your local branch. This command helps you synchronize your work with updates made by others. git pull origin <branch> 8. git branch Lists all local branches in your repository. You can also use it to create a new branch or delete an existing branch. To list all branches: git branch To create a new branch: git branch <branch-name> To delete a branch: git branch -d <branch-name> 9. git checkout <branch> Switches to the specified branch. This command changes the working directory to reflect the state of the chosen branch. git checkout <branch> To create and switch to a new branch in one command: git checkout -b <new-branch-name> 10. git merge <branch> Merges the specified branch into the current branch. This command incorporates changes from another branch, allowing you to combine development efforts. git merge <branch> 11. git log Displays a log of all commits made in the repository, including commit messages, authors, and dates. This is useful for reviewing the project\u2019s history. git log 12. git diff Shows differences between the working directory, staged changes, and the latest commit. This command is helpful to see exactly what modifications have been made. git diff To compare staged changes with the latest commit: git diff --staged 13. git reset Unstages changes that have been added to the staging area. This is useful if you accidentally staged files and need to remove them from the staging area. To unstage a specific file: git reset <file> To reset all staged files: git reset 14. git rm <file> Deletes a file from the working directory and stages the removal for the next commit. git rm <file> To remove a file only from the staging area but keep it in the working directory: git rm --cached <file> Tips and Best Practices Write Clear Commit Messages: Commit messages should describe the changes made. A good format is: \"Fix bug in X module\" or \"Add new feature Y\". Commit Often: Frequent commits make it easier to understand changes and revert them if needed. Keep Your Repository Updated: Use git pull regularly to stay in sync with the main branch of your repository. Use Branches for New Features or Fixes: Isolate new features or bug fixes in separate branches, then merge them when ready.","title":"02 - Day-to-day commands"},{"location":"documentation/version_control/02_commands/#git-commands-documentation","text":"This guide covers the most commonly used Git commands for managing and collaborating on repositories. These commands are essential for day-to-day work with Git, including staging, committing, and synchronizing code with remote repositories.","title":"Git Commands Documentation"},{"location":"documentation/version_control/02_commands/#1-git-init","text":"Initializes a new Git repository in the current directory. This command creates a hidden .git folder, setting up everything needed to start version controlling your files. git init","title":"1. git init"},{"location":"documentation/version_control/02_commands/#2-git-clone-repository-url","text":"Copies (clones) an existing Git repository from a remote source (e.g., GitHub, GitLab) to your local machine. git clone <repository-url>","title":"2. git clone &lt;repository-url&gt;"},{"location":"documentation/version_control/02_commands/#3-git-status","text":"Shows the current status of your repository. It displays untracked, modified, and staged files, helping you understand what changes need to be staged or committed. git status","title":"3. git status"},{"location":"documentation/version_control/02_commands/#4-git-add-file","text":"Stages changes in a specific file to be committed. You can stage all files with . instead of a specific filename. git add <file> To stage all files at once: git add .","title":"4. git add &lt;file&gt;"},{"location":"documentation/version_control/02_commands/#5-git-commit-m-commit-message","text":"Commits the staged changes with a descriptive message. Each commit creates a new entry in the repository\u2019s history, representing a snapshot of your project at that time. git commit -m \"Add a descriptive message\"","title":"5. git commit -m \"commit message\""},{"location":"documentation/version_control/02_commands/#6-git-push-origin-branch","text":"Uploads your local commits to a remote repository, making them available to others. Specify the branch you want to push to, typically main or master . git push origin <branch> Example: git push origin main","title":"6. git push origin &lt;branch&gt;"},{"location":"documentation/version_control/02_commands/#7-git-pull-origin-branch","text":"Fetches and merges changes from the specified branch on a remote repository into your local branch. This command helps you synchronize your work with updates made by others. git pull origin <branch>","title":"7. git pull origin &lt;branch&gt;"},{"location":"documentation/version_control/02_commands/#8-git-branch","text":"Lists all local branches in your repository. You can also use it to create a new branch or delete an existing branch. To list all branches: git branch To create a new branch: git branch <branch-name> To delete a branch: git branch -d <branch-name>","title":"8. git branch"},{"location":"documentation/version_control/02_commands/#9-git-checkout-branch","text":"Switches to the specified branch. This command changes the working directory to reflect the state of the chosen branch. git checkout <branch> To create and switch to a new branch in one command: git checkout -b <new-branch-name>","title":"9. git checkout &lt;branch&gt;"},{"location":"documentation/version_control/02_commands/#10-git-merge-branch","text":"Merges the specified branch into the current branch. This command incorporates changes from another branch, allowing you to combine development efforts. git merge <branch>","title":"10. git merge &lt;branch&gt;"},{"location":"documentation/version_control/02_commands/#11-git-log","text":"Displays a log of all commits made in the repository, including commit messages, authors, and dates. This is useful for reviewing the project\u2019s history. git log","title":"11. git log"},{"location":"documentation/version_control/02_commands/#12-git-diff","text":"Shows differences between the working directory, staged changes, and the latest commit. This command is helpful to see exactly what modifications have been made. git diff To compare staged changes with the latest commit: git diff --staged","title":"12. git diff"},{"location":"documentation/version_control/02_commands/#13-git-reset","text":"Unstages changes that have been added to the staging area. This is useful if you accidentally staged files and need to remove them from the staging area. To unstage a specific file: git reset <file> To reset all staged files: git reset","title":"13. git reset"},{"location":"documentation/version_control/02_commands/#14-git-rm-file","text":"Deletes a file from the working directory and stages the removal for the next commit. git rm <file> To remove a file only from the staging area but keep it in the working directory: git rm --cached <file>","title":"14. git rm &lt;file&gt;"},{"location":"documentation/version_control/02_commands/#tips-and-best-practices","text":"Write Clear Commit Messages: Commit messages should describe the changes made. A good format is: \"Fix bug in X module\" or \"Add new feature Y\". Commit Often: Frequent commits make it easier to understand changes and revert them if needed. Keep Your Repository Updated: Use git pull regularly to stay in sync with the main branch of your repository. Use Branches for New Features or Fixes: Isolate new features or bug fixes in separate branches, then merge them when ready.","title":"Tips and Best Practices"},{"location":"projects/airflow_guide/","text":"Airflow Guide This guide helps set up Apache Airflow using Virtualenv, installing dependencies, and creating the necessary environment variables for the project. It explains the key components of Airflow, such as DAGs (workflows), Tasks (units of work), and Operators (task logic). It shows how to start Airflow locally, including the database, web server, and scheduler for study purposes. Additionally, the guide highlights best practices for modifying configuration variables using env vars instead of directly editing the airflow.cfg file. The basic project structure and example DAGs are also provided.","title":"Airflow Guide"},{"location":"projects/airflow_guide/#airflow-guide","text":"This guide helps set up Apache Airflow using Virtualenv, installing dependencies, and creating the necessary environment variables for the project. It explains the key components of Airflow, such as DAGs (workflows), Tasks (units of work), and Operators (task logic). It shows how to start Airflow locally, including the database, web server, and scheduler for study purposes. Additionally, the guide highlights best practices for modifying configuration variables using env vars instead of directly editing the airflow.cfg file. The basic project structure and example DAGs are also provided.","title":"Airflow Guide"},{"location":"projects/amaralapps_webpage/","text":"AmaralApps Webpage The AmaralApps Webpage is a fully deployed web application built using Flask, designed to showcase the various projects and technologies used by AmaralApps. The platform provides an overview of the company\u2019s services, technical expertise, and portfolio, serving as the main online presence for the business. Key Features and Technologies: Flask Framework : The web application is built using Flask, a lightweight and powerful Python web framework, which allows for quick and efficient development. Docker : The entire project is containerized using Docker, ensuring consistency across different environments and making it easy to deploy and scale. Deployment on AWS EC2 : The website is deployed on an AWS EC2 instance, providing a robust and scalable cloud infrastructure. Nginx and Gunicorn : The application is served using Nginx as a reverse proxy and Gunicorn as the WSGI server. This setup allows for both HTTP and HTTPS protocols, ensuring secure communication through SSL/TLS certificates. HTTPS Configuration : Nginx is configured to handle both HTTP and HTTPS traffic, with secure SSL certificates providing encrypted connections for user security and trust. The AmaralApps Webpage showcases various projects developed by the company, as well as detailed descriptions of the technologies used, making it a comprehensive and professional platform for the company's online presence.","title":"AmaralApps Webpage"},{"location":"projects/amaralapps_webpage/#amaralapps-webpage","text":"The AmaralApps Webpage is a fully deployed web application built using Flask, designed to showcase the various projects and technologies used by AmaralApps. The platform provides an overview of the company\u2019s services, technical expertise, and portfolio, serving as the main online presence for the business.","title":"AmaralApps Webpage"},{"location":"projects/amaralapps_webpage/#key-features-and-technologies","text":"Flask Framework : The web application is built using Flask, a lightweight and powerful Python web framework, which allows for quick and efficient development. Docker : The entire project is containerized using Docker, ensuring consistency across different environments and making it easy to deploy and scale. Deployment on AWS EC2 : The website is deployed on an AWS EC2 instance, providing a robust and scalable cloud infrastructure. Nginx and Gunicorn : The application is served using Nginx as a reverse proxy and Gunicorn as the WSGI server. This setup allows for both HTTP and HTTPS protocols, ensuring secure communication through SSL/TLS certificates. HTTPS Configuration : Nginx is configured to handle both HTTP and HTTPS traffic, with secure SSL certificates providing encrypted connections for user security and trust. The AmaralApps Webpage showcases various projects developed by the company, as well as detailed descriptions of the technologies used, making it a comprehensive and professional platform for the company's online presence.","title":"Key Features and Technologies:"},{"location":"projects/codename/","text":"Codename Considerations What is a codename? A codename is a temporary name given to a new drug while it\u2019s still in development . It acts as a provisional label until the drug receives an official name . What is the purpose of a codename? It helps track the drug throughout its development process . Why are there multiple synonyms? Different researchers may refer to the same drug using various names during the research phase. Why are scientists interested in this? Codenames make it easier to organize and track the progress of the drug in development. They facilitate communication between researchers and help ensure intellectual property protection during the research process. What were the challenges encountered in this project? The main challenge of this project was scheduling execution pipelines , organizing and tracking large volumes of scientific articles , and extracting codenames using automated pipelines. Another challenge was unifying synonyms and distinguishing information as False Positive and True Positive to ensure precision in codename extraction . Finally, developing a data visualization platform was also challenging, as it needed to display relevant information for scientists, such as paper ID , publication year , and other essential details about the codename. Technologies and Tools Used This project was implemented using several technologies and tools: Python : Used as the main programming language for data processing and analysis. MongoDB : Employed as the primary database to store and manage large volumes of documents. Docker & Rancher : Used for scheduling cron jobs to keep the database updated and synchronized. PubChem API : Utilized for validating codenames to enhance data accuracy. Cell Lines and Gene Names Database (blacklist) : This database, containing about 20,000 entries, helped filter and refine the pipeline, especially in identifying true and false positives. Data Flow and Automation Pipeline The pipeline was structured to handle data ingestion, processing, verification, and visualization: Data Ingestion : Data from various sources, primarily scientific articles, was pulled into MongoDB for storage and processing. Processing : Utilizing a list of cell lines and gene names to refine data by filtering out unrelated content and identifying relevant entries. Verification : The PubChem API was used to validate codenames, improving the accuracy and reliability of extracted data. Visualization : A data visualization platform was developed to display essential information, including paper ID, publication year, and other relevant codename details, allowing scientists to easily access and understand the data. Conclusion This project aimed to create an efficient and accurate system for managing codenames used in drug development, ensuring consistent tracking and reliable access to important data for researchers. By addressing the challenges related to pipeline scheduling, data accuracy, and information organization, this project facilitates the efficient management of drug development information.","title":"Codename"},{"location":"projects/codename/#codename","text":"","title":"Codename"},{"location":"projects/codename/#considerations","text":"What is a codename? A codename is a temporary name given to a new drug while it\u2019s still in development . It acts as a provisional label until the drug receives an official name . What is the purpose of a codename? It helps track the drug throughout its development process . Why are there multiple synonyms? Different researchers may refer to the same drug using various names during the research phase. Why are scientists interested in this? Codenames make it easier to organize and track the progress of the drug in development. They facilitate communication between researchers and help ensure intellectual property protection during the research process. What were the challenges encountered in this project? The main challenge of this project was scheduling execution pipelines , organizing and tracking large volumes of scientific articles , and extracting codenames using automated pipelines. Another challenge was unifying synonyms and distinguishing information as False Positive and True Positive to ensure precision in codename extraction . Finally, developing a data visualization platform was also challenging, as it needed to display relevant information for scientists, such as paper ID , publication year , and other essential details about the codename.","title":"Considerations"},{"location":"projects/codename/#technologies-and-tools-used","text":"This project was implemented using several technologies and tools: Python : Used as the main programming language for data processing and analysis. MongoDB : Employed as the primary database to store and manage large volumes of documents. Docker & Rancher : Used for scheduling cron jobs to keep the database updated and synchronized. PubChem API : Utilized for validating codenames to enhance data accuracy. Cell Lines and Gene Names Database (blacklist) : This database, containing about 20,000 entries, helped filter and refine the pipeline, especially in identifying true and false positives.","title":"Technologies and Tools Used"},{"location":"projects/codename/#data-flow-and-automation-pipeline","text":"The pipeline was structured to handle data ingestion, processing, verification, and visualization: Data Ingestion : Data from various sources, primarily scientific articles, was pulled into MongoDB for storage and processing. Processing : Utilizing a list of cell lines and gene names to refine data by filtering out unrelated content and identifying relevant entries. Verification : The PubChem API was used to validate codenames, improving the accuracy and reliability of extracted data. Visualization : A data visualization platform was developed to display essential information, including paper ID, publication year, and other relevant codename details, allowing scientists to easily access and understand the data.","title":"Data Flow and Automation Pipeline"},{"location":"projects/codename/#conclusion","text":"This project aimed to create an efficient and accurate system for managing codenames used in drug development, ensuring consistent tracking and reliable access to important data for researchers. By addressing the challenges related to pipeline scheduling, data accuracy, and information organization, this project facilitates the efficient management of drug development information.","title":"Conclusion"},{"location":"projects/covid_dashboard/","text":"COVID-19 Data Dashboard The COVID-19 Data Dashboard is an interactive Python application developed with Streamlit for real-time visualization and analysis of COVID-19 data. Users can select specific countries like Germany, Brazil, Italy, the United Kingdom, the United States, and China, view interactive charts, and access detailed statistics on cases and deaths. Key features include country selection, various data visualization options with line, scatter, and bar charts, and summary statistics such as total cases, deaths, mortality rate, and averages for new cases and deaths. The dashboard also offers a date range control for flexible data analysis over different periods, with dynamic updates based on user inputs. The project utilizes Python 3.11 , Streamlit for the user interface, Pandas for data manipulation, Plotly for creating interactive charts, and Requests to fetch real-time data. It is easy to install and run with simple steps provided in the project documentation, making it a practical tool for understanding the evolution of COVID-19 across different countries and timelines.","title":"COVID-19 Data Dashboard"},{"location":"projects/covid_dashboard/#covid-19-data-dashboard","text":"The COVID-19 Data Dashboard is an interactive Python application developed with Streamlit for real-time visualization and analysis of COVID-19 data. Users can select specific countries like Germany, Brazil, Italy, the United Kingdom, the United States, and China, view interactive charts, and access detailed statistics on cases and deaths. Key features include country selection, various data visualization options with line, scatter, and bar charts, and summary statistics such as total cases, deaths, mortality rate, and averages for new cases and deaths. The dashboard also offers a date range control for flexible data analysis over different periods, with dynamic updates based on user inputs. The project utilizes Python 3.11 , Streamlit for the user interface, Pandas for data manipulation, Plotly for creating interactive charts, and Requests to fetch real-time data. It is easy to install and run with simple steps provided in the project documentation, making it a practical tool for understanding the evolution of COVID-19 across different countries and timelines.","title":"COVID-19 Data Dashboard"},{"location":"projects/data_queue_keeper/","text":"Data Queue Keeper Data Queue Keeper is a project that integrates messaging services with data storage using Docker Compose. It consists of three main components: RabbitMQ : Manages message queues. MongoDB : Provides persistent data storage. Python Application : Consumes messages from the RabbitMQ queue and stores the data in MongoDB. Importance of the Data Queue Keeper Project The Data Queue Keeper project demonstrates a scalable, reliable system that integrates RabbitMQ and MongoDB for efficient data processing. It highlights modern practices like containerization with Docker Compose, making the project easy to deploy and maintain. Additionally, it serves as a valuable educational tool for developers, showcasing how to build systems that ensure data integrity and handle increased load effectively. Project Structure The project is organized as follows: . \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 consumers/ \u2502 \u2502 \u251c\u2500\u2500 class_basic.py # Base class for data consumers \u2502 \u2502 \u251c\u2500\u2500 class_manager.py # Manager class for consuming messages from RabbitMQ \u2502 \u2502 \u2514\u2500\u2500 class_mongo.py # MongoDBClient class for connecting to MongoDB \u2502 \u251c\u2500\u2500 manager.py # Starts the Manager \u2502 \u251c\u2500\u2500 saver.py # Starts the Saver \u2502 \u2514\u2500\u2500 settings.py # Application settings and environment variables \u251c\u2500\u2500 config.yml # Configuration for RabbitMQ, MongoDB, and logger \u251c\u2500\u2500 docker-compose.yml # Docker Compose configuration for containers \u251c\u2500\u2500 Dockerfile_python_manager # Docker environment for the Python Manager application \u251c\u2500\u2500 Dockerfile_python_saver # Docker environment for the Python Saver application \u251c\u2500\u2500 README.md # Project documentation \u251c\u2500\u2500 requirements.txt # Python dependencies \u2514\u2500\u2500 sender.py # Script for sending messages to RabbitMQ for testing How to Run 1. Clone the repository: git clone git@github.com:miguelzeph/data_queue_keeper.git cd data-queue-keeper 2. Set up the Python environment: Create and activate a virtual environment (optional but recommended): virtualenv --python=<your_python_path> <your_env_name> source ./<your_env_name>/bin/activate # On Windows, use `<your_env_name>\\Scripts\\activate` Install the dependencies: pip install -r requirements.txt 3. Start the containers with Docker Compose: docker-compose up --build 4. Send messages to RabbitMQ: Run the sender.py script to send JSON messages to the RabbitMQ queue: python sender.py 5. Verify data insertion in MongoDB: Use the MongoDB CLI or a client to check the stored data. Additional Resources RabbitMQ Management Interface: Accessible at http://localhost:15672 using user/password . Resilience Testing: Stop the python_manager container to see how RabbitMQ holds messages until the consumer resumes.","title":"Data Queue Keeper"},{"location":"projects/data_queue_keeper/#data-queue-keeper","text":"Data Queue Keeper is a project that integrates messaging services with data storage using Docker Compose. It consists of three main components: RabbitMQ : Manages message queues. MongoDB : Provides persistent data storage. Python Application : Consumes messages from the RabbitMQ queue and stores the data in MongoDB.","title":"Data Queue Keeper"},{"location":"projects/data_queue_keeper/#importance-of-the-data-queue-keeper-project","text":"The Data Queue Keeper project demonstrates a scalable, reliable system that integrates RabbitMQ and MongoDB for efficient data processing. It highlights modern practices like containerization with Docker Compose, making the project easy to deploy and maintain. Additionally, it serves as a valuable educational tool for developers, showcasing how to build systems that ensure data integrity and handle increased load effectively.","title":"Importance of the Data Queue Keeper Project"},{"location":"projects/data_queue_keeper/#project-structure","text":"The project is organized as follows: . \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 consumers/ \u2502 \u2502 \u251c\u2500\u2500 class_basic.py # Base class for data consumers \u2502 \u2502 \u251c\u2500\u2500 class_manager.py # Manager class for consuming messages from RabbitMQ \u2502 \u2502 \u2514\u2500\u2500 class_mongo.py # MongoDBClient class for connecting to MongoDB \u2502 \u251c\u2500\u2500 manager.py # Starts the Manager \u2502 \u251c\u2500\u2500 saver.py # Starts the Saver \u2502 \u2514\u2500\u2500 settings.py # Application settings and environment variables \u251c\u2500\u2500 config.yml # Configuration for RabbitMQ, MongoDB, and logger \u251c\u2500\u2500 docker-compose.yml # Docker Compose configuration for containers \u251c\u2500\u2500 Dockerfile_python_manager # Docker environment for the Python Manager application \u251c\u2500\u2500 Dockerfile_python_saver # Docker environment for the Python Saver application \u251c\u2500\u2500 README.md # Project documentation \u251c\u2500\u2500 requirements.txt # Python dependencies \u2514\u2500\u2500 sender.py # Script for sending messages to RabbitMQ for testing","title":"Project Structure"},{"location":"projects/data_queue_keeper/#how-to-run","text":"1. Clone the repository: git clone git@github.com:miguelzeph/data_queue_keeper.git cd data-queue-keeper 2. Set up the Python environment: Create and activate a virtual environment (optional but recommended): virtualenv --python=<your_python_path> <your_env_name> source ./<your_env_name>/bin/activate # On Windows, use `<your_env_name>\\Scripts\\activate` Install the dependencies: pip install -r requirements.txt 3. Start the containers with Docker Compose: docker-compose up --build 4. Send messages to RabbitMQ: Run the sender.py script to send JSON messages to the RabbitMQ queue: python sender.py 5. Verify data insertion in MongoDB: Use the MongoDB CLI or a client to check the stored data.","title":"How to Run"},{"location":"projects/data_queue_keeper/#additional-resources","text":"RabbitMQ Management Interface: Accessible at http://localhost:15672 using user/password . Resilience Testing: Stop the python_manager container to see how RabbitMQ holds messages until the consumer resumes.","title":"Additional Resources"},{"location":"projects/document_library/","text":"Document Library Microservice Architecture (Simplified with RabbitMQ) Overview The Document Library is a microservice architecture that utilizes Python and RabbitMQ to ingest, process, and analyze documents from various sources and formats. RabbitMQ handles task distribution across microservices, removing the need for an additional asynchronous task system. This project includes components for monitoring and data storage, providing a scalable and efficient solution. Project Objectives Process documents in multiple formats, such as PDFs and spreadsheets. Perform information extraction using Natural Language Processing (NLP). Use RabbitMQ to manage communication and task orchestration between services. Monitor and log system events using Elasticsearch and Logstash. System Architecture The architecture consists of Python services that use the pika library for direct integration with RabbitMQ. This allows services to communicate via queues, with each service listening for and processing messages relevant to its specific function. Data storage and log analysis are handled by MongoDB and Elasticsearch, respectively. Data Flow Diagram +--------------------+ | User Interface | | (API) | +--------------------+ | v +--------------------+ | Dashboard | | (FastAPI) | +--------------------+ | v +--------------------+ | RabbitMQ | | (Message Queue) | +--------------------+ | v +---------|----------+ | | | | | | v v v +----------------+ +----------------+ +----------------+ | Spreadsheet | | Raw Text | | Unarchiver | | Converter | | Converter | | Service | | (Python, pika) | | (Python, pika) | | (Python, pika) | +----------------+ +----------------+ +----------------+ | v +--------------------+ | NER Processor | | (Python, pika) | +--------------------+ | v +--------------------+ | MongoDB | | (Data Storage) | +--------------------+ | v +--------------------+ | Logstash & | | Elasticsearch | | (Logs & Monitoring)| +--------------------+ System Components 1. User Interface (API): Receives documents uploaded by users for processing via HTTP. Technology: FastAPI Endpoints: POST /ingest : Initiates the document ingestion process. 2. RabbitMQ: Manages communication between microservices, distributing tasks and ensuring correct processing order. Queues: prefetch_queue : Stores documents needing metadata and initial preparation. converter_queue : Handles spreadsheets and files needing conversion. ner_queue : For documents requiring named entity recognition. 3. Spreadsheet Converter (Python, pika): Converts spreadsheets into tabular format. Listens to the converter_queue for documents and sends results to MongoDB. 4. Raw Text Converter (Python, pika): Extracts text from PDFs. Processes documents from the converter_queue and stores raw text. 5. Unarchiver (Python, pika): Unzips files and enqueues internal files for subsequent processing. Listens to the converter_queue. 6. NER Processor (Python, pika): Performs named entity recognition (NER) on text documents. Receives documents from the ner_queue and stores results in MongoDB. 7. MongoDB: Stores all document metadata and NER analysis results. Purpose: Provides persistence and facilitates retrieval of processed data. 8. Logstash & Elasticsearch: Aggregates logs and events from all services for storage and real-time analysis. Purpose: Supports continuous monitoring and log analysis of the system. Environment Setup Requirements Python 3.8+ RabbitMQ MongoDB Elasticsearch Docker (optional, to simplify deployment) Conclusion This simplified architecture, with RabbitMQ at the core of orchestration, provides a robust solution for document processing. By eliminating additional task management layers, the system is more direct and efficient, while retaining scalability and flexibility for the integration of new microservices as needed.","title":"Doc Lib"},{"location":"projects/document_library/#document-library-microservice-architecture-simplified-with-rabbitmq","text":"","title":"Document Library Microservice Architecture (Simplified with RabbitMQ)"},{"location":"projects/document_library/#overview","text":"The Document Library is a microservice architecture that utilizes Python and RabbitMQ to ingest, process, and analyze documents from various sources and formats. RabbitMQ handles task distribution across microservices, removing the need for an additional asynchronous task system. This project includes components for monitoring and data storage, providing a scalable and efficient solution.","title":"Overview"},{"location":"projects/document_library/#project-objectives","text":"Process documents in multiple formats, such as PDFs and spreadsheets. Perform information extraction using Natural Language Processing (NLP). Use RabbitMQ to manage communication and task orchestration between services. Monitor and log system events using Elasticsearch and Logstash.","title":"Project Objectives"},{"location":"projects/document_library/#system-architecture","text":"The architecture consists of Python services that use the pika library for direct integration with RabbitMQ. This allows services to communicate via queues, with each service listening for and processing messages relevant to its specific function. Data storage and log analysis are handled by MongoDB and Elasticsearch, respectively.","title":"System Architecture"},{"location":"projects/document_library/#data-flow-diagram","text":"+--------------------+ | User Interface | | (API) | +--------------------+ | v +--------------------+ | Dashboard | | (FastAPI) | +--------------------+ | v +--------------------+ | RabbitMQ | | (Message Queue) | +--------------------+ | v +---------|----------+ | | | | | | v v v +----------------+ +----------------+ +----------------+ | Spreadsheet | | Raw Text | | Unarchiver | | Converter | | Converter | | Service | | (Python, pika) | | (Python, pika) | | (Python, pika) | +----------------+ +----------------+ +----------------+ | v +--------------------+ | NER Processor | | (Python, pika) | +--------------------+ | v +--------------------+ | MongoDB | | (Data Storage) | +--------------------+ | v +--------------------+ | Logstash & | | Elasticsearch | | (Logs & Monitoring)| +--------------------+","title":"Data Flow Diagram"},{"location":"projects/document_library/#system-components","text":"","title":"System Components"},{"location":"projects/document_library/#1-user-interface-api","text":"Receives documents uploaded by users for processing via HTTP. Technology: FastAPI Endpoints: POST /ingest : Initiates the document ingestion process.","title":"1. User Interface (API):"},{"location":"projects/document_library/#2-rabbitmq","text":"Manages communication between microservices, distributing tasks and ensuring correct processing order. Queues: prefetch_queue : Stores documents needing metadata and initial preparation. converter_queue : Handles spreadsheets and files needing conversion. ner_queue : For documents requiring named entity recognition.","title":"2. RabbitMQ:"},{"location":"projects/document_library/#3-spreadsheet-converter-python-pika","text":"Converts spreadsheets into tabular format. Listens to the converter_queue for documents and sends results to MongoDB.","title":"3. Spreadsheet Converter (Python, pika):"},{"location":"projects/document_library/#4-raw-text-converter-python-pika","text":"Extracts text from PDFs. Processes documents from the converter_queue and stores raw text.","title":"4. Raw Text Converter (Python, pika):"},{"location":"projects/document_library/#5-unarchiver-python-pika","text":"Unzips files and enqueues internal files for subsequent processing. Listens to the converter_queue.","title":"5. Unarchiver (Python, pika):"},{"location":"projects/document_library/#6-ner-processor-python-pika","text":"Performs named entity recognition (NER) on text documents. Receives documents from the ner_queue and stores results in MongoDB.","title":"6. NER Processor (Python, pika):"},{"location":"projects/document_library/#7-mongodb","text":"Stores all document metadata and NER analysis results. Purpose: Provides persistence and facilitates retrieval of processed data.","title":"7. MongoDB:"},{"location":"projects/document_library/#8-logstash-elasticsearch","text":"Aggregates logs and events from all services for storage and real-time analysis. Purpose: Supports continuous monitoring and log analysis of the system.","title":"8. Logstash &amp; Elasticsearch:"},{"location":"projects/document_library/#environment-setup","text":"Requirements Python 3.8+ RabbitMQ MongoDB Elasticsearch Docker (optional, to simplify deployment)","title":"Environment Setup"},{"location":"projects/document_library/#conclusion","text":"This simplified architecture, with RabbitMQ at the core of orchestration, provides a robust solution for document processing. By eliminating additional task management layers, the system is more direct and efficient, while retaining scalability and flexibility for the integration of new microservices as needed.","title":"Conclusion"},{"location":"projects/end-to-end-pipeline-etl/","text":"Documenta\u00e7\u00e3o do Projeto: Sistema de Processamento de Dados em Streaming com Apache Kafka, Apache Spark e MongoDB 1. Vis\u00e3o Geral do Projeto Objetivo : Criar um sistema de processamento de dados em streaming que coleta dados de uma API, processa e analisa esses dados em tempo real e os armazena em um banco de dados distribu\u00eddo. 2. Arquitetura do Sistema Explique a arquitetura do sistema conforme ilustrado no diagrama: API : Ponto de entrada dos dados. Pode ser configurada para consumir dados de uma API RESTful. Apache Airflow : Utilizado para orquestrar e gerenciar(agendar) o pipeline ETL. PostgreSQL : Banco de dados relacional para armazenar metadados providos do Airflow . Apache Kafka : Middleware de streaming para transmiss\u00e3o de dados em tempo real. Apache Zookeeper : Coordena o Kafka, gerenciando os brokers e t\u00f3picos. Control Center e Schema Registry : Monitoramento e gerenciamento de schemas de dados. Apache Spark : Plataforma de processamento distribu\u00eddo usada para processar os dados em tempo real. MongoDB : Banco de dados NoSQL para armazenamento de dados n\u00e3o estruturados com alta flexibilidade e escalabilidade. Docker : Ambientes de cont\u00eainer para cada servi\u00e7o, facilitando o deployment e a manuten\u00e7\u00e3o. 3. Configura\u00e7\u00e3o do Ambiente Pr\u00e9-requisitos : - Instala\u00e7\u00e3o do Docker e Docker Compose - Python 3.8 ou superior - Instala\u00e7\u00e3o dos pacotes necess\u00e1rios: - kafka-python para Kafka - pymongo para MongoDB - pyspark para Spark - psycopg2 para PostgreSQL - apache-airflow para Airflow Configura\u00e7\u00e3o do Docker : - Cria\u00e7\u00e3o de arquivos Dockerfile para cada servi\u00e7o, se necess\u00e1rio. - Configura\u00e7\u00e3o de um docker-compose.yml para orquestrar todos os servi\u00e7os. - Instru\u00e7\u00f5es para executar docker-compose up para iniciar todos os cont\u00eaineres. 4. Estrutura do C\u00f3digo Diret\u00f3rios e arquivos principais : - /src : Cont\u00e9m o c\u00f3digo-fonte principal. - /src/api : C\u00f3digo para consumir dados da API. - /src/airflow : Pipelines e DAGs do Apache Airflow. - /src/kafka : Configura\u00e7\u00f5es e scripts para o Kafka. - /src/spark : Scripts para processamento de dados no Apache Spark. - /src/mongodb : Scripts para manipula\u00e7\u00e3o de dados no MongoDB. - /docker : Configura\u00e7\u00f5es e scripts Docker. - requirements.txt : Depend\u00eancias do projeto. 5. Componentes Principais e Implementa\u00e7\u00e3o 1. Coleta de Dados (API) : - Iremos usar a API CoinGecko (para dados de criptomoedas) - Descri\u00e7\u00e3o: Cria\u00e7\u00e3o de um script Python que coleta dados de uma API externa. - Exemplo de C\u00f3digo: ```python import requests def get_data(): response = requests.get(\"URL_DA_API\") data = response.json() # Processamento inicial dos dados return data ``` 2. Orquestra\u00e7\u00e3o com Apache Airflow : - Descri\u00e7\u00e3o: Configura\u00e7\u00e3o de DAGs no Airflow para agendar e gerenciar o pipeline de coleta e envio de dados. - Exemplo de DAG: ```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime def my_task(): # Tarefa de coleta de dados pass with DAG(\"meu_dag\", start_date=datetime(2023, 1, 1), schedule_interval=\"*/10 * * * *\") as dag: task = PythonOperator( task_id=\"coleta_dados\", python_callable=my_task ) ``` 3. Streaming de Dados com Apache Kafka : - Descri\u00e7\u00e3o: Configura\u00e7\u00e3o do Kafka para transmitir dados da API em tempo real para os consumidores. - Exemplo de Produ\u00e7\u00e3o de Mensagens: ```python from kafka import KafkaProducer import json producer = KafkaProducer(bootstrap_servers='localhost:9092') def send_data(data): producer.send('topico', json.dumps(data).encode('utf-8')) ``` 4. Processamento com Apache Spark : - Descri\u00e7\u00e3o: Spark \u00e9 usado para processar e transformar os dados que chegam pelo Kafka. - Exemplo de Transforma\u00e7\u00e3o no Spark: ```python from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"StreamingApp\").getOrCreate() def process_streaming_data(): # C\u00f3digo para consumir do Kafka e processar com Spark pass ``` 5. Armazenamento no MongoDB : - Descri\u00e7\u00e3o: Salvamento dos dados processados no MongoDB para consultas futuras. - Exemplo de Inser\u00e7\u00e3o no MongoDB: ```python from pymongo import MongoClient client = MongoClient('localhost', 27017) db = client['meu_banco'] collection = db['minha_colecao'] def insert_data(data): collection.insert_one(data) ``` 6. Configura\u00e7\u00e3o dos T\u00f3picos do Kafka Cria\u00e7\u00e3o de t\u00f3picos espec\u00edficos no Kafka para diferentes tipos de dados ou para dividir o processamento em v\u00e1rias etapas. Exemplo de comando Kafka: bash kafka-topics.sh --create --topic meu_topico --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 7. Monitoramento e Gest\u00e3o de Schemas Control Center : Utilizado para monitorar a performance e as m\u00e9tricas do Kafka. Schema Registry : Registro dos schemas de dados para garantir a consist\u00eancia entre produtores e consumidores. 8. Considera\u00e7\u00f5es de Desempenho e Escalabilidade Configura\u00e7\u00e3o de parti\u00e7\u00f5es no Kafka para suportar alta ingest\u00e3o de dados. Uso de cluster Spark com v\u00e1rios workers para distribuir o processamento. Configura\u00e7\u00e3o de sharding e replica\u00e7\u00e3o no MongoDB para escalabilidade. 9. Testes e Valida\u00e7\u00e3o Testes Unit\u00e1rios : Scripts para testar cada componente individual (API, Kafka, Spark, MongoDB). Testes de Integra\u00e7\u00e3o : Testar o fluxo completo para garantir que os dados fluem corretamente entre todos os componentes. Testes de Desempenho : Avalia\u00e7\u00e3o de desempenho sob carga para identificar gargalos e otimizar o sistema. 10. Deploy e Manuten\u00e7\u00e3o Docker Compose : Script para facilitar o deploy de todos os componentes em um ambiente de desenvolvimento. Ambientes : Explica\u00e7\u00e3o sobre ambientes (desenvolvimento, staging, produ\u00e7\u00e3o) e como configurar vari\u00e1veis para cada um. Monitoramento Cont\u00ednuo : Configura\u00e7\u00e3o de alertas e monitoramento para verificar o desempenho e a sa\u00fade do sistema. Estrutura do Projeto end-to-end-pipeline-etl/ \u2502 \u251c\u2500\u2500 docker/ # Arquivos de configura\u00e7\u00e3o para Docker \u2502 \u251c\u2500\u2500 docker-compose.yml # Docker Compose para orquestrar todos os servi\u00e7os \u2502 \u251c\u2500\u2500 Dockerfile_airflow # Dockerfile para o servi\u00e7o Airflow (se necess\u00e1rio) \u2502 \u2514\u2500\u2500 Dockerfile_spark # Dockerfile para o servi\u00e7o Spark (se necess\u00e1rio) \u2502 \u251c\u2500\u2500 requirements.txt # Depend\u00eancias Python do projeto \u2502 \u251c\u2500\u2500 src/ # C\u00f3digo-fonte principal \u2502 \u251c\u2500\u2500 api/ # C\u00f3digo para consumir dados da API externa \u2502 \u2502 \u2514\u2500\u2500 fetch_data.py # Script para coleta de dados da API e envio ao Kafka (fun\u00e7\u00e3o `extract_data`) \u2502 \u2502 \u2502 \u251c\u2500\u2500 airflow/ # Pipelines e DAGs do Apache Airflow \u2502 \u2502 \u2514\u2500\u2500 dags/ \u2502 \u2502 \u2514\u2500\u2500 etl_dag.py # DAG principal do pipeline ETL, importando as fun\u00e7\u00f5es \u2502 \u2502 \u2502 \u251c\u2500\u2500 kafka/ # Configura\u00e7\u00e3o e scripts para Kafka \u2502 \u2502 \u2514\u2500\u2500 producer.py # Script do produtor Kafka para enviar dados (fun\u00e7\u00e3o `send_to_kafka`) \u2502 \u2502 \u2502 \u251c\u2500\u2500 spark/ # Scripts para o processamento no Apache Spark \u2502 \u2502 \u2514\u2500\u2500 transform_data.py # Script para transforma\u00e7\u00e3o de dados com Spark (fun\u00e7\u00e3o `transform_data`) \u2502 \u2502 \u2502 \u2514\u2500\u2500 mongodb/ # Scripts para manipula\u00e7\u00e3o de dados no MongoDB \u2502 \u2514\u2500\u2500 load_data.py # Script para salvar dados no MongoDB (fun\u00e7\u00e3o `load_data`) \u2502 \u2514\u2500\u2500 README.md # Documenta\u00e7\u00e3o do projeto","title":"End-to-end Pipeline ETL"},{"location":"projects/end-to-end-pipeline-etl/#documentacao-do-projeto-sistema-de-processamento-de-dados-em-streaming-com-apache-kafka-apache-spark-e-mongodb","text":"","title":"Documenta\u00e7\u00e3o do Projeto: Sistema de Processamento de Dados em Streaming com Apache Kafka, Apache Spark e MongoDB"},{"location":"projects/end-to-end-pipeline-etl/#1-visao-geral-do-projeto","text":"Objetivo : Criar um sistema de processamento de dados em streaming que coleta dados de uma API, processa e analisa esses dados em tempo real e os armazena em um banco de dados distribu\u00eddo.","title":"1. Vis\u00e3o Geral do Projeto"},{"location":"projects/end-to-end-pipeline-etl/#2-arquitetura-do-sistema","text":"Explique a arquitetura do sistema conforme ilustrado no diagrama: API : Ponto de entrada dos dados. Pode ser configurada para consumir dados de uma API RESTful. Apache Airflow : Utilizado para orquestrar e gerenciar(agendar) o pipeline ETL. PostgreSQL : Banco de dados relacional para armazenar metadados providos do Airflow . Apache Kafka : Middleware de streaming para transmiss\u00e3o de dados em tempo real. Apache Zookeeper : Coordena o Kafka, gerenciando os brokers e t\u00f3picos. Control Center e Schema Registry : Monitoramento e gerenciamento de schemas de dados. Apache Spark : Plataforma de processamento distribu\u00eddo usada para processar os dados em tempo real. MongoDB : Banco de dados NoSQL para armazenamento de dados n\u00e3o estruturados com alta flexibilidade e escalabilidade. Docker : Ambientes de cont\u00eainer para cada servi\u00e7o, facilitando o deployment e a manuten\u00e7\u00e3o.","title":"2. Arquitetura do Sistema"},{"location":"projects/end-to-end-pipeline-etl/#3-configuracao-do-ambiente","text":"Pr\u00e9-requisitos : - Instala\u00e7\u00e3o do Docker e Docker Compose - Python 3.8 ou superior - Instala\u00e7\u00e3o dos pacotes necess\u00e1rios: - kafka-python para Kafka - pymongo para MongoDB - pyspark para Spark - psycopg2 para PostgreSQL - apache-airflow para Airflow Configura\u00e7\u00e3o do Docker : - Cria\u00e7\u00e3o de arquivos Dockerfile para cada servi\u00e7o, se necess\u00e1rio. - Configura\u00e7\u00e3o de um docker-compose.yml para orquestrar todos os servi\u00e7os. - Instru\u00e7\u00f5es para executar docker-compose up para iniciar todos os cont\u00eaineres.","title":"3. Configura\u00e7\u00e3o do Ambiente"},{"location":"projects/end-to-end-pipeline-etl/#4-estrutura-do-codigo","text":"Diret\u00f3rios e arquivos principais : - /src : Cont\u00e9m o c\u00f3digo-fonte principal. - /src/api : C\u00f3digo para consumir dados da API. - /src/airflow : Pipelines e DAGs do Apache Airflow. - /src/kafka : Configura\u00e7\u00f5es e scripts para o Kafka. - /src/spark : Scripts para processamento de dados no Apache Spark. - /src/mongodb : Scripts para manipula\u00e7\u00e3o de dados no MongoDB. - /docker : Configura\u00e7\u00f5es e scripts Docker. - requirements.txt : Depend\u00eancias do projeto.","title":"4. Estrutura do C\u00f3digo"},{"location":"projects/end-to-end-pipeline-etl/#5-componentes-principais-e-implementacao","text":"1. Coleta de Dados (API) : - Iremos usar a API CoinGecko (para dados de criptomoedas) - Descri\u00e7\u00e3o: Cria\u00e7\u00e3o de um script Python que coleta dados de uma API externa. - Exemplo de C\u00f3digo: ```python import requests def get_data(): response = requests.get(\"URL_DA_API\") data = response.json() # Processamento inicial dos dados return data ``` 2. Orquestra\u00e7\u00e3o com Apache Airflow : - Descri\u00e7\u00e3o: Configura\u00e7\u00e3o de DAGs no Airflow para agendar e gerenciar o pipeline de coleta e envio de dados. - Exemplo de DAG: ```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime def my_task(): # Tarefa de coleta de dados pass with DAG(\"meu_dag\", start_date=datetime(2023, 1, 1), schedule_interval=\"*/10 * * * *\") as dag: task = PythonOperator( task_id=\"coleta_dados\", python_callable=my_task ) ``` 3. Streaming de Dados com Apache Kafka : - Descri\u00e7\u00e3o: Configura\u00e7\u00e3o do Kafka para transmitir dados da API em tempo real para os consumidores. - Exemplo de Produ\u00e7\u00e3o de Mensagens: ```python from kafka import KafkaProducer import json producer = KafkaProducer(bootstrap_servers='localhost:9092') def send_data(data): producer.send('topico', json.dumps(data).encode('utf-8')) ``` 4. Processamento com Apache Spark : - Descri\u00e7\u00e3o: Spark \u00e9 usado para processar e transformar os dados que chegam pelo Kafka. - Exemplo de Transforma\u00e7\u00e3o no Spark: ```python from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"StreamingApp\").getOrCreate() def process_streaming_data(): # C\u00f3digo para consumir do Kafka e processar com Spark pass ``` 5. Armazenamento no MongoDB : - Descri\u00e7\u00e3o: Salvamento dos dados processados no MongoDB para consultas futuras. - Exemplo de Inser\u00e7\u00e3o no MongoDB: ```python from pymongo import MongoClient client = MongoClient('localhost', 27017) db = client['meu_banco'] collection = db['minha_colecao'] def insert_data(data): collection.insert_one(data) ```","title":"5. Componentes Principais e Implementa\u00e7\u00e3o"},{"location":"projects/end-to-end-pipeline-etl/#6-configuracao-dos-topicos-do-kafka","text":"Cria\u00e7\u00e3o de t\u00f3picos espec\u00edficos no Kafka para diferentes tipos de dados ou para dividir o processamento em v\u00e1rias etapas. Exemplo de comando Kafka: bash kafka-topics.sh --create --topic meu_topico --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3","title":"6. Configura\u00e7\u00e3o dos T\u00f3picos do Kafka"},{"location":"projects/end-to-end-pipeline-etl/#7-monitoramento-e-gestao-de-schemas","text":"Control Center : Utilizado para monitorar a performance e as m\u00e9tricas do Kafka. Schema Registry : Registro dos schemas de dados para garantir a consist\u00eancia entre produtores e consumidores.","title":"7. Monitoramento e Gest\u00e3o de Schemas"},{"location":"projects/end-to-end-pipeline-etl/#8-consideracoes-de-desempenho-e-escalabilidade","text":"Configura\u00e7\u00e3o de parti\u00e7\u00f5es no Kafka para suportar alta ingest\u00e3o de dados. Uso de cluster Spark com v\u00e1rios workers para distribuir o processamento. Configura\u00e7\u00e3o de sharding e replica\u00e7\u00e3o no MongoDB para escalabilidade.","title":"8. Considera\u00e7\u00f5es de Desempenho e Escalabilidade"},{"location":"projects/end-to-end-pipeline-etl/#9-testes-e-validacao","text":"Testes Unit\u00e1rios : Scripts para testar cada componente individual (API, Kafka, Spark, MongoDB). Testes de Integra\u00e7\u00e3o : Testar o fluxo completo para garantir que os dados fluem corretamente entre todos os componentes. Testes de Desempenho : Avalia\u00e7\u00e3o de desempenho sob carga para identificar gargalos e otimizar o sistema.","title":"9. Testes e Valida\u00e7\u00e3o"},{"location":"projects/end-to-end-pipeline-etl/#10-deploy-e-manutencao","text":"Docker Compose : Script para facilitar o deploy de todos os componentes em um ambiente de desenvolvimento. Ambientes : Explica\u00e7\u00e3o sobre ambientes (desenvolvimento, staging, produ\u00e7\u00e3o) e como configurar vari\u00e1veis para cada um. Monitoramento Cont\u00ednuo : Configura\u00e7\u00e3o de alertas e monitoramento para verificar o desempenho e a sa\u00fade do sistema.","title":"10. Deploy e Manuten\u00e7\u00e3o"},{"location":"projects/end-to-end-pipeline-etl/#estrutura-do-projeto","text":"end-to-end-pipeline-etl/ \u2502 \u251c\u2500\u2500 docker/ # Arquivos de configura\u00e7\u00e3o para Docker \u2502 \u251c\u2500\u2500 docker-compose.yml # Docker Compose para orquestrar todos os servi\u00e7os \u2502 \u251c\u2500\u2500 Dockerfile_airflow # Dockerfile para o servi\u00e7o Airflow (se necess\u00e1rio) \u2502 \u2514\u2500\u2500 Dockerfile_spark # Dockerfile para o servi\u00e7o Spark (se necess\u00e1rio) \u2502 \u251c\u2500\u2500 requirements.txt # Depend\u00eancias Python do projeto \u2502 \u251c\u2500\u2500 src/ # C\u00f3digo-fonte principal \u2502 \u251c\u2500\u2500 api/ # C\u00f3digo para consumir dados da API externa \u2502 \u2502 \u2514\u2500\u2500 fetch_data.py # Script para coleta de dados da API e envio ao Kafka (fun\u00e7\u00e3o `extract_data`) \u2502 \u2502 \u2502 \u251c\u2500\u2500 airflow/ # Pipelines e DAGs do Apache Airflow \u2502 \u2502 \u2514\u2500\u2500 dags/ \u2502 \u2502 \u2514\u2500\u2500 etl_dag.py # DAG principal do pipeline ETL, importando as fun\u00e7\u00f5es \u2502 \u2502 \u2502 \u251c\u2500\u2500 kafka/ # Configura\u00e7\u00e3o e scripts para Kafka \u2502 \u2502 \u2514\u2500\u2500 producer.py # Script do produtor Kafka para enviar dados (fun\u00e7\u00e3o `send_to_kafka`) \u2502 \u2502 \u2502 \u251c\u2500\u2500 spark/ # Scripts para o processamento no Apache Spark \u2502 \u2502 \u2514\u2500\u2500 transform_data.py # Script para transforma\u00e7\u00e3o de dados com Spark (fun\u00e7\u00e3o `transform_data`) \u2502 \u2502 \u2502 \u2514\u2500\u2500 mongodb/ # Scripts para manipula\u00e7\u00e3o de dados no MongoDB \u2502 \u2514\u2500\u2500 load_data.py # Script para salvar dados no MongoDB (fun\u00e7\u00e3o `load_data`) \u2502 \u2514\u2500\u2500 README.md # Documenta\u00e7\u00e3o do projeto","title":"Estrutura do Projeto"},{"location":"projects/minute_news/","text":"Minute News - Flask This project demonstrates how to use Flask to build a news website efficiently. It automates the process of updating news content and allows you to manage everything from your local machine. You can easily update the database and deploy changes without long deployment times, ensuring a smooth and quick process. Flask : A lightweight WSGI web application framework in Python that allows for easy development and deployment of web applications. MongoDB : A NoSQL database used to store news articles and related data, enabling flexible data management and quick retrieval of information. NewsAPI : Utilized to fetch the latest news articles from various sources, providing a comprehensive feed of current events and topics. Web Scraping : An automated web scraping tool is implemented to extract additional information that may not be fully processed by the API. This ensures that users receive the most relevant and updated content directly from the source pages. Another web scraping mechanism is employed to use online summarization tools to condense news articles. This is done to reduce processing load on local machines, allowing the use of less powerful hardware for efficient performance.","title":"Minute News"},{"location":"projects/minute_news/#minute-news-flask","text":"This project demonstrates how to use Flask to build a news website efficiently. It automates the process of updating news content and allows you to manage everything from your local machine. You can easily update the database and deploy changes without long deployment times, ensuring a smooth and quick process. Flask : A lightweight WSGI web application framework in Python that allows for easy development and deployment of web applications. MongoDB : A NoSQL database used to store news articles and related data, enabling flexible data management and quick retrieval of information. NewsAPI : Utilized to fetch the latest news articles from various sources, providing a comprehensive feed of current events and topics. Web Scraping : An automated web scraping tool is implemented to extract additional information that may not be fully processed by the API. This ensures that users receive the most relevant and updated content directly from the source pages. Another web scraping mechanism is employed to use online summarization tools to condense news articles. This is done to reduce processing load on local machines, allowing the use of less powerful hardware for efficient performance.","title":"Minute News - Flask"},{"location":"projects/train_your_classifier/","text":"Train Your Classifier This project provides a simple and effective way to train a machine learning model for image classification using Python and TensorFlow. The goal is to build a model that can automatically classify images into various categories, such as animals, objects, or custom classes. With the rise of machine learning and deep learning techniques, image classification has become a powerful tool in a variety of fields, including healthcare, security, and entertainment. This project allows you to harness that power by training your own classification model. Bird Reptiles Monkey Project Overview By following this guide, you will: Set up the development environment, including Jupyter Notebook, to easily experiment and visualize data. Prepare a dataset for training, organized by categories (e.g., cats, dogs, birds). Train a TensorFlow model to classify images using deep learning. Optionally enhance your model's performance using transfer learning with pre-trained models. Save the trained model and integrate it into your projects, allowing automated image recognition. This project provides an easy way to train a machine learning model for image classification using Python and TensorFlow. Author Miguel Angelo do Amaral Junior","title":"Train Your Classifier"},{"location":"projects/train_your_classifier/#train-your-classifier","text":"This project provides a simple and effective way to train a machine learning model for image classification using Python and TensorFlow. The goal is to build a model that can automatically classify images into various categories, such as animals, objects, or custom classes. With the rise of machine learning and deep learning techniques, image classification has become a powerful tool in a variety of fields, including healthcare, security, and entertainment. This project allows you to harness that power by training your own classification model. Bird Reptiles Monkey","title":"Train Your Classifier"},{"location":"projects/train_your_classifier/#project-overview","text":"By following this guide, you will: Set up the development environment, including Jupyter Notebook, to easily experiment and visualize data. Prepare a dataset for training, organized by categories (e.g., cats, dogs, birds). Train a TensorFlow model to classify images using deep learning. Optionally enhance your model's performance using transfer learning with pre-trained models. Save the trained model and integrate it into your projects, allowing automated image recognition. This project provides an easy way to train a machine learning model for image classification using Python and TensorFlow.","title":"Project Overview"},{"location":"projects/train_your_classifier/#author","text":"Miguel Angelo do Amaral Junior","title":"Author"},{"location":"scientific_articles/scientific_articles/","text":"Published Articles - Scientific Area This topic shows some works published between the years of 2015 to 2020 in scientific journals. Basically, in all works, the Python language was used together with its respective libraries to generate models and graphics. Published Works X Band Electromagnetic Property Influence of Multi-Walled Carbon Nanotube in Hybrid MnZn Ferrite and Carbonyl Iron Composites (2020) Investigation of Different Graphite Morphologies for Microwave Absorption at X and Ku-Band Frequency Range (2020) Study of Reflection Process for Nickel Coated Activated Carbon Fiber Felt Applied with Electromagnetic Interference Shielding (2019) Effect of Granulometric Distribution on Electromagnetic Shielding Effectiveness for Polymeric Composite Based on Natural Graphite (2019) Influence of the Permittivity on Carbon Fiber Particulates Applied in Radiation-Absorbing Materials (2018) Production and Characterization of Activated Carbon Fiber from Textile PAN Fiber (2017) Behavior of Porous Silicon Crystallite Size Analyzed by Raman Spectroscopy and Phonon Confinement Model (2015) Morphological Evolution of the Porous Silicon Surface for Different Etching Time and Current Density in HF-Ethanol Solution (2015)","title":"\ud83d\udd2c Scientific Articles"},{"location":"services/services/","text":"Services Data Engineering Services Data Engineering Services ETL Development & Pipeline Creation Design and implement ETL processes using Python and Kedro. Develop and maintain data pipelines for seamless data flow and integration. Database Management & Optimization Manage and organize databases (SQL & NoSQL) including PostgreSQL, MongoDB, and Elasticsearch. Optimize database schemas for efficient data storage and retrieval. Data Extraction & Integration Build and deploy web scraping pipelines for data extraction from websites and scientific articles. Integrate data from multiple sources using Python and cloud services. Automation & Scheduling Implement automation solutions and cron jobs for recurring data tasks. Develop scripts for automated data processing and reporting. Cloud Services & Deployment Utilize AWS services for cloud-based application deployment and management. Implement containerization with Docker and orchestration with Kubernetes and Rancher. Data Management Create and manage data repositories. Develop custom data management solutions tailored to client needs. Data Analysis & Visualization Services Data Analysis & Visualization Services Data Cleaning & Preprocessing Develop Python-based ETL pipelines for data cleaning and transformation. Handle missing data and outlier detection using advanced machine learning techniques. Exploratory Data Analysis (EDA) Perform EDA to uncover insights and guide decision-making. Utilize libraries such as Pandas, NumPy, and Matplotlib for data exploration. Predictive Modeling Build and deploy machine learning models for predictive analysis using Scikit-Learn and TensorFlow. Develop and implement custom models for specific client requirements. Data Visualization Create interactive dashboards and visualizations using Matplotlib, Seaborn, Plotly, and Kibana. Develop web-based applications for data visualization with Streamlit, Flask, and Django. Machine Learning & AI Services Machine Learning & AI Services Model Development & Integration Develop and integrate machine learning models for classification, regression, and prediction. Utilize TensorFlow, Scikit-Learn, and OpenAI API for AI solutions. Image Classification Implement image classification pipelines using TensorFlow and transfer learning techniques. Provide solutions for analyzing and categorizing images. Information Retrieval & Search Develop semantic (vector search) and lexical (free text search) functionalities. Implement Retrieval-Augmented Generation (RAG) pipelines for enhanced data retrieval. Web Development Services Web Development Services API Development Design and build RESTful APIs for data interaction and integration. Develop APIs for data submission and processing. Web Application Development Create web applications using Flask and Django for data visualization and interaction. Implement front-end technologies (HTML, CSS, JavaScript) for user interface design. Consulting & Project Management Consulting & Project Management Data Strategy & Consulting Provide expert advice on data strategy and management. Assist in developing data-driven solutions and optimizing data workflows. Project Management Lead data engineering projects and coordinate with cross-functional teams. Ensure timely delivery and adherence to best practices in data engineering and analysis. Code Review & Best Practices Conduct code reviews and enforce best practices in data engineering and machine learning. Provide recommendations for code optimization and maintainability.","title":"\ud83d\udee0\ufe0f Services"},{"location":"skills/skills/","text":"PROGRAMMING LANGUAGE Python 15 years specializing in data engineering, automation, integration, pipelines, and analytics. Bash Script Experience in writing and managing Bash scripts for automation and system tasks. CLOUD & INFRASTRUCTURE Mongo Atlas Experience with MongoDB's cloud-based NoSQL database service. Supabase Expertise in using Supabase for SQL database management and backend services. AWS Services Proficient in various AWS services including EC2, S3, Lambda, and RDS. MACHINE LEARNING & IMAGE CLASSIFIER & AI TensorFlow/Keras Experience in building and training image classifiers using TensorFlow and Keras. Scikit-Learn GPT/Openai Integration Transformers SEARCH & INFORMATION RETRIEVAL Vector Search Skilled in implementing semantic search using vector embeddings. Free Text Search Experience with implementing lexical search and information retrieval techniques. RAG Pipeline Expertise in using Retriever-Augmented Generation for enhanced search and information retrieval. CONTAINER ORCHESTRATION & DEPLOYMENT Docker Skilled in containerization with Docker for application deployment and management. Kubernetes Extensive experience in container orchestration with Kubernetes. Rancher Proficient in managing Kubernetes clusters using Rancher. Lens DEVELOPMENT ENVIRONMENT (Preferences) Linux (Ubuntu) 6 years of experience using Ubuntu for development and server management. Windows + WSL2 Comfortable with a hybrid development setup using Windows and WSL2 for Ubuntu terminal access. VSCode Preferred development environment using Visual Studio Code for coding and debugging. WEB DEVELOPMENT Flask Experience in developing lightweight web applications using Flask. Django Skilled in building robust and scalable web applications with Django. Front-End Technologies Proficient in HTML, CSS, and JavaScript for designing user interfaces and experiences. DATA ANALYSIS AND VISUALIZATION NumPy Extensive experience with NumPy for numerical computing and data manipulation. Pandas Proficient in using Pandas for data analysis and manipulation. Matplotlib Experienced in creating static, animated, and interactive visualizations with Matplotlib. Seaborn Skilled in statistical data visualization using Seaborn. Plotly Expertise in creating interactive visualizations with Plotly. Jupyter Notebook DATABASES (SQL & NOSQL) MySQL Proficient in using MySQL for relational database management and queries. PostgreSQL Experienced in designing and managing databases with PostgreSQL. MongoDB Skilled in working with MongoDB for flexible and scalable NoSQL solutions. Elastic Search WEB SCRAPING Scrapy Proficient in using Scrapy for extracting data from websites. BeautifulSoup Experience with BeautifulSoup for parsing HTML and XML documents. Selenium Skilled in using Selenium for automating web browser interactions. Request Experienced in making HTTP requests and handling responses. VERSION CONTROL Git Experienced with Git for version control and collaborative development. CI/CD Experienced in Continuous Integration and Continuous Deployment (CI/CD) practices. GitHub GitLab PROJECT MANAGEMENT SOFTWARE Jira Jira is a powerful project management tool developed by Atlassian. Redmine Redmine is an open-source project management and issue tracking tool.","title":"\ud83d\udcda Skills"},{"location":"skills/skills/#programming-language","text":"","title":"PROGRAMMING LANGUAGE"},{"location":"skills/skills/#cloud-infrastructure","text":"","title":"CLOUD &amp; INFRASTRUCTURE"},{"location":"skills/skills/#machine-learning-image-classifier-ai","text":"","title":"MACHINE LEARNING &amp; IMAGE CLASSIFIER &amp; AI"},{"location":"skills/skills/#search-information-retrieval","text":"","title":"SEARCH &amp; INFORMATION RETRIEVAL"},{"location":"skills/skills/#container-orchestration-deployment","text":"","title":"CONTAINER ORCHESTRATION &amp; DEPLOYMENT"},{"location":"skills/skills/#development-environment-preferences","text":"","title":"DEVELOPMENT ENVIRONMENT (Preferences)"},{"location":"skills/skills/#web-development","text":"","title":"WEB DEVELOPMENT"},{"location":"skills/skills/#data-analysis-and-visualization","text":"","title":"DATA ANALYSIS AND VISUALIZATION"},{"location":"skills/skills/#databases-sql-nosql","text":"","title":"DATABASES (SQL &amp; NOSQL)"},{"location":"skills/skills/#web-scraping","text":"","title":"WEB SCRAPING"},{"location":"skills/skills/#version-control","text":"","title":"VERSION CONTROL"},{"location":"skills/skills/#project-management-software","text":"","title":"PROJECT MANAGEMENT SOFTWARE"}]}